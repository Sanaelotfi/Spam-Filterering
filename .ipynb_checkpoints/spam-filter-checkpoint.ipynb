{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TP 2: Filtrage du spam \n",
    "___\n",
    "\n",
    "Nous proposons dans ce TP deux techniques pour la détection de spam. Il s'agit d'identifier si un mail fait partie des messages indésirables (spam) ou dans des messages légitimes (ham).\n",
    "\n",
    "La base de données utilisée est $Enron^{(1)(2)}$. Nous allons spécifier par la suite les dossiers utilisés pour l'entrainement et le test.\n",
    "\n",
    "Les 2 techniques que nous utilisons sont : la calssification naive bayésienne et la régression logistique.\n",
    "\n",
    "\n",
    "\n",
    "_**Remarque: Nous utilisons la version 1.1.0 de Julia.**_\n",
    "\n",
    "\n",
    "### Sommaire:\n",
    "___\n",
    "\n",
    "\n",
    "[I. Etat de l'art du filtrage du spam](#unit1)  \n",
    "[II. Classification naive bayésienne](#unit2)  \n",
    "[III. Régression logistique](#unit3)  \n",
    "[III.1. Préparation des données](#unit3.1)  \n",
    "[III.2. Méthode MCMC - Metropolis Hastings](#unit3.2)  \n",
    "[III.3. Descente de gradient](#unit3.3)  \n",
    "[IV. Références](#unit4)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I. Etat de l'art du filtrage du spam <a id = \"unit1\" > </a >\n",
    "___\n",
    "\n",
    "Pendent les dernière années, les gens ont commencé à utiliser la méthode de descente de gradient pour éviter le problème des matrices non inversibles (overfitting) en utilisant la régression logistique. Le passage à la régression logistique avec la méthode de descente de gradient a amélioré les performances significativement tout en permettant de prendre en compte les corrélations entre les mots. Cependant la classification naive bayésienne reste très simple tout en donnant des résultats impressionants <font co> . Nous allons alors explorer la classification naive bayésienne et la régression logistique (avec l'algorithme de Metropolis-Hastings et la descente de gradient). **Ne paniquez pas si vous ne comprenez pas les termes utilisés, tout sera expliqué par la suite**.\n",
    "\n",
    "\n",
    "### II. Classification naive bayésienne <a id = \"unit2\" > </a >\n",
    "___\n",
    "\n",
    "La classification naive bayésienne$^{(3)}$ consiste à utiliser le théoreme de Bayes pour calculer la probabilités à posteriori en estimant les probabilités conditionnelles d'une façon naive à partir des données d'entrainement.\n",
    "\n",
    "Dans notre contexte, notons la variable d'intérêt $S$ qui est égale à $1$ si le mail est un Spam et $0$ sinon. Soit $W$ le vecteur des mots contenus dans un mail donné $W = (w_1, ..., w_n)$. Alors, selon **le théorème de Bayes**, la probabilité que le mail soit un spam sachant son contenu (les mots qu'il contient) est:\n",
    "\n",
    "$$ P(S|W) = \\frac{P(W|S) P(S)}{P(W)}$$\n",
    "\n",
    "\n",
    "- La probabilité $P(S)$ représente notre probabilité a priori que le mail soit un spam ou non. Dans ce TP, nous prenons **$P(S=1) = P(S=0) = \\frac{1}{2}$**. Cependant, nous pouvons aussi laisser cette probabilité comme un **hyperparametre** de notre algorithme que nous pouvons déterminer en utilisant **La validation croisée**. Dans ce cas, la probabilité qui sera retenue est celle qui minimise l'erreur de classification sur l'ensemble de validation.  \n",
    "\n",
    "\n",
    "- La probabilité $P(W) = P(W|S) P(S) + P(W|\\overline{S}) P(\\overline{S})$ n'est pas importante, car pour classifier un mail en tant que un spam ou non, nous allons comparer $P(S = 1|W) = \\frac{P(W|S = 1) P(S = 1)}{P(W)}$ et $P(S = 0|W) = \\frac{P(W|S = 0) P(S = 1)}{P(W)}$. Les 2 partagent le même dénominateur, donc il suffit de comparer les nominateurs. C'est ce qu'on va faire dans ce TD.\n",
    "\n",
    "- Il reste l'estimation de la probabilité $P(W|S) = P(w_1, ..., w_n|S)$. Il est très difficile d'estimer cette probabilité à partir de l'ensemble d'entrainement directement. Pour cela, on fait **l'hypothèse (importante) de l'indépendence** qui caractérise la classification naive bayésienne: les mots sont considérés conditionnellement indépendants, donc on peut écrire:\n",
    "\n",
    "$$P(W|S) = P(w_1, ..., w_n|S) = P(w_1|S) \\times .. \\times P(w_n|S) = \\prod_{i=1}^{n} P(w_i|S) $$\n",
    "\n",
    "Nous pouvons alors estimer directement les probabilités $P(w_i|S)$. Pour un mot donné $w_i$, la probabilité $P(w_i|S)$ représente la fréquence de son apparition dans l'ensemble d'entrainement (la présence d'un mot dans un mail n'est comptée qu'une seule fois).\n",
    "\n",
    "Passons maintenant à l'implémentation de cette approche. Nous allons devoir parser tous les mails de l'ensemble d'entrainement. Nous utilisons le dossier *enron1* en tant qu'ensemble d'entrainement.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "using DataFrames\n",
    "using Distributions\n",
    "using JLD\n",
    "using Plots\n",
    "using Query\n",
    "using Random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous créons la fonction *textProcessing* qui prend un texte et retourne l'ensemble de mots contenus dans le texte (de longueur supérieure strictement à 1 pour la classification naive bayésienne et supérieure strictement à 2 pour la régression logistique)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "textProcessing (generic function with 2 methods)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "textProcessing: transformer un texte en une liste de mots\n",
    "            qui figurent dans le texte, en enlevant toute substance\n",
    "            de longueur inférieure à 2 et en mettant toutes les\n",
    "            lettres en minuscule\n",
    "input : - text [str]: chaîne de caractères \n",
    "output: - words [Array]: tableau qui contient les mots (chaînes de caractères)\n",
    "            qui figurent dans le texte\n",
    "\"\"\"\n",
    "\n",
    "function textProcessing(text, logReg = false)\n",
    "    # définir les séparateurs dans un texte\n",
    "    separators = [' ','-', '.', '_', '!', '@', ':', ',', '/', ';']\n",
    "    if logReg\n",
    "        separators = [' ','-', '.', '_', '!', '@', ':', ',', '/',\n",
    "            ';', ')','(', '[',']']\n",
    "    end\n",
    "    # mettre toutes les lettres en minuscule\n",
    "    # utiliser les séparateurs pour séparer les mots\n",
    "    words = split(lowercase(text), separators)\n",
    "    if logReg\n",
    "        # garder uniquement les substances à longueur supérieure strictement à 1\n",
    "        words = filter(x -> length(x) > 2, words)\n",
    "    else\n",
    "        # garder uniquement les substances à longueur supérieure strictement à 1\n",
    "        words = filter(x -> length(x) > 1, words)\n",
    "    end\n",
    "    # éviter la repétition des mots\n",
    "    words = unique(words)\n",
    "    \n",
    "    return words\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensuite, nous créons la fonction *parseEmail* qui prend en entrée le chemin vers un fichier-mail et effectue la lecture du mail ligne par ligne en utilisant la fonction définie précédemment *textProcessing* afin de récupérer les mots de chaque ligne."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "parseEmail"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "parseEmail: renvoyer une liste des mots qui figurent dans un mail\n",
    "input : - file_path [str]: chemin vers le fichier contenant le mail\n",
    "output: - list_words [Array] : tableau qui contient les mots (chaînes de caractères)\n",
    "            qui figurent dans le mail\n",
    "\"\"\"\n",
    "function parseEmail(file_path)\n",
    "    # initialiser la liste des mots\n",
    "    list_words = []\n",
    "    # lire le mail\n",
    "    f = open(file_path)\n",
    "    lines = readlines(f)\n",
    "    for line in lines\n",
    "        # récupérer les mots figurant dans cette ligne\n",
    "        words = textProcessing(line)\n",
    "        # concaténer tous les mots\n",
    "        list_words = vcat(list_words,words)\n",
    "    end\n",
    "    # éviter les répétition des mots\n",
    "    list_words = unique(list_words)\n",
    "    close(f)\n",
    "    return list_words   \n",
    "end\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La fonction *getTrainDF* permet de créer une dataframe qui contient tous les mots de l'ensemble d'entrainement et leurs probabilités (fréquences d'apparition). Notons bien que la lecture de certains fichiers retourne une erreur lorsqu'il contient des chaine de caractères avec un encodage non supporté par Julia pour les chaines de caractères. Pour cela, nous utilisons des outils de gestion d'exceptions (try/catch) et nous notons le nombre de fichiers non utilisés."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "getTrainDF (generic function with 1 method)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "getTrainDF: Créer un DFionnaire qui contient tous les mots\n",
    "            contenus dans les mails de l'ensemble d'entrainement.\n",
    "            Ces mots représentent les clés. Pour une clé donnée, la valeur\n",
    "            représente le nombre de mails dans lesqueles la clé (le mot)\n",
    "            est apparue. Cela donne la fréquence d'apparition de chaque\n",
    "            mot dans les mails.\n",
    "input : - dir_path [str]: chemin vers le dossier qui contient les mails\n",
    "output: - trainDF [Dataframe]: contient 2 colonnes: la première donne les mots contenus\n",
    "            dans les mails et le 2ème donne leurs fréquences d'apparition\n",
    "\"\"\"\n",
    "\n",
    "function getTrainDF(dir_path)\n",
    "    trainDF = DataFrame()\n",
    "    # initialiser l'ensemble des mots\n",
    "    all_words = []\n",
    "    # récupérer les fichier dans le dossier donné\n",
    "    train_dir = readdir(dir_path)\n",
    "    numFiles = length(train_dir)\n",
    "    # compteur du nombre des fichiers non utilisés\n",
    "    # à cause des problèmes d'encodage des chaines\n",
    "    # de caractères dans les mails\n",
    "    not_used_files = 0\n",
    "    i = 0\n",
    "    for path in train_dir\n",
    "        try\n",
    "            # récupérer le chemin vers le fichier\n",
    "            file_path = dir_path*\"/\"*path\n",
    "            # récupérer la liste des mots contenus dans ce mail\n",
    "            list_words = parseEmail(file_path)\n",
    "            # concaténer tous les mots\n",
    "            all_words = vcat(all_words, list_words)\n",
    "            # print(\"length of all words: \", length(all_words), \"\\n\")\n",
    "        catch\n",
    "            # incrémenter le nombre des fichiers non utilisés\n",
    "            not_used_files = not_used_files + 1\n",
    "        end\n",
    "        i = i +1\n",
    "    end\n",
    "    print(\"Le nombre de fichiers non utilisés est: \", not_used_files, \"\\n\")\n",
    "    \n",
    "    # nombre de fichiers utilisés réellement\n",
    "    numUsedFiles = i - not_used_files\n",
    "    \n",
    "    # créer un ensemble des mots uniques\n",
    "    unique_words = unique(all_words)\n",
    "        \n",
    "    # créer une dataframe avec tous les mots et leurs fréquences d'apparition\n",
    "    \n",
    "    # Ajouter la colonne des mots dans la dataframe\n",
    "    trainDF.words = unique_words\n",
    "    \n",
    "    # rajouter une colonne de probabilités , \"+1\" pour Laplace smoothing, section suivante\n",
    "    trainDF.proba = [(count(x->x==i,all_words) + 1)/numUsedFiles for i in unique_words]\n",
    "    \n",
    "    return trainDF, numUsedFiles\n",
    "    \n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Laplace Smoothing$^{(4)}$**\n",
    "\n",
    "En utilisant l'expression:\n",
    "\n",
    "$$P(W|S) = P(w_1, ..., w_n|S) = P(w_1|S) \\times .. \\times P(w_n|S) = \\prod_{i=1}^{n} P(w_i|S) $$\n",
    "\n",
    "on se rend compte rapidement que si un mot de l'ensemble test d'existe pas dans l'ensemble d'entrainement, sa probabilité estimée sera égale à zéro, et cela va faire que le produit soit nul et annuler l'effet de toutes les autres probabilités. C'est ce qu'on appelle le surapprentissage. De la même façon, nous souhaitons éviter les probabilités égales à 1. \n",
    "\n",
    "Pour cela, on va utiliser Laplace smoothing$^{(4)}$ qui consiste à changer la façon de laquelle on calcule l'estimation de la probabilité de la façon classique:\n",
    "\n",
    "$$ P(w_i|S) = \\frac{\\text{nombre de mail contenant } w_i}{\\text{nombre de mails de classe } S}$$\n",
    "\n",
    "à utiliser cette expression:\n",
    "\n",
    "$$ P(w_i|S) = \\frac{\\text{nombre de mail contenant} w_i + 1}{\\text{nombre de mails de classe } S + \\text{taille de l'ensemble d'entrainement}}$$\n",
    "\n",
    "Cela nous permet de résoudre le problème de surapprentissage.\n",
    "\n",
    "Nous allons également créer une colonne qui contient le logarithme des probabilités au lieu des probabilités elle-même. En effet, les probabilités sont très petites et leur produit pourrait nous causer des problèmes numériques. Donc nous préférons sommer les log des probabilités."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rescaleProba"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\"\"\"\n",
    "rescaleProba : mettre à jour le calcul de la probabilité\n",
    "            en utilisant Laplace smoothing. Créer une nouvelle colonne\n",
    "            qui représente le logarithme de la probabilité\n",
    "input : - trainDF [Dataframe]: contient les mots de l'ensemble d'entrainement\n",
    "            et leur probabilités d'apparition \n",
    "        - numUsedFiles [int]: Le nombre de fichier utilisés pour créer la Dataframe\n",
    "            pour cette classe (Spam ou Ham).\n",
    "        - totalNumFiles [int]: nombre total des fichiers utilisés pour l'entrainement\n",
    "output: - trainDF [Dataframe]: Dataframe mise à jour.\n",
    "\"\"\"\n",
    "function rescaleProba(trainDF, numUsedFiles, totalNumFiles)\n",
    "    # mettre à jour le calcul de la probabilité\n",
    "    trainDF.proba = map(x -> x * numUsedFiles /(numUsedFiles + totalNumFiles) , trainDF.proba)\n",
    "    # ajouter une nouvelle colonne pour le log de la probabilité\n",
    "    trainDF.logProba = map(x -> log(x) , trainDF.proba)\n",
    "    \n",
    "    return trainDF\n",
    "end\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous créons finalement la fonction qui connecte toutes les parties afin de retourner 2 dataframes pour les mails spam et non spams et le nombres de fichiers utilisés dans chaque cas. Puisque l'entrainement prend beaucoup de temps, nous avons sauvegarder ces résultats sous forme d'un fichier *trainData.jld*. Vous pouvez alors soit relancer l'entrainement (mode=\"train\") soit utilisés directement ce ficher (mode=\"upload\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "trainingProbabilities"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "trainingProbabilities : Récupérer les dataframes des probabilités pour les 2 classes\n",
    "            ham et spam, pour l'ensemble d'entrainement\n",
    "input : - path_train_spam [str]: chemin vers le dossier des mails spam pour\n",
    "            l'entrainement \n",
    "        - path_train_ham [str]: chemin vers le dossier des mails non spam pour\n",
    "            l'entrainement \n",
    "        - mode [binary - str : \"upload\"/train]: si mode est \"train\", nous récupérons les \n",
    "            fréquences des mots dans l'ensemble d'entrainement, nous les sauvegardons\n",
    "            dans le dossier \"data\" (pour utilisation ultérieure) et nous les retournons.\n",
    "            Si le mode est \"upload\", nous récupérns les données déjà sauvegardées, et nous\n",
    "            les retournons.\n",
    "output: - trainDFSpam [Dataframe]: contient les mots de l'ensemble d'entrainement\n",
    "            contenus dans des mails \"spam\" et leur probabilités d'apparition.\n",
    "        - trainDFHam [Dataframe]: contient les mots de l'ensemble d'entrainement\n",
    "            contenus dans des mails \"ham\" et leur probabilités d'apparition.\n",
    "\"\"\"\n",
    "function trainingProbabilities(path_train_spam, path_train_ham, mode=\"upload\")\n",
    "    \n",
    "    if mode == \"train\"\n",
    "        # créer des dataframes\n",
    "        processSpam = getTrainDF(path_train_spam)\n",
    "        processHam = getTrainDF(path_train_ham)\n",
    "        # récupérer les dataframes\n",
    "        trainDFSpam = processSpam[1]\n",
    "        trainDFHam = processHam[1]\n",
    "        # récupérer le nombre des fichier utilisés \n",
    "        numUsedFilesSpam = processSpam[2]\n",
    "        numUsedFilesHam = processHam[2]\n",
    "        # nombre total des fichiers utilisés pour l'entrainement\n",
    "        totalNumFiles = numUsedFilesSpam + numUsedFilesHam\n",
    "        # mettre à jour les probabilités\n",
    "        trainDFSpam = rescaleProba(trainDFSpam, numUsedFilesSpam, totalNumFiles)\n",
    "        trainDFHam = rescaleProba(trainDFHam, numUsedFilesHam, totalNumFiles)\n",
    "        # sauvegarder les données d'entrainement (fréquence de chaque mot)\n",
    "        save(\"data/trainData.jld\",\"trainDFSpam\",trainDFSpam,\n",
    "            \"trainDFHam\",trainDFHam, \"numUsedFilesSpam\", numUsedFilesSpam,\n",
    "            \"numUsedFilesHam\", numUsedFilesHam)\n",
    "    else\n",
    "        # mode: upload\n",
    "        # récupérer les données sauvegardées\n",
    "        trainDFSpam = load(\"data/trainData.jld\")[\"trainDFSpam\"]\n",
    "        trainDFHam = load(\"data/trainData.jld\")[\"trainDFHam\"]\n",
    "        numUsedFilesSpam = load(\"data/trainData.jld\")[\"numUsedFilesSpam\"]\n",
    "        numUsedFilesHam = load(\"data/trainData.jld\")[\"numUsedFilesHam\"]\n",
    "        \n",
    "    end\n",
    "    return trainDFSpam, trainDFHam, numUsedFilesSpam, numUsedFilesHam\n",
    "end\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous implémentons finalement la fonction *naiveBayes* qui prends une liste de mots et retourne *1* si le mail est classifié en tant que spam et 0 sinon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "naiveBayes (generic function with 2 methods)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\"\"\"\n",
    "naiveBayes : Implémentation simple de Naive Bayes avec Laplace smoothing.\n",
    "            Renvoie une variable binaire qui détermine si le mail est un spam ou non\n",
    "input : - testWords [Array]: liste des mots contenus dans le mail\n",
    "        - trainDFSpam [Dataframe]: contient les mots de l'ensemble d'entrainement\n",
    "            contenus dans des mails \"spam\" et leur probabilités d'apparition.\n",
    "        - trainDFHam [Dataframe]: contient les mots de l'ensemble d'entrainement\n",
    "            contenus dans des mails \"ham\" et leur probabilités d'apparition.\n",
    "        - spamProba [float]: probabilité qu'un mail soit un spam, en général (a priori)\n",
    "output: - isSpam [binary- 0,1] : égale à 1 si le mail est un spam, 0 sinon.\n",
    "\"\"\"\n",
    "\n",
    "function naiveBayes(testWords, trainDFSpam, trainDFHam, numUsedFilesSpam, numUsedFilesHam, spamProba = 0.5)\n",
    "    # la probabilité qu'un email ne soit pas spam, en général\n",
    "    hamProba = 1 - spamProba\n",
    "    \n",
    "    totalNumFiles = numUsedFilesHam + numUsedFilesSpam\n",
    "    # les probabilités pour qu'un nouveau mot soit un spam ou non\n",
    "    # sont données par Laplace smoothing\n",
    "    proba_new_word_spam = 1 / (numUsedFilesSpam + totalNumFiles)\n",
    "    proba_new_word_ham = 1 / (numUsedFilesHam + totalNumFiles)\n",
    "    # récupérer les mots contenus dans le mail qui figurent dans\n",
    "    # l'ensemble d'entrainement du spam\n",
    "    intersection_spam = intersect(testWords, trainDFSpam.words)\n",
    "    # nombre du nouveau mots par rapport à la Dataframe du spam\n",
    "    new_words_spam = length(testWords) - length(intersection_spam)\n",
    "    # récupérer les mots contenus dans le mail qui figurent dans\n",
    "    # l'ensemble d'entrainement du ham\n",
    "    intersection_ham = intersect(testWords, trainDFHam.words)\n",
    "    # nombre du nouveau mots par rapport à la Dataframe du non spam\n",
    "    new_words_ham = length(testWords) - length(intersection_ham)\n",
    "    # récupérer une sous dataframe avec les mots contenus dans le spam uniquement\n",
    "    spamDF = @from i in trainDFSpam begin\n",
    "        @where i.words in intersection_spam\n",
    "        @select {i.words, i.logProba, i.proba}\n",
    "        @collect DataFrame\n",
    "        end\n",
    "    # récupérer une sous dataframe avec les mots contenus dans le ham uniquement\n",
    "    hamDF = @from i in trainDFHam begin\n",
    "        @where i.words in intersection_ham\n",
    "        @select {i.words, i.logProba, i.proba}\n",
    "        @collect DataFrame\n",
    "        end\n",
    "    \n",
    "    # logarithme de la probabilité de que le mail soit un spam\n",
    "    logprobaspam = sum(spamDF.logProba) + new_words_spam * log(proba_new_word_spam) + log(spamProba)\n",
    "    # logarithme de la probabilité de que le mail ne soit pas un spam\n",
    "    logprobaham = sum(hamDF.logProba) + new_words_ham * log(proba_new_word_ham) + log(spamProba)\n",
    "    \n",
    "    # On définie une variable binaire égale à 1 si le mail est un spam\n",
    "    isSpam = 0\n",
    "    if logprobaspam > logprobaham\n",
    "        isSpam = 1\n",
    "    end\n",
    "    return isSpam\n",
    "    \n",
    "end\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous créons la fonction *emailIsSpam* qui elle prend le chemin vers un fichier test et appelle la fonction *naiveBayes* pour retourner si le mail est un spam ou non."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "emailIsSpam"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "emailIsSpam : renvoyer une variable binaire qui détermine si le mail est un spam ou non\n",
    "input : - file_path [str]: chemin vers le fichier contenant le mail test \n",
    "        - trainDFSpam [Dataframe]: contient les mots de l'ensemble d'entrainement\n",
    "            contenus dans des mails \"spam\" et leur probabilités d'apparition.\n",
    "        - trainDFHam [Dataframe]: contient les mots de l'ensemble d'entrainement\n",
    "            contenus dans des mails \"ham\" et leur probabilités d'apparition.\n",
    "        - spamProba [float]: probabilité qu'un mail soit un spam, en général (a priori)\n",
    "output: - isSpam [binary- 0,1] : égale à 1 si le mail est un spam, 0 sinon.\n",
    "\"\"\"\n",
    "function emailIsSpam(file_path, trainDFSpam, trainDFHam, numUsedFilesSpam, numUsedFilesHam, spamProba = 0.5)\n",
    "    # récupérer les mots dans le mail test\n",
    "    testWords = parseEmail(file_path)\n",
    "    # On récupère la variable binaire isSpam égale à 1 si le mail est un spam\n",
    "    isSpam = naiveBayes(testWords, trainDFSpam, trainDFHam, numUsedFilesSpam, numUsedFilesHam, spamProba)    \n",
    "    return isSpam\n",
    "    \n",
    "end  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entrainement :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"./data/enron1/ham\""
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_train_spam = \"./data/enron1/spam\"\n",
    "path_train_ham = \"./data/enron1/ham\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDFSpam, trainDFHam, numUsedFilesSpam, numUsedFilesHam = trainingProbabilities(path_train_spam,\n",
    "    path_train_ham);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test :\n",
    "- Pour un seul exemple:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tester la fonction emailIsSpam avec un email Spam\n",
    "test = \"./data/enron2/spam\"\n",
    "path = readdir(test)[5]\n",
    "file_path = test*\"/\"*path\n",
    "emailIsSpam(file_path, trainDFSpam, trainDFHam, numUsedFilesSpam, numUsedFilesHam)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La classification est correcte. La mail effectivement est un spam."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Pour tout l'ensemble d'entrainement:  \n",
    "Nous utilisons l'ensemble enron2 (en entier) pour tester la performance de notre classifieur. Vous avez le choix entre relancer l'évaluation (perform_test = true) ou faire un print de la précision trouvéé lors d'un test précédent (perform_test = false)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy in the last test was: 0.9245810055865922"
     ]
    }
   ],
   "source": [
    "# tester la fonction emailIsSpam avec tout l'ensemble test spam et ham\n",
    "testSpam = \"./data/enron2/spam\"\n",
    "pathsSpam = readdir(testSpam)\n",
    "testHam = \"./data/enron2/ham\"\n",
    "pathsHam = readdir(testHam)\n",
    "\n",
    "perform_test = false\n",
    "\n",
    "if perform_test\n",
    "    i = 0\n",
    "    well_classified = 0\n",
    "    for path in pathsSpam\n",
    "        try\n",
    "            file_path = testSpam*\"/\"*path\n",
    "            well_classified = well_classified + emailIsSpam(file_path, trainDFSpam, trainDFHam, numUsedFilesSpam, numUsedFilesHam)\n",
    "\n",
    "        catch  \n",
    "            i = i + 1\n",
    "        end\n",
    "    end\n",
    "\n",
    "    for path in pathsHam\n",
    "        try\n",
    "            file_path = testHam*\"/\"*path\n",
    "            well_classified = well_classified + 1 - emailIsSpam(file_path, trainDFSpam, trainDFHam,\n",
    "                numUsedFilesSpam, numUsedFilesHam)\n",
    "        catch  \n",
    "            i = i + 1\n",
    "        end\n",
    "    end\n",
    "    accuracy = well_classified / (length(pathsSpam) + length(pathsHam) - i)\n",
    "    print(\"The accuracy is : \", accuracy)\n",
    "else\n",
    "    old_accuracy = 0.9245810055865922\n",
    "    print(\"The accuracy in the last test was: \", old_accuracy)\n",
    "    \n",
    "end\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion: \n",
    "\n",
    "La précision de la classification naive bayésienne est **92%**. La classification naive bayésienne n'est pas assez naive finalement ! \n",
    "\n",
    "Nous pouvons améliorer les résultats en changeant notre probabilité a priori qu'un mail soit un spam (en utilisant notamment un ensemble de validation).  \n",
    "\n",
    "Nous passons maintenant à une 2ème méthode qui est la régression logistique."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### III. Régression logistique$^{(8)}$ <a id = \"unit3\" > </a >\n",
    "___\n",
    "La régression logistique est un algorithme de classification, utilisé lorsque la variable de réponse est catégorique. L'idée de la régression logistique est de trouver une **relation entre les caractéristiques (variables explicatives) et la probabilité d'un résultat particulier (classe particulière de la variable d'intérêt)**. Dans notre cas, la variable d'intérêt est une variable binaire codée en tant que 1 (le mail est un Spam) ou 0 (le mail n'est pas un Spam). En d’autres termes, le modèle de régression logistique prédit $P(Y = 1|X=x) = \\frac{e^{X\\beta}}{1 + e^{X\\beta}} $ en déterminant le vecteur des coefficient $\\beta$.\n",
    "\n",
    "L'algorithme d'estimation de maximum de vraisemble (MLE) détermine les coefficients de régression du modèle qui prédit avec précision la probabilité de la variable d'intérêt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### III.1. Préparation des données: <a id = \"unit3.1\" > </a > "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Vectoriser chaque email:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vectoriser des emails constitue à:\n",
    "\n",
    "- Créer un vecteur pour chaque email, sa taille sera égale au nombre des mots générés à partir de l'ensemble d'entrainement.\n",
    "\n",
    "- Pour le mot n°i de l'ensemble d'entrainement, on vérifie s'il existe dans l'email qu'on veut vectoriser. Si c'est le cas, on donne la valeur 1 au $i^{eme}$ indice du vecteur, sinon 0.\n",
    "\n",
    "- On execute ce processus pour tous les emails."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "vectorize_parsed_mail"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "vectorize_parsed_mail: Transformer tous les email dans data_path\n",
    "                       à des vecteurs de même taille (0/1).\n",
    "Input: data_path: le chemin des données\n",
    "       train_words: Les mots générés de l'ensemble d'entrainement.\n",
    "Output: Matrice, chaque vecteur ligne représente un email.\n",
    "\"\"\"\n",
    "function vectorize_parsed_mail(data_path,train_words)\n",
    "    # Lecture des fichiers dans le data_path\n",
    "    data_dir = readdir(data_path)\n",
    "    # Créer une matrice vide où l'on mettra les vecteurs\n",
    "    vector = zeros(length(data_dir),length(train_words))\n",
    "    println(\"Nombre de fichier dans le dossier: \",length(data_dir))\n",
    "    # Problèmes avec caractères spéciaux -> sauter le fichier\n",
    "    files_not_used = 0\n",
    "    for file in 1:length(data_dir)\n",
    "        try\n",
    "            # Extraire les mots du fichier\n",
    "            words = parseEmail(string(data_path,data_dir[file]))\n",
    "            for i in 1:length(train_words)\n",
    "                # Vérification si le ieme mot dans train existe dans le fichier\n",
    "                if train_words[i] in words\n",
    "                    vector[file,i] = 1\n",
    "                end \n",
    "            end\n",
    "        catch\n",
    "            files_not_used = files_not_used+1\n",
    "        end\n",
    "    end\n",
    "    println(\"fichiers non utilisées: \", files_not_used)\n",
    "    return vector\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Diviser les données en Train/Test:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On divise nos données pour avoir une base sur laquelle on entraîne notre modèle, puis une qui nous servira à généraliser la précision/erreur qu'on obtiendra avec des nouvelles données.\n",
    "\n",
    "Aussi, il ne faut surtout pas oublier d'ajouter le vecteur correspondant aux biais."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "prepare_data"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "prepare_data: Séparer les données en Train/Test sets\n",
    "input: Spam Train/Test & Ham Train/Test\n",
    "output: Input_train/test & Output_train/test\n",
    "\"\"\"\n",
    "function prepare_data(train_path_spam, train_path_ham, test_path_spam, test_path_ham, trainDFSpam, trainDFHam)\n",
    "    # Extraction de tous les mots dans notre base d'entrainement\n",
    "    \n",
    "    train_words = unique(vcat(trainDFHam[1][:,1], trainDFSpam[1][:,1]));\n",
    "    \n",
    "    # Vectorisation + Ajout du biais + Création des outputs (Spam=1, Ham=0)\n",
    "    X_train_spam = vectorize_parsed_mail(train_path_spam, train_words);\n",
    "    println(\"taille de train_spam set: \",size(X_train_spam))\n",
    "    X_train_ham = vectorize_parsed_mail(train_path_ham, train_words);\n",
    "    println(\"taille de train_ham set: \",size(X_train_ham))\n",
    "    X_train = vcat(X_train_spam,X_train_ham);\n",
    "    X_train = hcat(ones(size(X_train)[1]), X_train);\n",
    "    println(\"taille de train set: \",size(X_train))\n",
    "    y_train = vcat(ones(size(X_train_spam)[1]), zeros(size(X_train_ham)[1]));\n",
    "    \n",
    "    # Vectorisation + Ajout du biais + Création des outputs (Spam=1, Ham=0)\n",
    "    X_test_spam = vectorize_parsed_mail(test_path_spam, train_words);\n",
    "    println(\"taille de test_spam set: \",size(X_test_spam))\n",
    "    X_test_ham = vectorize_parsed_mail(test_path_ham, train_words);\n",
    "    println(\"taille de test_ham set: \",size(X_test_ham))\n",
    "    X_test = vcat(X_test_spam,X_test_ham);\n",
    "    X_test = hcat(ones(size(X_test)[1]), X_test);\n",
    "    println(\"taille de test set: \",size(X_test))\n",
    "    y_test = vcat(ones(size(X_test_spam)[1]), zeros(size(X_test_ham)[1]));\n",
    "    \n",
    "    return train_words, X_train, y_train, X_test, y_test\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de fichier dans le dossier: 1500\n",
      "fichiers non utilisées: 1500\n",
      "taille de train_spam set: (1500, 50482)\n",
      "Nombre de fichier dans le dossier: 3672\n",
      "fichiers non utilisées: 3672\n",
      "taille de train_ham set: (3672, 50482)\n",
      "taille de train set: (5172, 50483)\n",
      "Nombre de fichier dans le dossier: 1496\n",
      "fichiers non utilisées: 1496\n",
      "taille de test_spam set: (1496, 50482)\n",
      "Nombre de fichier dans le dossier: 4361\n",
      "fichiers non utilisées: 4361\n",
      "taille de test_ham set: (4361, 50482)\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "OutOfMemoryError()",
     "output_type": "error",
     "traceback": [
      "OutOfMemoryError()",
      "",
      "Stacktrace:",
      " [1] Type at .\\boot.jl:404 [inlined]",
      " [2] Type at .\\boot.jl:412 [inlined]",
      " [3] similar at .\\array.jl:317 [inlined]",
      " [4] similar at .\\abstractarray.jl:575 [inlined]",
      " [5] _typed_hcat(::Type{Float64}, ::Tuple{Array{Float64,1},Array{Float64,2}}) at .\\abstractarray.jl:1276",
      " [6] typed_hcat at .\\abstractarray.jl:1257 [inlined]",
      " [7] hcat at C:\\cygwin\\home\\Administrator\\buildbot\\worker\\package_win64\\build\\usr\\share\\julia\\stdlib\\v1.1\\SparseArrays\\src\\sparsevector.jl:1068 [inlined]",
      " [8] prepare_data(::String, ::String, ::String, ::String, ::DataFrame, ::DataFrame) at .\\In[21]:27",
      " [9] top-level scope at In[23]:5"
     ]
    }
   ],
   "source": [
    "path_train_spam = \"./data/enron1/spam\"\n",
    "path_train_ham = \"./data/enron1/ham\"\n",
    "path_test_spam = \"./data/enron2/spam\"\n",
    "path_test_ham = \"./data/enron2/ham\"\n",
    "train_words, X_train, y_train, X_test, y_test = prepare_data(path_train_spam, path_train_ham,\n",
    "    path_test_spam, path_test_ham, trainDFSpam, trainDFHam);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### III.2. Méthode MCMC - Metropolis Hastings$^{(6)}$: <a id = \"unit3.2\" > </a > "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans cette implémentation, nous allons aborder la méthode Metropolis-Hastings.\n",
    "\n",
    "Pour l'implémenter, nous avons besoin de:\n",
    "\n",
    "- La loi à priori des poids $f_{\\beta}$\n",
    "\n",
    "- La vraisemblance $f_{Y|\\beta,X}$\n",
    "\n",
    "Cela nous permettra en premier lieu d'avoir la loi à posteriori des poids connaissant X et Y $$f_{\\beta|Y,X} = \\frac{f_{Y|\\beta,X}.f_{\\beta}}{f_Y}$$\n",
    "\n",
    "Ensuite, nous aurons besoins d'utiliser l'algorithme de Metropolis Hastings pour avoir de bonnes estimations des poids dont on dispose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sigmoid"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "sigmoid: calule la sigmoid de la sortie pour modéliser la probabilité que Y=1.\n",
    "output: vecteur de la taille du nombre d'exemples. Chaque valeur sera entre 0 et 1.\n",
    "\"\"\"\n",
    "function sigmoid(output)\n",
    "    if typeof(output) == Float64\n",
    "        return exp(output)/(1+exp(output))\n",
    "    end\n",
    "    l = zeros(length(output))\n",
    "    for i in 1:size(output)[1]\n",
    "        l[i] = exp(output[i])/(1+exp(output[i]))\n",
    "    end\n",
    "    return l\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour des raisons de stabilité numérique, nous allons utiliser le $log(f_{\\beta|Y,X})$ à la place de $f_{\\beta|Y,X}$, la multiplication se transforme en somme alors.\n",
    "\n",
    "Concernant la loi à priori des poids, nous allons supposer que les poids suivent une loi à priori impropre.\n",
    "\n",
    "Nous pouvons tout de même supposer que chaque élément suit une loi normale de moyenne 0 et de variance 10, Il suffit de décocher les commentaires dans la prochaine cellule.$$f_{\\beta} = N(\\beta|0,10)$$.\n",
    "\n",
    "On peut montrer que le log de la vraisemblance devient comme suit:\n",
    "$$\n",
    "\\sum_{\\substack{1<i<m}} y^{(i)} log( \\hat{p}^{(i)} ) + (1-y^{(i)}) log( 1 - \\hat{p}^{(i)} )\n",
    "$$\n",
    "où $$\\hat{p}^{(i)} = P(y^{(i)}=1)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "log_posterior"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "log_posteriori: Retourne le logarithme de la loi à posteriori des poids\n",
    "\"\"\"\n",
    "function log_posterior(x, y, theta)\n",
    "    # P(y=1)\n",
    "    proba = sigmoid(x*theta)\n",
    "    vraisemblance = 0\n",
    "    for i in 1:length(y)\n",
    "        vraisemblance = vraisemblance + y[i] * log(proba[i]) + (1-y[i]) * log(1-proba[i])\n",
    "    end\n",
    "    priori = 0\n",
    "    # Si on veut \n",
    "    #for k in 1:length(theta)\n",
    "        #priori = priori + log(pdf(Normal(0,10),theta[k]))\n",
    "    #end\n",
    "    return vraisemblance + priori\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "logistic_bayes"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "logistique_bayes: Méthode itérative qui utiliser l'algorithme de Metropolis-Hastings\n",
    "                  pour trouver les meilleurs poids avec le jeu de données d'entrainement\n",
    "Inputs: x: Entrée\n",
    "        y: Sortie\n",
    "        n_iterations: pour M-H\n",
    "        sd: l'écart type de la loi normale de proposition des candidats\n",
    "Output: Les poids enregistrés pour chaque itération\n",
    "        Taux d'acceptation de chaque poid\n",
    "\"\"\"\n",
    "function logistic_bayes(x, y, n_iterations=10000,sd=0.1)\n",
    "    Random.seed!(21);\n",
    "    # Initialisation\n",
    "    acceptance = zeros(size(x)[2])\n",
    "    total = zeros(size(x)[2])\n",
    "    save = zeros(n_iterations,size(x)[2])\n",
    "    # Initialisation des poids par une lois normale\n",
    "    theta = randn(size(x)[2])\n",
    "    # Enregistrer les premiers poids\n",
    "    save[1,:] = theta\n",
    "    # Loi à posteriori\n",
    "    posterior = log_posterior(x,y,theta)\n",
    "    for i in 2:n_iterations\n",
    "        println(\"Itération n°\",i-1)\n",
    "        # Pour chaque poid\n",
    "        for j in 1:length(theta)\n",
    "            total[j] = total[j] + 1\n",
    "            theta_maybe = theta\n",
    "            # Générer un candidat pour le jeme poid\n",
    "            d = Normal(theta[j], sd)\n",
    "            theta_maybe[j] = rand(d,1)[1]\n",
    "            # Calcul de la nouvelle loi à posteriori\n",
    "            posterior_maybe = log_posterior(x,y,theta_maybe)\n",
    "            # Calcul du Ratio\n",
    "            R = min(exp(posterior_maybe - posterior),1)\n",
    "            # Générer un échantillon avec la loi uniforme\n",
    "            U = rand(1)[1]\n",
    "            # Si acceptation\n",
    "            if U < R\n",
    "                #println(i,\",\",j)\n",
    "                theta = theta_maybe\n",
    "                posterior = posterior_maybe\n",
    "                acceptance[j] = acceptance[j] + 1\n",
    "            end\n",
    "        end\n",
    "        save[i,:] = theta\n",
    "    end\n",
    "    return save, acceptance./total\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'algorithme de Metropolis Hastings prendra beaucoup de temps avec une grand nombre de poids. Nous appliquons l'algorithme avec 100 itérations seulement. Ça pourrait ne pas être suffisant, mais on verra que ça donne un bon résultat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Itération n°1\n",
      "Itération n°2\n",
      "Itération n°3\n",
      "Itération n°4\n",
      "Itération n°5\n",
      "Itération n°6\n",
      "Itération n°7\n",
      "Itération n°8\n",
      "Itération n°9\n",
      "Itération n°10\n",
      "Itération n°11\n",
      "Itération n°12\n",
      "Itération n°13\n",
      "Itération n°14\n",
      "Itération n°15\n",
      "Itération n°16\n",
      "Itération n°17\n",
      "Itération n°18\n",
      "Itération n°19\n",
      "Itération n°20\n",
      "Itération n°21\n",
      "Itération n°22\n",
      "Itération n°23\n",
      "Itération n°24\n",
      "Itération n°25\n",
      "Itération n°26\n",
      "Itération n°27\n",
      "Itération n°28\n",
      "Itération n°29\n",
      "Itération n°30\n",
      "Itération n°31\n",
      "Itération n°32\n",
      "Itération n°33\n",
      "Itération n°34\n",
      "Itération n°35\n",
      "Itération n°36\n",
      "Itération n°37\n",
      "Itération n°38\n",
      "Itération n°39\n",
      "Itération n°40\n",
      "Itération n°41\n",
      "Itération n°42\n",
      "Itération n°43\n",
      "Itération n°44\n",
      "Itération n°45\n",
      "Itération n°46\n",
      "Itération n°47\n",
      "Itération n°48\n",
      "Itération n°49\n",
      "Itération n°50\n",
      "Itération n°51\n",
      "Itération n°52\n",
      "Itération n°53\n",
      "Itération n°54\n",
      "Itération n°55\n",
      "Itération n°56\n",
      "Itération n°57\n",
      "Itération n°58\n",
      "Itération n°59\n",
      "Itération n°60\n",
      "Itération n°61\n",
      "Itération n°62\n",
      "Itération n°63\n",
      "Itération n°64\n",
      "Itération n°65\n",
      "Itération n°66\n",
      "Itération n°67\n",
      "Itération n°68\n",
      "Itération n°69\n",
      "Itération n°70\n",
      "Itération n°71\n",
      "Itération n°72\n",
      "Itération n°73\n",
      "Itération n°74\n",
      "Itération n°75\n",
      "Itération n°76\n",
      "Itération n°77\n",
      "Itération n°78\n",
      "Itération n°79\n",
      "Itération n°80\n",
      "Itération n°81\n",
      "Itération n°82\n",
      "Itération n°83\n",
      "Itération n°84\n",
      "Itération n°85\n",
      "Itération n°86\n",
      "Itération n°87\n",
      "Itération n°88\n",
      "Itération n°89\n",
      "Itération n°90\n",
      "Itération n°91\n",
      "Itération n°92\n",
      "Itération n°93\n",
      "Itération n°94\n",
      "Itération n°95\n",
      "Itération n°96\n",
      "Itération n°97\n",
      "Itération n°98\n",
      "Itération n°99\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "246.34178709983826"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start = time()\n",
    "saved_weights, acceptance_rate = logistic_bayes(X_train,y_train,100);\n",
    "elapsed = time() - start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regardons l'evolution de la précision sur l'ensemble de Test selon chaque itération."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "accuracy"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "accuracy: Proportion de bonnes réponses\n",
    "\"\"\"\n",
    "function accuracy(x, y, theta)\n",
    "    prediction = sigmoid(x*theta) .>0.5\n",
    "    return sum(prediction .== y)/size(x)[1]\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"utf-8\"?>\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"600\" height=\"400\" viewBox=\"0 0 2400 1600\">\n",
       "<defs>\n",
       "  <clipPath id=\"clip6500\">\n",
       "    <rect x=\"0\" y=\"0\" width=\"2000\" height=\"2000\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<defs>\n",
       "  <clipPath id=\"clip6501\">\n",
       "    <rect x=\"0\" y=\"0\" width=\"2400\" height=\"1600\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<polygon clip-path=\"url(#clip6501)\" points=\"\n",
       "0,1600 2400,1600 2400,0 0,0 \n",
       "  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<defs>\n",
       "  <clipPath id=\"clip6502\">\n",
       "    <rect x=\"480\" y=\"0\" width=\"1681\" height=\"1600\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<polygon clip-path=\"url(#clip6501)\" points=\"\n",
       "251.149,1440.48 2321.26,1440.48 2321.26,125.984 251.149,125.984 \n",
       "  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<defs>\n",
       "  <clipPath id=\"clip6503\">\n",
       "    <rect x=\"251\" y=\"125\" width=\"2071\" height=\"1315\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<polyline clip-path=\"url(#clip6503)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  290.01,1440.48 290.01,125.984 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip6503)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  783.175,1440.48 783.175,125.984 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip6503)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  1276.34,1440.48 1276.34,125.984 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip6503)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  1769.51,1440.48 1769.51,125.984 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip6503)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  2262.67,1440.48 2262.67,125.984 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip6503)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  251.149,1344.81 2321.26,1344.81 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip6503)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  251.149,1114.96 2321.26,1114.96 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip6503)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  251.149,885.109 2321.26,885.109 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip6503)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  251.149,655.261 2321.26,655.261 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip6503)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  251.149,425.412 2321.26,425.412 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip6503)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  251.149,195.563 2321.26,195.563 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip6501)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  251.149,1440.48 2321.26,1440.48 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip6501)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  251.149,1440.48 251.149,125.984 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip6501)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  290.01,1440.48 290.01,1420.77 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip6501)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  783.175,1440.48 783.175,1420.77 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip6501)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1276.34,1440.48 1276.34,1420.77 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip6501)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1769.51,1440.48 1769.51,1420.77 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip6501)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  2262.67,1440.48 2262.67,1420.77 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip6501)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  251.149,1344.81 282.2,1344.81 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip6501)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  251.149,1114.96 282.2,1114.96 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip6501)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  251.149,885.109 282.2,885.109 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip6501)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  251.149,655.261 282.2,655.261 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip6501)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  251.149,425.412 282.2,425.412 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip6501)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  251.149,195.563 282.2,195.563 \n",
       "  \"/>\n",
       "<g clip-path=\"url(#clip6501)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:middle;\" transform=\"rotate(0, 290.01, 1494.48)\" x=\"290.01\" y=\"1494.48\">0</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip6501)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:middle;\" transform=\"rotate(0, 783.175, 1494.48)\" x=\"783.175\" y=\"1494.48\">25</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip6501)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:middle;\" transform=\"rotate(0, 1276.34, 1494.48)\" x=\"1276.34\" y=\"1494.48\">50</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip6501)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:middle;\" transform=\"rotate(0, 1769.51, 1494.48)\" x=\"1769.51\" y=\"1494.48\">75</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip6501)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:middle;\" transform=\"rotate(0, 2262.67, 1494.48)\" x=\"2262.67\" y=\"1494.48\">100</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip6501)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:end;\" transform=\"rotate(0, 227.149, 1362.31)\" x=\"227.149\" y=\"1362.31\">0.70</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip6501)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:end;\" transform=\"rotate(0, 227.149, 1132.46)\" x=\"227.149\" y=\"1132.46\">0.71</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip6501)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:end;\" transform=\"rotate(0, 227.149, 902.609)\" x=\"227.149\" y=\"902.609\">0.72</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip6501)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:end;\" transform=\"rotate(0, 227.149, 672.761)\" x=\"227.149\" y=\"672.761\">0.73</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip6501)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:end;\" transform=\"rotate(0, 227.149, 442.912)\" x=\"227.149\" y=\"442.912\">0.74</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip6501)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:end;\" transform=\"rotate(0, 227.149, 213.063)\" x=\"227.149\" y=\"213.063\">0.75</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip6501)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:84px; text-anchor:middle;\" transform=\"rotate(0, 1286.2, 73.2)\" x=\"1286.2\" y=\"73.2\">Accuracy evolution</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip6501)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:66px; text-anchor:middle;\" transform=\"rotate(0, 1286.2, 1590.4)\" x=\"1286.2\" y=\"1590.4\">Number of iterations</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip6501)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:66px; text-anchor:middle;\" transform=\"rotate(-90, 57.6, 783.233)\" x=\"57.6\" y=\"783.233\">Accuracy</text>\n",
       "</g>\n",
       "<polyline clip-path=\"url(#clip6503)\" style=\"stroke:#009af9; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  309.737,237.75 329.463,163.187 349.19,214.204 368.916,194.582 388.643,335.858 408.37,312.312 428.096,190.657 447.823,335.858 467.55,194.582 487.276,692.974 \n",
       "  507.003,398.648 526.729,496.756 546.456,728.293 566.183,948.056 585.909,951.98 605.636,951.98 625.362,951.98 645.089,1003 664.816,991.224 684.542,955.905 \n",
       "  704.269,1050.09 723.996,1183.52 743.722,1297.32 763.449,963.753 783.175,1085.41 802.902,1285.55 822.629,1171.74 842.355,1316.94 862.082,1156.05 881.808,1403.28 \n",
       "  901.535,1324.79 921.262,1124.65 940.988,1199.21 960.715,944.132 980.442,716.52 1000.17,944.132 1019.89,963.753 1039.62,857.796 1059.35,822.477 1079.07,767.536 \n",
       "  1098.8,818.553 1118.53,783.233 1138.25,712.595 1157.98,689.049 1177.71,638.033 1197.43,645.881 1217.16,889.191 1236.89,963.753 1256.61,810.704 1276.34,590.941 \n",
       "  1296.07,681.2 1315.79,641.957 1335.52,575.243 1355.25,677.276 1374.97,673.352 1394.7,571.319 1414.43,575.243 1434.15,724.368 1453.88,614.487 1473.61,441.815 \n",
       "  1493.33,182.809 1513.06,171.036 1532.79,194.582 1552.51,308.388 1572.24,363.329 1591.97,280.917 1611.69,280.917 1631.42,308.388 1651.15,359.404 1670.87,437.891 \n",
       "  1690.6,661.579 1710.33,743.99 1730.05,893.115 1749.78,1144.27 1769.51,1018.69 1789.23,826.401 1808.96,1038.32 1828.69,1026.54 1848.41,1132.5 1868.14,1269.85 \n",
       "  1887.87,1375.81 1907.59,1234.53 1927.32,979.451 1947.05,912.737 1966.77,857.796 1986.5,622.335 2006.23,520.302 2025.95,575.243 2045.68,618.411 2065.41,645.881 \n",
       "  2085.13,622.335 2104.86,783.233 2124.59,751.839 2144.31,555.621 2164.04,606.638 2183.77,567.394 2203.49,504.605 2223.22,398.648 2242.95,626.26 2262.67,787.158 \n",
       "  \n",
       "  \"/>\n",
       "<polygon clip-path=\"url(#clip6501)\" points=\"\n",
       "1811.33,330.464 2249.26,330.464 2249.26,209.504 1811.33,209.504 \n",
       "  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip6501)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1811.33,330.464 2249.26,330.464 2249.26,209.504 1811.33,209.504 1811.33,330.464 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip6501)\" style=\"stroke:#009af9; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1835.33,269.984 1979.33,269.984 \n",
       "  \"/>\n",
       "<g clip-path=\"url(#clip6501)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:start;\" transform=\"rotate(0, 2003.33, 287.484)\" x=\"2003.33\" y=\"287.484\">Accuracy</text>\n",
       "</g>\n",
       "</svg>\n"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l = [accuracy(X_test,y_test,saved_weights[i,:]) for i in 1:100]\n",
    "Plots.plot(l, label=\"Accuracy\", title=\"Accuracy evolution\")\n",
    "xlabel!(\"Number of iterations\")\n",
    "ylabel!(\"Accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Afin d'avoir le meilleur taux d'erreur quand nous testerons sur d'autres base de données, nous allons prendre le jeu de poids correspondant à la plus grande précision sur l'ensemble de Test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A l'itération n°2:\n",
      "Nous avons obtenu la meilleur précision: 0.7514085709407546"
     ]
    }
   ],
   "source": [
    "print(\"A l'itération n°\",findmax(l)[2],\":\\n\",\n",
    "    \"Nous avons obtenu la meilleur précision: \",findmax(l)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta = saved_weights[findmax(l)[2],:];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "predict (generic function with 1 method)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function predict(x, theta)\n",
    "    \"\"\"\n",
    "    predict: prédire si le mail (x) constitue un spam ou un ham\n",
    "    params: x: constitue le mail\n",
    "            theta: le vecteur des poids\n",
    "    \"\"\"\n",
    "    # Si x est un vecteur (un seul email)\n",
    "    if typeof(x) == Array{Float64,1}\n",
    "        y = x'*theta\n",
    "        if sigmoid(y)>0.5\n",
    "            print(\"C'est un Spam !\")\n",
    "        else\n",
    "            print(\"C'est un Ham !\")\n",
    "        end\n",
    "        return\n",
    "    # Si x est constitué de plusieurs mails\n",
    "    else\n",
    "        y =  x * theta\n",
    "        l = []\n",
    "        for i in 1:length(y)\n",
    "            # Si proba>0.5 alors on décide que c'est un Spam\n",
    "            if (sigmoid(y).>0.5)[i] == true\n",
    "                append!(l, [\"Spam\"])\n",
    "            # Si proba<0.5 alors on décide que c'est un Spam\n",
    "            else\n",
    "                append!(l,[\"Ham\"])\n",
    "            end\n",
    "        end\n",
    "    end   \n",
    "    \n",
    "    return l\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Essayons maintenant de prédire avec un exemple si c'est un Spam ou un Ham !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C'est un Spam !"
     ]
    }
   ],
   "source": [
    "predict(X_train[1496,:], theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[1496]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On a donc une bonne prédiction !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour faciliter la tâche, et être plus transparent, nous créons la fonction suivante, qui prédit à partir du chemin du fichier, si c'est un spam ou ham."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "predict_from_file (generic function with 1 method)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function predict_from_file(file_path,theta)\n",
    "    \n",
    "    x =zeros(length(train_words))\n",
    "    try\n",
    "        global words = parseEmail(file_path)\n",
    "    catch\n",
    "        println(file_path,\" contient des caractères non traitées !\")\n",
    "    end\n",
    "    for i in 1:length(train_words)\n",
    "        if train_words[i] in words\n",
    "            x[i] = 1\n",
    "        end \n",
    "    end\n",
    "    print(file_path,\" --> \")\n",
    "    predict(vcat(1,x),theta)\n",
    "    print(\"\\n\")\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./enron2/ham/0003.1999-12-10.kaminski.ham.txt --> C'est un Ham !\n"
     ]
    }
   ],
   "source": [
    "predict_from_file(\"./enron2/ham/0003.1999-12-10.kaminski.ham.txt\", theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### III.3. Descente de gradient$^{(7)}$: <a id = \"unit3.3\" > </a > "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans cette partie, nous allons voir la méthode de descente de gradient. \n",
    "\n",
    "La descente de gradient est un algorithme qui permet trouver la solution optimale d'un certains nombre de problèmes. Le principe est le suivant: on définit une **fonction de coût J**  qui caractérise le problème.\n",
    "Cette fonction dépend d'un ensemble de **paramètres $\\theta$ **. La descente de gradient cherche à **minimiser** la fonction de coût en **modifiant itérativement** les paramètres.\n",
    "\n",
    "#### Gradient\n",
    "\n",
    "Le gradient de la fonction de coûts pour un $\\theta$ donné, correspond à la direction dans laquelle il faut modifier $\\theta$ pour réduire la valeur de la fonction de coût. \n",
    "\n",
    "La fonction de coût est minimale quand le gradient est nul.\n",
    "\n",
    "Concrètement, on initialize $\\theta$ aléatoirement, et on effectue à chaque itération un pas pour réduire la fonction de coût jusqu'à convergence de l'algorithme à un minimum.\n",
    "\n",
    "#### Learning rate\n",
    "\n",
    "Le taux d'apprentissage correspond à la taille du pas que l'on va effectuer dans la direction du gradient.\n",
    "Plus il est grand, plus la convergence est rapide mais il y a un risque que l'algorithme diverge.\n",
    "\n",
    "Plus il est petit, plus la convergence est lente.\n",
    "\n",
    "#### Epoch\n",
    "\n",
    "Il s'agit de combien on a besoin d'exemple du train set avant de faire la descente de gradient, soit une unique mise à jour de gradient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### La prédiction doit être entre 0 et 1.\n",
    "\n",
    "Pour cela, nous faisons introduire la fonction Sigmoid de la forme suivante:\n",
    "\n",
    "$$ P(Y^{(k)}=1| X^{(k)}) = \\hat{p}^{(k)} = sigmoid(output_k) = \\frac{exp(output_k)}{1 + exp(output_k)} $$\n",
    "\n",
    "où n est le nombre de catégorie qu'on a, n=2 dans notre cas car nous avons les deux catégories Spam/Ham."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sigmoid"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "sigmoid: calule la sigmoid de la sortie pour avoir une nouvelle sortie entre 0 et 1\n",
    "params: x: considéré dans notre cas comme la sortie\n",
    "\"\"\"\n",
    "function sigmoid(output)\n",
    "    if typeof(output) == Float64\n",
    "        return exp(output)/(1+exp(output))\n",
    "    end\n",
    "    l = zeros(length(output))\n",
    "    for i in 1:size(output)[1]\n",
    "        l[i] = exp(output[i])/(1+exp(output[i]))\n",
    "    end\n",
    "    return l\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour appliquer l'algorithme de descente de gradient, nous avons besoin d'une fontion de coût que nous allons essayer de minimiser. Cette fonction de coût est très importante, elle est induite du logarithme du maximum de vraisemblance.\n",
    "\n",
    "$$ J( \\Theta) = -\\frac{1}{m}\\sum_{\\substack{1<i<m}} y^{(i)} log( \\hat{p}^{(i)} ) + (1-y^{(i)}) log(1 - \\hat{p}^{(i)}) $$\n",
    "\n",
    "Où $m$ est le nombre d'exemples dans l'ensemble de train.\n",
    "\n",
    "Un bon exercice serait de la démontrer étape par étape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "loss"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "loss: Calculer le coût sur l'ensemble donné\n",
    "params: X: La matrice d'entrée\n",
    "        y: Le vecteur sortie\n",
    "        theta: le vecteur des poids\n",
    "\"\"\"\n",
    "function loss(x, y, theta)\n",
    "    err = 0\n",
    "    output = sigmoid(x*theta)\n",
    "    for i in 1:size(y)[1]\n",
    "        err += y[i] * log(output[i]) + (1-y[i]) * log(1-output[i])\n",
    "    end\n",
    "    return -err/size(y)[1]\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensuite, nous souhaitons minimiser cette fonction de coût par rapport au vecteur des poids.\n",
    "\n",
    "Pour cela nous devons, pour chaque poids, aller contre la direction de la descente. \n",
    "\n",
    "La direction de la descente est le vecteur des dérivées de la fonction de coût par rapport à chaque poids:\n",
    "\n",
    "$$ \\Delta_{\\theta}J( \\Theta) = \\frac{1}{m} \\sum_{\\substack{1<i<m}}( \\hat{p}^{(i)} - y^{(i)})x^{(i)}  $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "gradient"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "gradient: calcule le gradient de la fonction de coût par rapport à theta\n",
    "\"\"\"\n",
    "function gradient(x, y, theta)\n",
    "    grad = zeros(size(theta))\n",
    "    output = sigmoid(x*theta)\n",
    "    for i in 1:size(y)[1]\n",
    "        grad = grad + (output[i] - y[i]) * x[i,:]\n",
    "    end\n",
    "    return grad/size(x)[1]\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maintenant que le gradient est calculé, nous devons commencer la partie d'entrainement qui consiste à appliquer la descente de gradient à chaque fois après avoir parcouru tout l'ensemble d'entrainement. Nous allons arrêter l'algorithme si l'un des deux cas est vérifié:\n",
    "\n",
    "- La fonction de coût est plus petite qu'un seuil qu'on choisit.\n",
    "\n",
    "- On atteint le nombre maximale d'itérations  qu'on choisit sur l'ensemble d'entrainement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "train"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "train: Phase d'entrainement\n",
    "params: x_train/test: input\n",
    "        y_train/test: output\n",
    "        theta: vecteur de poids\n",
    "        max_iter: Le nombre maximale d'itérations sur la base d'entrainement qu'on souhaite atteindre\n",
    "        learning_rate: taux d'apprentissage correspond à la taille du pas que l'on prendre\n",
    "                       effectuer dans la direction du gradient\n",
    "\"\"\"\n",
    "function train(x_train, y_train, x_test, y_test, max_iter = 1000, learning_rate=0.05)\n",
    "    theta = rand(1:10,size(x_test)[2])/100;\n",
    "    err_train = 10000\n",
    "    train_loss_evolution = []\n",
    "    test_loss_evolution = []\n",
    "    train_accuracy_evolution = []\n",
    "    test_accuracy_evolution = []\n",
    "    i = 0\n",
    "    while err_train > 0.005 && i < max_iter\n",
    "        # Mise à jour des poids\n",
    "        theta = theta - learning_rate * gradient(x_train, y_train, theta)\n",
    "        # Calcul de la fonction de coût sur l'ensemble d'entrainement\n",
    "        err_train = loss(x_train, y_train, theta)\n",
    "        # Calcul de la fonction de coût sur l'ensemble de test\n",
    "        err_test = loss(x_test, y_test, theta)\n",
    "        append!(train_loss_evolution, err_train)\n",
    "        append!(test_loss_evolution, err_test)\n",
    "        # La précision sur l'ensemble d'entrainement\n",
    "        append!(train_accuracy_evolution, accuracy(x_train, y_train, theta))\n",
    "        # La précision sur l'ensemble de test\n",
    "        append!(test_accuracy_evolution, accuracy(x_test, y_test, theta))\n",
    "        if (i-1)%10 ==0\n",
    "            println(\"Episode $i\")\n",
    "            println(\"cost: $err_train\")\n",
    "        end\n",
    "        i += 1\n",
    "    end\n",
    "    return theta, train_loss_evolution, test_loss_evolution, train_accuracy_evolution, test_accuracy_evolution\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous effectuons l'entrainement avec 500 itérations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1\n",
      "cost: 1.1592199982990867\n",
      "Episode 11\n",
      "cost: 0.5735843947751186\n",
      "Episode 21\n",
      "cost: 0.5069464343998039\n",
      "Episode 31\n",
      "cost: 0.4860157154451163\n",
      "Episode 41\n",
      "cost: 0.47061393188875417\n",
      "Episode 51\n",
      "cost: 0.4570568818585751\n",
      "Episode 61\n",
      "cost: 0.4447851676039225\n",
      "Episode 71\n",
      "cost: 0.43360304222758905\n",
      "Episode 81\n",
      "cost: 0.423375619466743\n",
      "Episode 91\n",
      "cost: 0.41399025167318976\n",
      "Episode 101\n",
      "cost: 0.40534992086840377\n",
      "Episode 111\n",
      "cost: 0.39737072138528673\n",
      "Episode 121\n",
      "cost: 0.38997999432499497\n",
      "Episode 131\n",
      "cost: 0.38311471794016316\n",
      "Episode 141\n",
      "cost: 0.37672011142566636\n",
      "Episode 151\n",
      "cost: 0.37074843611990765\n",
      "Episode 161\n",
      "cost: 0.36515797403978356\n",
      "Episode 171\n",
      "cost: 0.35991216135544823\n",
      "Episode 181\n",
      "cost: 0.354978854891843\n",
      "Episode 191\n",
      "cost: 0.3503297116371167\n",
      "Episode 201\n",
      "cost: 0.34593966363909984\n",
      "Episode 211\n",
      "cost: 0.3417864731212812\n",
      "Episode 221\n",
      "cost: 0.3378503549357419\n",
      "Episode 231\n",
      "cost: 0.33411365550596506\n",
      "Episode 241\n",
      "cost: 0.33056057917511106\n",
      "Episode 251\n",
      "cost: 0.3271769543760052\n",
      "Episode 261\n",
      "cost: 0.32395003330253036\n",
      "Episode 271\n",
      "cost: 0.32086831981746966\n",
      "Episode 281\n",
      "cost: 0.317921421210095\n",
      "Episode 291\n",
      "cost: 0.3150999201447623\n",
      "Episode 301\n",
      "cost: 0.3123952637453928\n",
      "Episode 311\n",
      "cost: 0.3097996672595695\n",
      "Episode 321\n",
      "cost: 0.30730603015989516\n",
      "Episode 331\n",
      "cost: 0.3049078628828498\n",
      "Episode 341\n",
      "cost: 0.3025992226897614\n",
      "Episode 351\n",
      "cost: 0.3003746573710847\n",
      "Episode 361\n",
      "cost: 0.2982291557119967\n",
      "Episode 371\n",
      "cost: 0.2961581038015257\n",
      "Episode 381\n",
      "cost: 0.2941572464049993\n",
      "Episode 391\n",
      "cost: 0.29222265273466175\n",
      "Episode 401\n",
      "cost: 0.290350686049904\n",
      "Episode 411\n",
      "cost: 0.2885379766002936\n",
      "Episode 421\n",
      "cost: 0.28678139749308823\n",
      "Episode 431\n",
      "cost: 0.28507804312528034\n",
      "Episode 441\n",
      "cost: 0.283425209869379\n",
      "Episode 451\n",
      "cost: 0.28182037874426247\n",
      "Episode 461\n",
      "cost: 0.2802611998381265\n",
      "Episode 471\n",
      "cost: 0.27874547828095436\n",
      "Episode 481\n",
      "cost: 0.27727116159036613\n",
      "Episode 491\n",
      "cost: 0.27583632823681875\n"
     ]
    }
   ],
   "source": [
    "theta, tr_loss_evol, te_loss_evol, tr_accuracy_evol, te_accuracy_evol = train(X_train, y_train, X_test, y_test, 500);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous visualisons l'évolution de la fonction de coût sur l'ensemble d'entrainement et de test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"utf-8\"?>\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"600\" height=\"400\" viewBox=\"0 0 2400 1600\">\n",
       "<defs>\n",
       "  <clipPath id=\"clip6700\">\n",
       "    <rect x=\"0\" y=\"0\" width=\"2000\" height=\"2000\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<defs>\n",
       "  <clipPath id=\"clip6701\">\n",
       "    <rect x=\"0\" y=\"0\" width=\"2400\" height=\"1600\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<polygon clip-path=\"url(#clip6701)\" points=\"\n",
       "0,1600 2400,1600 2400,0 0,0 \n",
       "  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<defs>\n",
       "  <clipPath id=\"clip6702\">\n",
       "    <rect x=\"480\" y=\"0\" width=\"1681\" height=\"1600\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<polygon clip-path=\"url(#clip6701)\" points=\"\n",
       "224.386,1440.48 2321.26,1440.48 2321.26,125.984 224.386,125.984 \n",
       "  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<defs>\n",
       "  <clipPath id=\"clip6703\">\n",
       "    <rect x=\"224\" y=\"125\" width=\"2098\" height=\"1315\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<polyline clip-path=\"url(#clip6703)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  279.767,1440.48 279.767,125.984 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip6703)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  676.197,1440.48 676.197,125.984 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip6703)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  1072.63,1440.48 1072.63,125.984 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip6703)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  1469.06,1440.48 1469.06,125.984 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip6703)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  1865.48,1440.48 1865.48,125.984 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip6703)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  2261.91,1440.48 2261.91,125.984 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip6703)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  224.386,1307.96 2321.26,1307.96 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip6703)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  224.386,1155.78 2321.26,1155.78 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip6703)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  224.386,1003.61 2321.26,1003.61 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip6703)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  224.386,851.44 2321.26,851.44 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip6703)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  224.386,699.268 2321.26,699.268 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip6703)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  224.386,547.096 2321.26,547.096 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip6703)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  224.386,394.924 2321.26,394.924 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip6703)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  224.386,242.752 2321.26,242.752 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip6701)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  224.386,1440.48 2321.26,1440.48 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip6701)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  224.386,1440.48 224.386,125.984 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip6701)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  279.767,1440.48 279.767,1420.77 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip6701)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  676.197,1440.48 676.197,1420.77 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip6701)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1072.63,1440.48 1072.63,1420.77 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip6701)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1469.06,1440.48 1469.06,1420.77 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip6701)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1865.48,1440.48 1865.48,1420.77 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip6701)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  2261.91,1440.48 2261.91,1420.77 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip6701)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  224.386,1307.96 255.839,1307.96 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip6701)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  224.386,1155.78 255.839,1155.78 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip6701)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  224.386,1003.61 255.839,1003.61 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip6701)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  224.386,851.44 255.839,851.44 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip6701)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  224.386,699.268 255.839,699.268 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip6701)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  224.386,547.096 255.839,547.096 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip6701)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  224.386,394.924 255.839,394.924 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip6701)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  224.386,242.752 255.839,242.752 \n",
       "  \"/>\n",
       "<g clip-path=\"url(#clip6701)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:middle;\" transform=\"rotate(0, 279.767, 1494.48)\" x=\"279.767\" y=\"1494.48\">0</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip6701)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:middle;\" transform=\"rotate(0, 676.197, 1494.48)\" x=\"676.197\" y=\"1494.48\">100</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip6701)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:middle;\" transform=\"rotate(0, 1072.63, 1494.48)\" x=\"1072.63\" y=\"1494.48\">200</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip6701)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:middle;\" transform=\"rotate(0, 1469.06, 1494.48)\" x=\"1469.06\" y=\"1494.48\">300</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip6701)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:middle;\" transform=\"rotate(0, 1865.48, 1494.48)\" x=\"1865.48\" y=\"1494.48\">400</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip6701)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:middle;\" transform=\"rotate(0, 2261.91, 1494.48)\" x=\"2261.91\" y=\"1494.48\">500</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip6701)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:end;\" transform=\"rotate(0, 200.386, 1325.46)\" x=\"200.386\" y=\"1325.46\">0.4</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip6701)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:end;\" transform=\"rotate(0, 200.386, 1173.28)\" x=\"200.386\" y=\"1173.28\">0.6</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip6701)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:end;\" transform=\"rotate(0, 200.386, 1021.11)\" x=\"200.386\" y=\"1021.11\">0.8</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip6701)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:end;\" transform=\"rotate(0, 200.386, 868.94)\" x=\"200.386\" y=\"868.94\">1.0</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip6701)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:end;\" transform=\"rotate(0, 200.386, 716.768)\" x=\"200.386\" y=\"716.768\">1.2</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip6701)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:end;\" transform=\"rotate(0, 200.386, 564.596)\" x=\"200.386\" y=\"564.596\">1.4</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip6701)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:end;\" transform=\"rotate(0, 200.386, 412.424)\" x=\"200.386\" y=\"412.424\">1.6</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip6701)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:end;\" transform=\"rotate(0, 200.386, 260.252)\" x=\"200.386\" y=\"260.252\">1.8</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip6701)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:84px; text-anchor:middle;\" transform=\"rotate(0, 1272.82, 73.2)\" x=\"1272.82\" y=\"73.2\">Loss evolution</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip6701)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:66px; text-anchor:middle;\" transform=\"rotate(0, 1272.82, 1590.4)\" x=\"1272.82\" y=\"1590.4\">Number of epochs</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip6701)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:66px; text-anchor:middle;\" transform=\"rotate(-90, 57.6, 783.233)\" x=\"57.6\" y=\"783.233\">Loss</text>\n",
       "</g>\n",
       "<polyline clip-path=\"url(#clip6703)\" style=\"stroke:#009af9; stroke-width:12; stroke-opacity:1; fill:none\" points=\"\n",
       "  283.732,628.54 287.696,730.296 291.66,820.366 295.625,898.244 299.589,963.976 303.553,1018.19 307.517,1062.01 311.482,1096.88 315.446,1124.33 319.41,1145.82 \n",
       "  323.375,1162.65 327.339,1175.88 331.303,1186.35 335.267,1194.72 339.232,1201.49 343.196,1207.03 347.16,1211.64 351.125,1215.52 355.089,1218.85 359.053,1221.74 \n",
       "  363.018,1224.3 366.982,1226.58 370.946,1228.66 374.91,1230.56 378.875,1232.33 382.839,1233.99 386.803,1235.56 390.768,1237.06 394.732,1238.49 398.696,1239.87 \n",
       "  402.66,1241.21 406.625,1242.51 410.589,1243.78 414.553,1245.02 418.518,1246.24 422.482,1247.43 426.446,1248.6 430.411,1249.76 434.375,1250.9 438.339,1252.02 \n",
       "  442.303,1253.13 446.268,1254.23 450.232,1255.31 454.196,1256.38 458.161,1257.44 462.125,1258.49 466.089,1259.52 470.053,1260.55 474.018,1261.56 477.982,1262.57 \n",
       "  481.946,1263.56 485.911,1264.54 489.875,1265.52 493.839,1266.48 497.804,1267.44 501.768,1268.39 505.732,1269.32 509.696,1270.25 513.661,1271.17 517.625,1272.08 \n",
       "  521.589,1272.99 525.554,1273.88 529.518,1274.77 533.482,1275.65 537.446,1276.52 541.411,1277.38 545.375,1278.23 549.339,1279.08 553.304,1279.92 557.268,1280.75 \n",
       "  561.232,1281.57 565.197,1282.39 569.161,1283.2 573.125,1284 577.089,1284.8 581.054,1285.58 585.018,1286.36 588.982,1287.14 592.947,1287.91 596.911,1288.67 \n",
       "  600.875,1289.42 604.839,1290.17 608.804,1290.91 612.768,1291.65 616.732,1292.38 620.697,1293.1 624.661,1293.82 628.625,1294.53 632.59,1295.23 636.554,1295.93 \n",
       "  640.518,1296.62 644.482,1297.31 648.447,1297.99 652.411,1298.67 656.375,1299.34 660.34,1300.01 664.304,1300.67 668.268,1301.32 672.232,1301.97 676.197,1302.61 \n",
       "  680.161,1303.25 684.125,1303.89 688.09,1304.51 692.054,1305.14 696.018,1305.76 699.983,1306.37 703.947,1306.98 707.911,1307.58 711.875,1308.18 715.84,1308.78 \n",
       "  719.804,1309.37 723.768,1309.96 727.733,1310.54 731.697,1311.12 735.661,1311.69 739.625,1312.26 743.59,1312.82 747.554,1313.38 751.518,1313.94 755.483,1314.49 \n",
       "  759.447,1315.04 763.411,1315.58 767.376,1316.12 771.34,1316.66 775.304,1317.19 779.268,1317.71 783.233,1318.24 787.197,1318.76 791.161,1319.28 795.126,1319.79 \n",
       "  799.09,1320.3 803.054,1320.8 807.018,1321.31 810.983,1321.8 814.947,1322.3 818.911,1322.79 822.876,1323.28 826.84,1323.76 830.804,1324.24 834.769,1324.72 \n",
       "  838.733,1325.2 842.697,1325.67 846.661,1326.14 850.626,1326.6 854.59,1327.06 858.554,1327.52 862.519,1327.98 866.483,1328.43 870.447,1328.88 874.411,1329.33 \n",
       "  878.376,1329.77 882.34,1330.21 886.304,1330.65 890.269,1331.09 894.233,1331.52 898.197,1331.95 902.162,1332.37 906.126,1332.8 910.09,1333.22 914.054,1333.64 \n",
       "  918.019,1334.05 921.983,1334.47 925.947,1334.88 929.912,1335.28 933.876,1335.69 937.84,1336.09 941.804,1336.49 945.769,1336.89 949.733,1337.29 953.697,1337.68 \n",
       "  957.662,1338.07 961.626,1338.46 965.59,1338.84 969.555,1339.23 973.519,1339.61 977.483,1339.99 981.447,1340.36 985.412,1340.74 989.376,1341.11 993.34,1341.48 \n",
       "  997.305,1341.85 1001.27,1342.21 1005.23,1342.57 1009.2,1342.93 1013.16,1343.29 1017.13,1343.65 1021.09,1344.01 1025.05,1344.36 1029.02,1344.71 1032.98,1345.06 \n",
       "  1036.95,1345.4 1040.91,1345.75 1044.88,1346.09 1048.84,1346.43 1052.8,1346.77 1056.77,1347.11 1060.73,1347.44 1064.7,1347.77 1068.66,1348.11 1072.63,1348.44 \n",
       "  1076.59,1348.76 1080.55,1349.09 1084.52,1349.41 1088.48,1349.73 1092.45,1350.05 1096.41,1350.37 1100.38,1350.69 1104.34,1351 1108.3,1351.32 1112.27,1351.63 \n",
       "  1116.23,1351.94 1120.2,1352.25 1124.16,1352.55 1128.13,1352.86 1132.09,1353.16 1136.05,1353.47 1140.02,1353.77 1143.98,1354.06 1147.95,1354.36 1151.91,1354.66 \n",
       "  1155.88,1354.95 1159.84,1355.24 1163.8,1355.53 1167.77,1355.82 1171.73,1356.11 1175.7,1356.4 1179.66,1356.68 1183.63,1356.97 1187.59,1357.25 1191.55,1357.53 \n",
       "  1195.52,1357.81 1199.48,1358.09 1203.45,1358.36 1207.41,1358.64 1211.38,1358.91 1215.34,1359.18 1219.31,1359.45 1223.27,1359.72 1227.23,1359.99 1231.2,1360.26 \n",
       "  1235.16,1360.53 1239.13,1360.79 1243.09,1361.05 1247.06,1361.31 1251.02,1361.58 1254.98,1361.83 1258.95,1362.09 1262.91,1362.35 1266.88,1362.6 1270.84,1362.86 \n",
       "  1274.81,1363.11 1278.77,1363.36 1282.73,1363.61 1286.7,1363.86 1290.66,1364.11 1294.63,1364.36 1298.59,1364.61 1302.56,1364.85 1306.52,1365.09 1310.48,1365.34 \n",
       "  1314.45,1365.58 1318.41,1365.82 1322.38,1366.06 1326.34,1366.3 1330.31,1366.53 1334.27,1366.77 1338.23,1367.01 1342.2,1367.24 1346.16,1367.47 1350.13,1367.7 \n",
       "  1354.09,1367.93 1358.06,1368.16 1362.02,1368.39 1365.98,1368.62 1369.95,1368.85 1373.91,1369.07 1377.88,1369.3 1381.84,1369.52 1385.81,1369.74 1389.77,1369.97 \n",
       "  1393.73,1370.19 1397.7,1370.41 1401.66,1370.63 1405.63,1370.84 1409.59,1371.06 1413.56,1371.28 1417.52,1371.49 1421.48,1371.71 1425.45,1371.92 1429.41,1372.13 \n",
       "  1433.38,1372.34 1437.34,1372.55 1441.31,1372.76 1445.27,1372.97 1449.23,1373.18 1453.2,1373.39 1457.16,1373.59 1461.13,1373.8 1465.09,1374 1469.06,1374.21 \n",
       "  1473.02,1374.41 1476.98,1374.61 1480.95,1374.81 1484.91,1375.01 1488.88,1375.21 1492.84,1375.41 1496.81,1375.61 1500.77,1375.81 1504.73,1376 1508.7,1376.2 \n",
       "  1512.66,1376.39 1516.63,1376.59 1520.59,1376.78 1524.56,1376.97 1528.52,1377.16 1532.48,1377.35 1536.45,1377.54 1540.41,1377.73 1544.38,1377.92 1548.34,1378.11 \n",
       "  1552.31,1378.3 1556.27,1378.48 1560.23,1378.67 1564.2,1378.85 1568.16,1379.04 1572.13,1379.22 1576.09,1379.4 1580.06,1379.59 1584.02,1379.77 1587.98,1379.95 \n",
       "  1591.95,1380.13 1595.91,1380.31 1599.88,1380.49 1603.84,1380.66 1607.81,1380.84 1611.77,1381.02 1615.73,1381.19 1619.7,1381.37 1623.66,1381.54 1627.63,1381.72 \n",
       "  1631.59,1381.89 1635.56,1382.06 1639.52,1382.24 1643.48,1382.41 1647.45,1382.58 1651.41,1382.75 1655.38,1382.92 1659.34,1383.09 1663.31,1383.26 1667.27,1383.42 \n",
       "  1671.23,1383.59 1675.2,1383.76 1679.16,1383.92 1683.13,1384.09 1687.09,1384.25 1691.06,1384.42 1695.02,1384.58 1698.98,1384.74 1702.95,1384.91 1706.91,1385.07 \n",
       "  1710.88,1385.23 1714.84,1385.39 1718.81,1385.55 1722.77,1385.71 1726.73,1385.87 1730.7,1386.03 1734.66,1386.18 1738.63,1386.34 1742.59,1386.5 1746.56,1386.65 \n",
       "  1750.52,1386.81 1754.48,1386.97 1758.45,1387.12 1762.41,1387.27 1766.38,1387.43 1770.34,1387.58 1774.31,1387.73 1778.27,1387.88 1782.23,1388.04 1786.2,1388.19 \n",
       "  1790.16,1388.34 1794.13,1388.49 1798.09,1388.64 1802.06,1388.79 1806.02,1388.93 1809.98,1389.08 1813.95,1389.23 1817.91,1389.38 1821.88,1389.52 1825.84,1389.67 \n",
       "  1829.81,1389.81 1833.77,1389.96 1837.73,1390.1 1841.7,1390.25 1845.66,1390.39 1849.63,1390.53 1853.59,1390.68 1857.56,1390.82 1861.52,1390.96 1865.48,1391.1 \n",
       "  1869.45,1391.24 1873.41,1391.38 1877.38,1391.52 1881.34,1391.66 1885.31,1391.8 1889.27,1391.94 1893.24,1392.08 1897.2,1392.22 1901.16,1392.35 1905.13,1392.49 \n",
       "  1909.09,1392.63 1913.06,1392.76 1917.02,1392.9 1920.99,1393.03 1924.95,1393.17 1928.91,1393.3 1932.88,1393.44 1936.84,1393.57 1940.81,1393.7 1944.77,1393.84 \n",
       "  1948.74,1393.97 1952.7,1394.1 1956.66,1394.23 1960.63,1394.36 1964.59,1394.49 1968.56,1394.62 1972.52,1394.75 1976.49,1394.88 1980.45,1395.01 1984.41,1395.14 \n",
       "  1988.38,1395.27 1992.34,1395.4 1996.31,1395.52 2000.27,1395.65 2004.24,1395.78 2008.2,1395.9 2012.16,1396.03 2016.13,1396.15 2020.09,1396.28 2024.06,1396.4 \n",
       "  2028.02,1396.53 2031.99,1396.65 2035.95,1396.78 2039.91,1396.9 2043.88,1397.02 2047.84,1397.15 2051.81,1397.27 2055.77,1397.39 2059.74,1397.51 2063.7,1397.63 \n",
       "  2067.66,1397.75 2071.63,1397.87 2075.59,1397.99 2079.56,1398.11 2083.52,1398.23 2087.49,1398.35 2091.45,1398.47 2095.41,1398.59 2099.38,1398.71 2103.34,1398.83 \n",
       "  2107.31,1398.94 2111.27,1399.06 2115.24,1399.18 2119.2,1399.29 2123.16,1399.41 2127.13,1399.53 2131.09,1399.64 2135.06,1399.76 2139.02,1399.87 2142.99,1399.99 \n",
       "  2146.95,1400.1 2150.91,1400.21 2154.88,1400.33 2158.84,1400.44 2162.81,1400.55 2166.77,1400.67 2170.74,1400.78 2174.7,1400.89 2178.66,1401 2182.63,1401.11 \n",
       "  2186.59,1401.22 2190.56,1401.34 2194.52,1401.45 2198.49,1401.56 2202.45,1401.67 2206.41,1401.78 2210.38,1401.88 2214.34,1401.99 2218.31,1402.1 2222.27,1402.21 \n",
       "  2226.24,1402.32 2230.2,1402.43 2234.16,1402.53 2238.13,1402.64 2242.09,1402.75 2246.06,1402.86 2250.02,1402.96 2253.99,1403.07 2257.95,1403.17 2261.91,1403.28 \n",
       "  \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip6703)\" style=\"stroke:#e26f46; stroke-width:12; stroke-opacity:1; fill:none\" points=\"\n",
       "  283.732,163.187 287.696,325.464 291.66,471.672 295.625,600.577 299.589,711.696 303.553,805.424 307.517,882.978 311.482,946.179 315.446,997.141 319.41,1038 \n",
       "  323.375,1070.71 327.339,1096.95 331.303,1118.09 335.267,1135.24 339.232,1149.26 343.196,1160.83 347.16,1170.44 351.125,1178.52 355.089,1185.37 359.053,1191.22 \n",
       "  363.018,1196.28 366.982,1200.68 370.946,1204.55 374.91,1207.97 378.875,1211.02 382.839,1213.77 386.803,1216.26 390.768,1218.52 394.732,1220.61 398.696,1222.53 \n",
       "  402.66,1224.31 406.625,1225.97 410.589,1227.53 414.553,1229 418.518,1230.39 422.482,1231.71 426.446,1232.96 430.411,1234.16 434.375,1235.32 438.339,1236.42 \n",
       "  442.303,1237.49 446.268,1238.52 450.232,1239.52 454.196,1240.5 458.161,1241.44 462.125,1242.36 466.089,1243.25 470.053,1244.13 474.018,1244.99 477.982,1245.83 \n",
       "  481.946,1246.65 485.911,1247.46 489.875,1248.25 493.839,1249.03 497.804,1249.8 501.768,1250.56 505.732,1251.3 509.696,1252.04 513.661,1252.76 517.625,1253.48 \n",
       "  521.589,1254.19 525.554,1254.89 529.518,1255.58 533.482,1256.26 537.446,1256.94 541.411,1257.61 545.375,1258.27 549.339,1258.92 553.304,1259.57 557.268,1260.22 \n",
       "  561.232,1260.85 565.197,1261.49 569.161,1262.11 573.125,1262.73 577.089,1263.35 581.054,1263.96 585.018,1264.56 588.982,1265.16 592.947,1265.76 596.911,1266.35 \n",
       "  600.875,1266.94 604.839,1267.52 608.804,1268.1 612.768,1268.67 616.732,1269.24 620.697,1269.8 624.661,1270.36 628.625,1270.92 632.59,1271.47 636.554,1272.02 \n",
       "  640.518,1272.56 644.482,1273.1 648.447,1273.64 652.411,1274.17 656.375,1274.7 660.34,1275.23 664.304,1275.75 668.268,1276.27 672.232,1276.79 676.197,1277.3 \n",
       "  680.161,1277.81 684.125,1278.31 688.09,1278.81 692.054,1279.31 696.018,1279.8 699.983,1280.3 703.947,1280.78 707.911,1281.27 711.875,1281.75 715.84,1282.23 \n",
       "  719.804,1282.71 723.768,1283.18 727.733,1283.65 731.697,1284.12 735.661,1284.58 739.625,1285.04 743.59,1285.5 747.554,1285.95 751.518,1286.41 755.483,1286.86 \n",
       "  759.447,1287.3 763.411,1287.75 767.376,1288.19 771.34,1288.62 775.304,1289.06 779.268,1289.49 783.233,1289.92 787.197,1290.35 791.161,1290.78 795.126,1291.2 \n",
       "  799.09,1291.62 803.054,1292.04 807.018,1292.45 810.983,1292.86 814.947,1293.27 818.911,1293.68 822.876,1294.08 826.84,1294.49 830.804,1294.89 834.769,1295.28 \n",
       "  838.733,1295.68 842.697,1296.07 846.661,1296.46 850.626,1296.85 854.59,1297.24 858.554,1297.62 862.519,1298 866.483,1298.38 870.447,1298.76 874.411,1299.13 \n",
       "  878.376,1299.51 882.34,1299.88 886.304,1300.25 890.269,1300.61 894.233,1300.98 898.197,1301.34 902.162,1301.7 906.126,1302.06 910.09,1302.41 914.054,1302.77 \n",
       "  918.019,1303.12 921.983,1303.47 925.947,1303.82 929.912,1304.16 933.876,1304.51 937.84,1304.85 941.804,1305.19 945.769,1305.53 949.733,1305.87 953.697,1306.2 \n",
       "  957.662,1306.53 961.626,1306.87 965.59,1307.2 969.555,1307.52 973.519,1307.85 977.483,1308.17 981.447,1308.49 985.412,1308.81 989.376,1309.13 993.34,1309.45 \n",
       "  997.305,1309.77 1001.27,1310.08 1005.23,1310.39 1009.2,1310.7 1013.16,1311.01 1017.13,1311.32 1021.09,1311.62 1025.05,1311.93 1029.02,1312.23 1032.98,1312.53 \n",
       "  1036.95,1312.83 1040.91,1313.13 1044.88,1313.42 1048.84,1313.72 1052.8,1314.01 1056.77,1314.3 1060.73,1314.59 1064.7,1314.88 1068.66,1315.16 1072.63,1315.45 \n",
       "  1076.59,1315.73 1080.55,1316.01 1084.52,1316.3 1088.48,1316.58 1092.45,1316.85 1096.41,1317.13 1100.38,1317.4 1104.34,1317.68 1108.3,1317.95 1112.27,1318.22 \n",
       "  1116.23,1318.49 1120.2,1318.76 1124.16,1319.03 1128.13,1319.29 1132.09,1319.56 1136.05,1319.82 1140.02,1320.08 1143.98,1320.34 1147.95,1320.6 1151.91,1320.86 \n",
       "  1155.88,1321.12 1159.84,1321.37 1163.8,1321.62 1167.77,1321.88 1171.73,1322.13 1175.7,1322.38 1179.66,1322.63 1183.63,1322.88 1187.59,1323.12 1191.55,1323.37 \n",
       "  1195.52,1323.61 1199.48,1323.86 1203.45,1324.1 1207.41,1324.34 1211.38,1324.58 1215.34,1324.82 1219.31,1325.05 1223.27,1325.29 1227.23,1325.53 1231.2,1325.76 \n",
       "  1235.16,1325.99 1239.13,1326.23 1243.09,1326.46 1247.06,1326.69 1251.02,1326.91 1254.98,1327.14 1258.95,1327.37 1262.91,1327.59 1266.88,1327.82 1270.84,1328.04 \n",
       "  1274.81,1328.26 1278.77,1328.48 1282.73,1328.7 1286.7,1328.92 1290.66,1329.14 1294.63,1329.36 1298.59,1329.58 1302.56,1329.79 1306.52,1330.01 1310.48,1330.22 \n",
       "  1314.45,1330.43 1318.41,1330.64 1322.38,1330.85 1326.34,1331.06 1330.31,1331.27 1334.27,1331.48 1338.23,1331.68 1342.2,1331.89 1346.16,1332.1 1350.13,1332.3 \n",
       "  1354.09,1332.5 1358.06,1332.7 1362.02,1332.91 1365.98,1333.11 1369.95,1333.31 1373.91,1333.5 1377.88,1333.7 1381.84,1333.9 1385.81,1334.09 1389.77,1334.29 \n",
       "  1393.73,1334.48 1397.7,1334.68 1401.66,1334.87 1405.63,1335.06 1409.59,1335.25 1413.56,1335.44 1417.52,1335.63 1421.48,1335.82 1425.45,1336.01 1429.41,1336.19 \n",
       "  1433.38,1336.38 1437.34,1336.57 1441.31,1336.75 1445.27,1336.93 1449.23,1337.12 1453.2,1337.3 1457.16,1337.48 1461.13,1337.66 1465.09,1337.84 1469.06,1338.02 \n",
       "  1473.02,1338.2 1476.98,1338.38 1480.95,1338.55 1484.91,1338.73 1488.88,1338.9 1492.84,1339.08 1496.81,1339.25 1500.77,1339.43 1504.73,1339.6 1508.7,1339.77 \n",
       "  1512.66,1339.94 1516.63,1340.11 1520.59,1340.28 1524.56,1340.45 1528.52,1340.62 1532.48,1340.79 1536.45,1340.95 1540.41,1341.12 1544.38,1341.28 1548.34,1341.45 \n",
       "  1552.31,1341.61 1556.27,1341.78 1560.23,1341.94 1564.2,1342.1 1568.16,1342.26 1572.13,1342.42 1576.09,1342.58 1580.06,1342.74 1584.02,1342.9 1587.98,1343.06 \n",
       "  1591.95,1343.22 1595.91,1343.38 1599.88,1343.53 1603.84,1343.69 1607.81,1343.84 1611.77,1344 1615.73,1344.15 1619.7,1344.31 1623.66,1344.46 1627.63,1344.61 \n",
       "  1631.59,1344.76 1635.56,1344.91 1639.52,1345.06 1643.48,1345.21 1647.45,1345.36 1651.41,1345.51 1655.38,1345.66 1659.34,1345.81 1663.31,1345.95 1667.27,1346.1 \n",
       "  1671.23,1346.25 1675.2,1346.39 1679.16,1346.54 1683.13,1346.68 1687.09,1346.82 1691.06,1346.97 1695.02,1347.11 1698.98,1347.25 1702.95,1347.39 1706.91,1347.53 \n",
       "  1710.88,1347.67 1714.84,1347.81 1718.81,1347.95 1722.77,1348.09 1726.73,1348.23 1730.7,1348.37 1734.66,1348.51 1738.63,1348.64 1742.59,1348.78 1746.56,1348.91 \n",
       "  1750.52,1349.05 1754.48,1349.18 1758.45,1349.32 1762.41,1349.45 1766.38,1349.59 1770.34,1349.72 1774.31,1349.85 1778.27,1349.98 1782.23,1350.11 1786.2,1350.25 \n",
       "  1790.16,1350.38 1794.13,1350.51 1798.09,1350.63 1802.06,1350.76 1806.02,1350.89 1809.98,1351.02 1813.95,1351.15 1817.91,1351.27 1821.88,1351.4 1825.84,1351.53 \n",
       "  1829.81,1351.65 1833.77,1351.78 1837.73,1351.9 1841.7,1352.03 1845.66,1352.15 1849.63,1352.27 1853.59,1352.4 1857.56,1352.52 1861.52,1352.64 1865.48,1352.76 \n",
       "  1869.45,1352.89 1873.41,1353.01 1877.38,1353.13 1881.34,1353.25 1885.31,1353.37 1889.27,1353.49 1893.24,1353.6 1897.2,1353.72 1901.16,1353.84 1905.13,1353.96 \n",
       "  1909.09,1354.08 1913.06,1354.19 1917.02,1354.31 1920.99,1354.42 1924.95,1354.54 1928.91,1354.65 1932.88,1354.77 1936.84,1354.88 1940.81,1355 1944.77,1355.11 \n",
       "  1948.74,1355.22 1952.7,1355.34 1956.66,1355.45 1960.63,1355.56 1964.59,1355.67 1968.56,1355.78 1972.52,1355.9 1976.49,1356.01 1980.45,1356.12 1984.41,1356.23 \n",
       "  1988.38,1356.33 1992.34,1356.44 1996.31,1356.55 2000.27,1356.66 2004.24,1356.77 2008.2,1356.88 2012.16,1356.98 2016.13,1357.09 2020.09,1357.2 2024.06,1357.3 \n",
       "  2028.02,1357.41 2031.99,1357.51 2035.95,1357.62 2039.91,1357.72 2043.88,1357.83 2047.84,1357.93 2051.81,1358.04 2055.77,1358.14 2059.74,1358.24 2063.7,1358.34 \n",
       "  2067.66,1358.45 2071.63,1358.55 2075.59,1358.65 2079.56,1358.75 2083.52,1358.85 2087.49,1358.95 2091.45,1359.05 2095.41,1359.15 2099.38,1359.25 2103.34,1359.35 \n",
       "  2107.31,1359.45 2111.27,1359.55 2115.24,1359.65 2119.2,1359.75 2123.16,1359.84 2127.13,1359.94 2131.09,1360.04 2135.06,1360.14 2139.02,1360.23 2142.99,1360.33 \n",
       "  2146.95,1360.42 2150.91,1360.52 2154.88,1360.62 2158.84,1360.71 2162.81,1360.8 2166.77,1360.9 2170.74,1360.99 2174.7,1361.09 2178.66,1361.18 2182.63,1361.27 \n",
       "  2186.59,1361.37 2190.56,1361.46 2194.52,1361.55 2198.49,1361.64 2202.45,1361.74 2206.41,1361.83 2210.38,1361.92 2214.34,1362.01 2218.31,1362.1 2222.27,1362.19 \n",
       "  2226.24,1362.28 2230.2,1362.37 2234.16,1362.46 2238.13,1362.55 2242.09,1362.64 2246.06,1362.73 2250.02,1362.81 2253.99,1362.9 2257.95,1362.99 2261.91,1363.08 \n",
       "  \n",
       "  \"/>\n",
       "<polygon clip-path=\"url(#clip6701)\" points=\"\n",
       "1800.65,390.944 2249.26,390.944 2249.26,209.504 1800.65,209.504 \n",
       "  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip6701)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1800.65,390.944 2249.26,390.944 2249.26,209.504 1800.65,209.504 1800.65,390.944 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip6701)\" style=\"stroke:#009af9; stroke-width:12; stroke-opacity:1; fill:none\" points=\"\n",
       "  1824.65,269.984 1968.65,269.984 \n",
       "  \"/>\n",
       "<g clip-path=\"url(#clip6701)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:start;\" transform=\"rotate(0, 1992.65, 287.484)\" x=\"1992.65\" y=\"287.484\">Train loss</text>\n",
       "</g>\n",
       "<polyline clip-path=\"url(#clip6701)\" style=\"stroke:#e26f46; stroke-width:12; stroke-opacity:1; fill:none\" points=\"\n",
       "  1824.65,330.464 1968.65,330.464 \n",
       "  \"/>\n",
       "<g clip-path=\"url(#clip6701)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:start;\" transform=\"rotate(0, 1992.65, 347.964)\" x=\"1992.65\" y=\"347.964\">Test loss</text>\n",
       "</g>\n",
       "</svg>\n"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Plots.plot(hcat(tr_loss_evol,te_loss_evol), title=\"Loss evolution\",label=[\"Train loss\" \"Test loss\"],lw=3)\n",
    "xlabel!(\"Number of epochs\")\n",
    "ylabel!(\"Loss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous remarquons que les deux fonctions de coût descendend drastiquement dans les premières itérations, puis commence à converger."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regardons maintenant l'evolution de la précision (pourcentage de bonne réponses) sur les deux ensembles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"utf-8\"?>\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"600\" height=\"400\" viewBox=\"0 0 2400 1600\">\n",
       "<defs>\n",
       "  <clipPath id=\"clip6900\">\n",
       "    <rect x=\"0\" y=\"0\" width=\"2000\" height=\"2000\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<defs>\n",
       "  <clipPath id=\"clip6901\">\n",
       "    <rect x=\"0\" y=\"0\" width=\"2400\" height=\"1600\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<polygon clip-path=\"url(#clip6901)\" points=\"\n",
       "0,1600 2400,1600 2400,0 0,0 \n",
       "  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<defs>\n",
       "  <clipPath id=\"clip6902\">\n",
       "    <rect x=\"480\" y=\"0\" width=\"1681\" height=\"1600\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<polygon clip-path=\"url(#clip6901)\" points=\"\n",
       "224.386,1440.48 2321.26,1440.48 2321.26,125.984 224.386,125.984 \n",
       "  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<defs>\n",
       "  <clipPath id=\"clip6903\">\n",
       "    <rect x=\"224\" y=\"125\" width=\"2098\" height=\"1315\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<polyline clip-path=\"url(#clip6903)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  279.767,1440.48 279.767,125.984 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip6903)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  676.197,1440.48 676.197,125.984 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip6903)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  1072.63,1440.48 1072.63,125.984 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip6903)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  1469.06,1440.48 1469.06,125.984 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip6903)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  1865.48,1440.48 1865.48,125.984 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip6903)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  2261.91,1440.48 2261.91,125.984 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip6903)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  224.386,1301.83 2321.26,1301.83 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip6903)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  224.386,1110.22 2321.26,1110.22 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip6903)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  224.386,918.602 2321.26,918.602 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip6903)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  224.386,726.988 2321.26,726.988 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip6903)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  224.386,535.374 2321.26,535.374 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip6903)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  224.386,343.76 2321.26,343.76 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip6903)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  224.386,152.147 2321.26,152.147 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip6901)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  224.386,1440.48 2321.26,1440.48 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip6901)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  224.386,1440.48 224.386,125.984 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip6901)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  279.767,1440.48 279.767,1420.77 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip6901)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  676.197,1440.48 676.197,1420.77 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip6901)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1072.63,1440.48 1072.63,1420.77 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip6901)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1469.06,1440.48 1469.06,1420.77 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip6901)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1865.48,1440.48 1865.48,1420.77 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip6901)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  2261.91,1440.48 2261.91,1420.77 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip6901)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  224.386,1301.83 255.839,1301.83 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip6901)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  224.386,1110.22 255.839,1110.22 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip6901)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  224.386,918.602 255.839,918.602 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip6901)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  224.386,726.988 255.839,726.988 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip6901)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  224.386,535.374 255.839,535.374 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip6901)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  224.386,343.76 255.839,343.76 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip6901)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  224.386,152.147 255.839,152.147 \n",
       "  \"/>\n",
       "<g clip-path=\"url(#clip6901)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:middle;\" transform=\"rotate(0, 279.767, 1494.48)\" x=\"279.767\" y=\"1494.48\">0</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip6901)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:middle;\" transform=\"rotate(0, 676.197, 1494.48)\" x=\"676.197\" y=\"1494.48\">100</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip6901)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:middle;\" transform=\"rotate(0, 1072.63, 1494.48)\" x=\"1072.63\" y=\"1494.48\">200</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip6901)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:middle;\" transform=\"rotate(0, 1469.06, 1494.48)\" x=\"1469.06\" y=\"1494.48\">300</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip6901)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:middle;\" transform=\"rotate(0, 1865.48, 1494.48)\" x=\"1865.48\" y=\"1494.48\">400</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip6901)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:middle;\" transform=\"rotate(0, 2261.91, 1494.48)\" x=\"2261.91\" y=\"1494.48\">500</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip6901)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:end;\" transform=\"rotate(0, 200.386, 1319.33)\" x=\"200.386\" y=\"1319.33\">0.3</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip6901)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:end;\" transform=\"rotate(0, 200.386, 1127.72)\" x=\"200.386\" y=\"1127.72\">0.4</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip6901)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:end;\" transform=\"rotate(0, 200.386, 936.102)\" x=\"200.386\" y=\"936.102\">0.5</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip6901)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:end;\" transform=\"rotate(0, 200.386, 744.488)\" x=\"200.386\" y=\"744.488\">0.6</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip6901)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:end;\" transform=\"rotate(0, 200.386, 552.874)\" x=\"200.386\" y=\"552.874\">0.7</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip6901)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:end;\" transform=\"rotate(0, 200.386, 361.26)\" x=\"200.386\" y=\"361.26\">0.8</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip6901)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:end;\" transform=\"rotate(0, 200.386, 169.647)\" x=\"200.386\" y=\"169.647\">0.9</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip6901)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:84px; text-anchor:middle;\" transform=\"rotate(0, 1272.82, 73.2)\" x=\"1272.82\" y=\"73.2\">Accuracy evolution</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip6901)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:66px; text-anchor:middle;\" transform=\"rotate(0, 1272.82, 1590.4)\" x=\"1272.82\" y=\"1590.4\">Number of epochs</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip6901)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:66px; text-anchor:middle;\" transform=\"rotate(-90, 57.6, 783.233)\" x=\"57.6\" y=\"783.233\">Accuracy</text>\n",
       "</g>\n",
       "<polyline clip-path=\"url(#clip6903)\" style=\"stroke:#009af9; stroke-width:12; stroke-opacity:1; fill:none\" points=\"\n",
       "  283.732,1320.95 287.696,1313.54 291.66,1223.51 295.625,1137.93 299.589,1034.93 303.553,874.144 307.517,711.872 311.482,587.39 315.446,516.998 319.41,477.357 \n",
       "  323.375,455.869 327.339,446.236 331.303,450.311 335.267,450.682 339.232,455.498 343.196,450.682 347.16,456.61 351.125,456.239 355.089,458.832 359.053,461.426 \n",
       "  363.018,461.796 366.982,463.278 370.946,464.019 374.91,464.39 378.875,466.983 382.839,465.501 386.803,464.39 390.768,465.131 394.732,463.278 398.696,462.537 \n",
       "  402.66,462.167 406.625,461.426 410.589,460.685 414.553,459.944 418.518,458.462 422.482,457.351 426.446,457.351 430.411,456.98 434.375,455.869 438.339,454.387 \n",
       "  442.303,453.275 446.268,451.793 450.232,449.941 454.196,448.459 458.161,448.088 462.125,446.236 466.089,444.754 470.053,444.013 474.018,442.161 477.982,440.679 \n",
       "  481.946,439.938 485.911,438.826 489.875,437.715 493.839,436.603 497.804,434.751 501.768,432.528 505.732,431.417 509.696,429.935 513.661,429.194 517.625,428.823 \n",
       "  521.589,427.341 525.554,425.859 529.518,425.119 533.482,424.378 537.446,423.266 541.411,421.414 545.375,419.191 549.339,418.079 553.304,417.709 557.268,416.597 \n",
       "  561.232,415.486 565.197,414.004 569.161,412.893 573.125,408.817 577.089,406.594 581.054,404.742 585.018,403.26 588.982,401.408 592.947,399.185 596.911,396.591 \n",
       "  600.875,394.739 604.839,393.257 608.804,392.146 612.768,390.293 616.732,388.07 620.697,386.588 624.661,384.736 628.625,382.513 632.59,381.402 636.554,380.29 \n",
       "  640.518,377.326 644.482,375.844 648.447,374.362 652.411,372.139 656.375,371.028 660.34,369.917 664.304,368.805 668.268,366.582 672.232,365.841 676.197,364.73 \n",
       "  680.161,362.877 684.125,361.766 688.09,358.802 692.054,358.432 696.018,357.691 699.983,356.95 703.947,355.097 707.911,352.874 711.875,351.392 715.84,351.022 \n",
       "  719.804,349.17 723.768,347.317 727.733,346.576 731.697,344.353 735.661,343.612 739.625,341.019 743.59,339.907 747.554,338.055 751.518,336.573 755.483,335.091 \n",
       "  759.447,334.35 763.411,332.868 767.376,331.016 771.34,329.534 775.304,327.681 779.268,325.459 783.233,325.088 787.197,323.606 791.161,321.754 795.126,319.901 \n",
       "  799.09,318.049 803.054,316.937 807.018,315.826 810.983,314.344 814.947,312.492 818.911,308.787 822.876,308.046 826.84,306.193 830.804,305.453 834.769,305.082 \n",
       "  838.733,302.489 842.697,300.636 846.661,298.413 850.626,297.672 854.59,295.449 858.554,294.338 862.519,292.856 866.483,291.004 870.447,290.263 874.411,289.151 \n",
       "  878.376,288.04 882.34,287.669 886.304,286.558 890.269,285.076 894.233,282.112 898.197,281.371 902.162,280.63 906.126,279.148 910.09,277.296 914.054,275.814 \n",
       "  918.019,274.702 921.983,271.368 925.947,270.257 929.912,269.145 933.876,267.663 937.84,267.293 941.804,266.922 945.769,265.811 949.733,265.44 953.697,264.699 \n",
       "  957.662,263.588 961.626,263.217 965.59,262.106 969.555,260.995 973.519,260.624 977.483,259.883 981.447,259.142 985.412,258.031 989.376,257.66 993.34,257.29 \n",
       "  997.305,256.919 1001.27,256.178 1005.23,254.696 1009.2,252.844 1013.16,252.473 1017.13,250.992 1021.09,250.992 1025.05,249.88 1029.02,249.51 1032.98,249.51 \n",
       "  1036.95,248.398 1040.91,248.398 1044.88,248.028 1048.84,247.287 1052.8,246.916 1056.77,246.546 1060.73,245.064 1064.7,243.952 1068.66,243.582 1072.63,242.47 \n",
       "  1076.59,241.729 1080.55,241.359 1084.52,240.618 1088.48,240.248 1092.45,239.507 1096.41,239.136 1100.38,238.766 1104.34,238.025 1108.3,235.802 1112.27,234.32 \n",
       "  1116.23,234.32 1120.2,233.579 1124.16,233.579 1128.13,231.726 1132.09,231.726 1136.05,230.244 1140.02,229.874 1143.98,228.763 1147.95,228.763 1151.91,228.392 \n",
       "  1155.88,227.651 1159.84,227.281 1163.8,226.91 1167.77,226.169 1171.73,225.428 1175.7,224.687 1179.66,224.317 1183.63,223.946 1187.59,223.205 1191.55,222.464 \n",
       "  1195.52,222.094 1199.48,221.723 1203.45,221.353 1207.41,220.982 1211.38,220.982 1215.34,220.241 1219.31,219.13 1223.27,218.389 1227.23,218.389 1231.2,217.648 \n",
       "  1235.16,218.019 1239.13,217.648 1243.09,216.907 1247.06,215.425 1251.02,214.684 1254.98,214.684 1258.95,214.684 1262.91,213.943 1266.88,213.573 1270.84,213.202 \n",
       "  1274.81,213.202 1278.77,213.202 1282.73,212.461 1286.7,212.461 1290.66,212.091 1294.63,212.091 1298.59,212.091 1302.56,211.72 1306.52,210.979 1310.48,210.609 \n",
       "  1314.45,210.238 1318.41,209.127 1322.38,208.015 1326.34,207.645 1330.31,207.645 1334.27,207.275 1338.23,206.904 1342.2,206.534 1346.16,206.534 1350.13,206.163 \n",
       "  1354.09,205.793 1358.06,205.793 1362.02,205.793 1365.98,205.793 1369.95,205.422 1373.91,205.052 1377.88,204.681 1381.84,204.311 1385.81,203.199 1389.77,202.829 \n",
       "  1393.73,202.458 1397.7,201.717 1401.66,202.088 1405.63,201.347 1409.59,201.347 1413.56,201.347 1417.52,200.976 1421.48,200.976 1425.45,200.976 1429.41,200.235 \n",
       "  1433.38,198.753 1437.34,198.012 1441.31,197.642 1445.27,197.642 1449.23,197.271 1453.2,195.79 1457.16,195.79 1461.13,195.419 1465.09,195.049 1469.06,195.049 \n",
       "  1473.02,194.678 1476.98,194.678 1480.95,193.937 1484.91,193.937 1488.88,193.567 1492.84,193.567 1496.81,193.196 1500.77,193.196 1504.73,193.196 1508.7,192.826 \n",
       "  1512.66,192.826 1516.63,192.455 1520.59,191.344 1524.56,190.973 1528.52,190.603 1532.48,190.603 1536.45,190.232 1540.41,190.232 1544.38,190.232 1548.34,189.862 \n",
       "  1552.31,189.862 1556.27,189.862 1560.23,189.491 1564.2,188.75 1568.16,188.38 1572.13,187.268 1576.09,186.898 1580.06,187.268 1584.02,187.639 1587.98,187.639 \n",
       "  1591.95,187.639 1595.91,187.268 1599.88,186.898 1603.84,186.898 1607.81,186.898 1611.77,186.527 1615.73,186.527 1619.7,186.527 1623.66,186.157 1627.63,186.157 \n",
       "  1631.59,186.157 1635.56,186.157 1639.52,186.157 1643.48,185.786 1647.45,185.786 1651.41,185.786 1655.38,185.786 1659.34,185.786 1663.31,185.046 1667.27,183.564 \n",
       "  1671.23,183.193 1675.2,183.193 1679.16,183.193 1683.13,183.193 1687.09,183.193 1691.06,183.193 1695.02,183.564 1698.98,182.823 1702.95,182.082 1706.91,182.082 \n",
       "  1710.88,182.082 1714.84,181.711 1718.81,181.341 1722.77,181.711 1726.73,181.341 1730.7,181.711 1734.66,181.341 1738.63,181.341 1742.59,181.341 1746.56,181.341 \n",
       "  1750.52,180.97 1754.48,180.6 1758.45,180.6 1762.41,179.859 1766.38,179.859 1770.34,179.488 1774.31,179.488 1778.27,179.488 1782.23,179.488 1786.2,179.488 \n",
       "  1790.16,179.118 1794.13,179.118 1798.09,178.747 1802.06,178.747 1806.02,178.747 1809.98,178.747 1813.95,178.377 1817.91,178.377 1821.88,178.377 1825.84,178.377 \n",
       "  1829.81,178.006 1833.77,177.636 1837.73,177.636 1841.7,177.636 1845.66,177.636 1849.63,176.895 1853.59,176.895 1857.56,176.154 1861.52,176.154 1865.48,176.154 \n",
       "  1869.45,176.154 1873.41,175.783 1877.38,175.413 1881.34,175.042 1885.31,174.672 1889.27,174.672 1893.24,174.672 1897.2,174.672 1901.16,174.672 1905.13,174.672 \n",
       "  1909.09,174.302 1913.06,174.672 1917.02,174.672 1920.99,174.672 1924.95,174.302 1928.91,173.931 1932.88,173.931 1936.84,173.561 1940.81,173.561 1944.77,173.19 \n",
       "  1948.74,173.19 1952.7,173.19 1956.66,172.82 1960.63,172.449 1964.59,172.079 1968.56,172.079 1972.52,172.079 1976.49,172.079 1980.45,171.708 1984.41,171.708 \n",
       "  1988.38,171.708 1992.34,172.079 1996.31,172.449 2000.27,172.079 2004.24,171.708 2008.2,171.708 2012.16,171.338 2016.13,170.967 2020.09,170.967 2024.06,170.597 \n",
       "  2028.02,170.226 2031.99,170.226 2035.95,170.226 2039.91,169.485 2043.88,169.485 2047.84,169.485 2051.81,169.485 2055.77,169.115 2059.74,169.115 2063.7,168.744 \n",
       "  2067.66,168.374 2071.63,166.892 2075.59,166.521 2079.56,166.521 2083.52,166.521 2087.49,166.521 2091.45,166.521 2095.41,166.521 2099.38,166.151 2103.34,166.151 \n",
       "  2107.31,166.151 2111.27,166.151 2115.24,165.41 2119.2,165.41 2123.16,165.039 2127.13,164.669 2131.09,164.298 2135.06,164.669 2139.02,164.669 2142.99,164.669 \n",
       "  2146.95,164.669 2150.91,164.669 2154.88,164.298 2158.84,163.928 2162.81,164.298 2166.77,164.298 2170.74,164.298 2174.7,163.928 2178.66,163.558 2182.63,163.558 \n",
       "  2186.59,163.558 2190.56,163.558 2194.52,163.558 2198.49,163.558 2202.45,163.558 2206.41,163.187 2210.38,163.558 2214.34,163.928 2218.31,163.928 2222.27,163.928 \n",
       "  2226.24,163.928 2230.2,163.928 2234.16,163.928 2238.13,163.928 2242.09,164.298 2246.06,163.928 2250.02,163.928 2253.99,163.928 2257.95,163.928 2261.91,163.558 \n",
       "  \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip6903)\" style=\"stroke:#e26f46; stroke-width:12; stroke-opacity:1; fill:none\" points=\"\n",
       "  283.732,1387.25 287.696,1382.34 291.66,1371.87 295.625,1403.28 299.589,1369.91 303.553,1289.43 307.517,1172.96 311.482,1034.25 315.446,919.093 319.41,802.299 \n",
       "  323.375,715.276 327.339,662.931 331.303,616.148 335.267,580.489 339.232,555.952 343.196,539.922 347.16,524.545 351.125,510.478 355.089,500.336 359.053,493.466 \n",
       "  363.018,484.633 366.982,479.725 370.946,478.417 374.91,475.472 378.875,475.8 382.839,471.219 386.803,470.238 390.768,468.275 394.732,465.985 398.696,465.004 \n",
       "  402.66,462.059 406.625,461.405 410.589,460.423 414.553,460.096 418.518,459.769 422.482,458.133 426.446,458.133 430.411,457.806 434.375,457.806 438.339,457.152 \n",
       "  442.303,456.17 446.268,455.189 450.232,453.226 454.196,452.572 458.161,452.572 462.125,451.917 466.089,452.245 470.053,451.263 474.018,450.936 477.982,449.627 \n",
       "  481.946,448.973 485.911,447.992 489.875,447.992 493.839,446.356 497.804,445.701 501.768,444.393 505.732,444.066 509.696,443.411 513.661,440.467 517.625,439.486 \n",
       "  521.589,438.504 525.554,438.177 529.518,432.615 533.482,430.652 537.446,429.998 541.411,429.344 545.375,426.072 549.339,424.764 553.304,424.109 557.268,423.782 \n",
       "  561.232,420.184 565.197,419.202 569.161,417.239 573.125,415.276 577.089,414.622 581.054,412.659 585.018,410.369 588.982,410.369 592.947,407.752 596.911,406.443 \n",
       "  600.875,406.116 604.839,403.172 608.804,401.209 612.768,396.628 616.732,392.703 620.697,392.703 624.661,390.74 628.625,390.74 632.59,386.814 636.554,386.814 \n",
       "  640.518,386.487 644.482,385.505 648.447,382.561 652.411,381.579 656.375,380.271 660.34,378.962 664.304,377.981 668.268,376.672 672.232,375.363 676.197,373.728 \n",
       "  680.161,372.419 684.125,371.765 688.09,369.475 692.054,368.166 696.018,367.512 699.983,366.203 703.947,365.876 707.911,363.586 711.875,364.24 715.84,360.969 \n",
       "  719.804,358.679 723.768,356.389 727.733,354.753 731.697,353.117 735.661,351.481 739.625,350.5 743.59,348.21 747.554,347.228 751.518,344.938 755.483,344.611 \n",
       "  759.447,342.648 763.411,340.031 767.376,336.432 771.34,335.451 775.304,333.488 779.268,332.179 783.233,331.198 787.197,330.216 791.161,329.889 795.126,329.889 \n",
       "  799.09,329.235 803.054,329.562 807.018,327.272 810.983,326.29 814.947,325.309 818.911,323.346 822.876,321.71 826.84,321.056 830.804,317.784 834.769,316.476 \n",
       "  838.733,314.84 842.697,314.513 846.661,313.531 850.626,313.531 854.59,310.587 858.554,309.278 862.519,308.624 866.483,307.97 870.447,307.316 874.411,306.334 \n",
       "  878.376,305.025 882.34,304.698 886.304,303.39 890.269,303.063 894.233,302.735 898.197,302.408 902.162,301.1 906.126,299.137 910.09,298.81 914.054,299.137 \n",
       "  918.019,298.81 921.983,294.557 925.947,293.902 929.912,293.575 933.876,287.032 937.84,286.051 941.804,285.723 945.769,286.051 949.733,286.051 953.697,286.378 \n",
       "  957.662,286.378 961.626,285.723 965.59,284.415 969.555,284.415 973.519,283.76 977.483,283.76 981.447,283.106 985.412,283.433 989.376,282.779 993.34,281.798 \n",
       "  997.305,281.47 1001.27,281.798 1005.23,281.143 1009.2,280.816 1013.16,280.489 1017.13,280.162 1021.09,280.162 1025.05,280.162 1029.02,279.508 1032.98,279.18 \n",
       "  1036.95,277.872 1040.91,277.217 1044.88,276.89 1048.84,276.236 1052.8,274.927 1056.77,274.6 1060.73,273.946 1064.7,272.31 1068.66,271.983 1072.63,271.329 \n",
       "  1076.59,270.347 1080.55,270.347 1084.52,270.02 1088.48,270.347 1092.45,270.347 1096.41,269.366 1100.38,268.384 1104.34,267.73 1108.3,267.076 1112.27,266.421 \n",
       "  1116.23,265.767 1120.2,264.786 1124.16,264.458 1128.13,264.131 1132.09,264.458 1136.05,262.496 1140.02,262.168 1143.98,261.514 1147.95,260.533 1151.91,260.533 \n",
       "  1155.88,260.86 1159.84,259.878 1163.8,258.897 1167.77,258.243 1171.73,257.588 1175.7,257.915 1179.66,256.934 1183.63,255.952 1187.59,255.952 1191.55,256.28 \n",
       "  1195.52,257.588 1199.48,257.588 1203.45,256.934 1207.41,256.28 1211.38,255.625 1215.34,254.644 1219.31,254.317 1223.27,252.027 1227.23,251.699 1231.2,252.027 \n",
       "  1235.16,252.027 1239.13,252.027 1243.09,250.391 1247.06,249.737 1251.02,248.428 1254.98,248.428 1258.95,248.101 1262.91,248.101 1266.88,248.101 1270.84,247.119 \n",
       "  1274.81,247.446 1278.77,246.138 1282.73,245.484 1286.7,245.484 1290.66,245.484 1294.63,245.156 1298.59,244.502 1302.56,244.175 1306.52,243.521 1310.48,242.866 \n",
       "  1314.45,242.539 1318.41,242.212 1322.38,241.885 1326.34,242.212 1330.31,242.212 1334.27,241.885 1338.23,241.231 1342.2,241.231 1346.16,240.903 1350.13,240.903 \n",
       "  1354.09,241.231 1358.06,240.576 1362.02,240.249 1365.98,239.922 1369.95,240.249 1373.91,240.249 1377.88,238.286 1381.84,237.959 1385.81,237.959 1389.77,237.632 \n",
       "  1393.73,237.632 1397.7,237.305 1401.66,237.959 1405.63,236.978 1409.59,237.632 1413.56,237.632 1417.52,236.65 1421.48,236.65 1425.45,236.323 1429.41,235.669 \n",
       "  1433.38,235.342 1437.34,235.342 1441.31,235.015 1445.27,235.342 1449.23,234.36 1453.2,233.706 1457.16,233.052 1461.13,232.725 1465.09,232.397 1469.06,232.07 \n",
       "  1473.02,232.07 1476.98,232.07 1480.95,231.416 1484.91,231.089 1488.88,230.107 1492.84,230.762 1496.81,230.762 1500.77,230.434 1504.73,229.78 1508.7,229.453 \n",
       "  1512.66,229.453 1516.63,229.126 1520.59,229.126 1524.56,229.126 1528.52,229.453 1532.48,229.126 1536.45,229.126 1540.41,229.126 1544.38,229.453 1548.34,229.453 \n",
       "  1552.31,229.453 1556.27,228.472 1560.23,227.49 1564.2,227.49 1568.16,227.163 1572.13,227.163 1576.09,227.49 1580.06,227.817 1584.02,227.49 1587.98,227.49 \n",
       "  1591.95,226.509 1595.91,226.181 1599.88,226.181 1603.84,226.181 1607.81,226.181 1611.77,225.854 1615.73,225.2 1619.7,224.873 1623.66,224.873 1627.63,224.873 \n",
       "  1631.59,224.873 1635.56,224.873 1639.52,224.219 1643.48,224.219 1647.45,223.891 1651.41,223.891 1655.38,223.564 1659.34,223.237 1663.31,223.237 1667.27,222.91 \n",
       "  1671.23,222.256 1675.2,221.928 1679.16,221.928 1683.13,221.601 1687.09,221.601 1691.06,221.601 1695.02,221.601 1698.98,220.62 1702.95,220.62 1706.91,220.62 \n",
       "  1710.88,220.62 1714.84,220.293 1718.81,220.293 1722.77,220.293 1726.73,220.62 1730.7,220.947 1734.66,220.947 1738.63,220.62 1742.59,219.966 1746.56,219.966 \n",
       "  1750.52,219.966 1754.48,218.984 1758.45,218.003 1762.41,218.003 1766.38,218.003 1770.34,218.657 1774.31,218.657 1778.27,218.984 1782.23,218.984 1786.2,218.984 \n",
       "  1790.16,218.984 1794.13,218.657 1798.09,218.657 1802.06,218.33 1806.02,218.33 1809.98,218.33 1813.95,218.003 1817.91,217.675 1821.88,217.348 1825.84,217.675 \n",
       "  1829.81,217.675 1833.77,217.348 1837.73,217.348 1841.7,217.675 1845.66,217.348 1849.63,217.021 1853.59,217.021 1857.56,216.694 1861.52,216.367 1865.48,216.367 \n",
       "  1869.45,216.367 1873.41,216.367 1877.38,216.694 1881.34,216.367 1885.31,215.713 1889.27,215.713 1893.24,215.385 1897.2,215.058 1901.16,214.731 1905.13,214.404 \n",
       "  1909.09,213.75 1913.06,213.75 1917.02,213.75 1920.99,213.75 1924.95,214.404 1928.91,214.731 1932.88,214.731 1936.84,214.404 1940.81,214.404 1944.77,214.404 \n",
       "  1948.74,214.404 1952.7,214.077 1956.66,214.077 1960.63,214.077 1964.59,214.404 1968.56,214.404 1972.52,215.058 1976.49,215.058 1980.45,215.713 1984.41,215.713 \n",
       "  1988.38,215.713 1992.34,215.713 1996.31,215.385 2000.27,215.385 2004.24,215.385 2008.2,215.385 2012.16,215.713 2016.13,215.385 2020.09,215.385 2024.06,214.731 \n",
       "  2028.02,213.095 2031.99,213.095 2035.95,213.095 2039.91,213.095 2043.88,212.768 2047.84,212.768 2051.81,212.768 2055.77,212.768 2059.74,212.768 2063.7,211.46 \n",
       "  2067.66,211.46 2071.63,211.132 2075.59,211.132 2079.56,210.805 2083.52,210.805 2087.49,211.46 2091.45,211.46 2095.41,211.46 2099.38,211.46 2103.34,210.805 \n",
       "  2107.31,210.805 2111.27,210.805 2115.24,210.805 2119.2,210.478 2123.16,210.151 2127.13,209.824 2131.09,208.515 2135.06,208.515 2139.02,208.515 2142.99,208.515 \n",
       "  2146.95,208.515 2150.91,208.515 2154.88,208.515 2158.84,208.188 2162.81,208.188 2166.77,207.534 2170.74,207.861 2174.7,207.861 2178.66,207.534 2182.63,207.534 \n",
       "  2186.59,207.207 2190.56,207.207 2194.52,207.207 2198.49,206.879 2202.45,207.207 2206.41,207.534 2210.38,207.861 2214.34,207.861 2218.31,207.861 2222.27,207.207 \n",
       "  2226.24,207.207 2230.2,207.207 2234.16,207.534 2238.13,206.879 2242.09,206.879 2246.06,207.207 2250.02,207.207 2253.99,207.534 2257.95,207.534 2261.91,207.861 \n",
       "  \n",
       "  \"/>\n",
       "<polygon clip-path=\"url(#clip6901)\" points=\"\n",
       "1693.65,390.944 2249.26,390.944 2249.26,209.504 1693.65,209.504 \n",
       "  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip6901)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1693.65,390.944 2249.26,390.944 2249.26,209.504 1693.65,209.504 1693.65,390.944 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip6901)\" style=\"stroke:#009af9; stroke-width:12; stroke-opacity:1; fill:none\" points=\"\n",
       "  1717.65,269.984 1861.65,269.984 \n",
       "  \"/>\n",
       "<g clip-path=\"url(#clip6901)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:start;\" transform=\"rotate(0, 1885.65, 287.484)\" x=\"1885.65\" y=\"287.484\">Train accuracy</text>\n",
       "</g>\n",
       "<polyline clip-path=\"url(#clip6901)\" style=\"stroke:#e26f46; stroke-width:12; stroke-opacity:1; fill:none\" points=\"\n",
       "  1717.65,330.464 1861.65,330.464 \n",
       "  \"/>\n",
       "<g clip-path=\"url(#clip6901)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:start;\" transform=\"rotate(0, 1885.65, 347.964)\" x=\"1885.65\" y=\"347.964\">Test accuracy</text>\n",
       "</g>\n",
       "</svg>\n"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Plots.plot(hcat(tr_accuracy_evol,te_accuracy_evol), title=\"Accuracy evolution\",label=[\"Train accuracy\" \"Test accuracy\"],lw=3)\n",
    "xlabel!(\"Number of epochs\")\n",
    "ylabel!(\"Accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous remarquons qu'on atteint une précision de 91%, cela est largement meilleur que la méthode Bayes_Naïve, et la regression logistique en utilisant l'algorithme de Metropolis Hastings (50 itérations)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8709236810653919"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(X_test,y_test,theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C'est un Spam !"
     ]
    }
   ],
   "source": [
    "predict(X_train[1496,:], theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[1496]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous avons donc une bonne prédiction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Essayons maintenant de prédire à partir du chemin d'un fichier, si ce fichier est un spam ou un ham"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./enron2/ham/0003.1999-12-10.kaminski.ham.txt --> C'est un Ham !\n"
     ]
    }
   ],
   "source": [
    "predict_from_file(\"./enron2/ham/0003.1999-12-10.kaminski.ham.txt\", theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion de la régression logistique:\n",
    "\n",
    "Pour bien estimer les poids, nous avons eu recours à deux algorithmes: \n",
    "\n",
    "- Metropolis Hastings: Cet algorithme permet de converger quelque soit l'initialisation de nos paramètres, d'où son utilisation. Le problème est qu'il est moins utilisé quand le nombre de paramètres devient grand; On utiise l'algorithme de Gibbs dans ce cas. Mais en n'ayant pas la loi de la loi conditionnelle complète des paramètres, nous ne pouvons qu'utiliser Metropolis Hasting avec un nombre de paramètres égal à 907. Nous remarquons aussi qu'avec 100 itérations **(très peu)**, nous avons attendu environs 5 minutes, ce qui limite la recherche de meilleurs paramètres.\n",
    "\n",
    "- Regression logstique: Cet algorithme est un algorithme de classification très robuste, qui malgré les performances des nouveaux algorithmes tels que les réseaux de neurones, est toujours utilisés dans beaucoup de domaines. Nous avons aussi pu voir la rapidité de l'entrainement (500 itérations) qui nous a permis d'arriver à 0.87% en précision sur l'ensemble de Test."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IV. <a id = \"unit4\" > </a >  Références \n",
    "____\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "(1) Base de données utilisée \n",
    ": https://labs-repos.iit.demokritos.gr/skel/i-config/downloads/enron-spam/preprocessed/ ; \n",
    "\n",
    "(2) Sur la base de données d'enron: https://en.wikipedia.org/wiki/Enron_Corpus  \n",
    "\n",
    "(3) Classification naïve bayésienne:  https://fr.wikipedia.org/wiki/Classification_na%C3%AFve_bay%C3%A9sienne\n",
    "\n",
    "(4) Laplace smoothing: \n",
    "-  https://en.wikipedia.org/wiki/Additive_smoothing  \n",
    "- https://stats.stackexchange.com/questions/108797/in-naive-bayes-why-bother-with-laplace-smoothing-when-we-have-unknown-words-in    \n",
    "\n",
    "(5) Surapprentissage: https://fr.wikipedia.org/wiki/Surapprentissage  \n",
    "\n",
    "(6) Metropolis-Hastings : http://www.mit.edu/~ilkery/papers/MetropolisHastingsSampling.pdf  \n",
    "\n",
    "(7) Descente de gradient: https://eric.univ-lyon2.fr/~ricco/cours/slides/gradient_descent.pdf\n",
    "\n",
    "(8) Régression logistique: https://data.princeton.edu/wws509/notes/c3.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.1.0",
   "language": "julia",
   "name": "julia-1.1"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
