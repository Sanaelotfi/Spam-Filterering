{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TP 2: Filtrage du spam \n",
    "___\n",
    "\n",
    "Nous proposons dans ce TP deux techniques pour la détection de spam. Il s'agit d'identifier si un mail fait partie des messages indésirables (spam) ou dans des messages légitimes (ham).\n",
    "\n",
    "La base de données utilisée est $Enron^{(1)(2)}$. Nous allons spécifier par la suite les dossiers utilisés pour l'entrainement et le test.\n",
    "\n",
    "Les 2 techniques que nous utilisons sont : la calssification naive bayésienne et la régression logistique.\n",
    "\n",
    "_**Remarques importantes:**_\n",
    "\n",
    "- Nous utilisons la version 1.1.0 de Julia.\n",
    "\n",
    "- Vous pouvez télécharger les deux dossier \"enron1\" et \"enron2\" qui seront utilisés dans ce TP et les mettre dans un dossier \"data\". Vous trouverez le fichier \"trainData.jld\" sur moodle, (à mettre dans le dossier \"data\" également) ce qui vous permettra d'éviter de relancer l'entrainement.\n",
    "\n",
    "### Sommaire:\n",
    "___\n",
    "\n",
    "\n",
    "[I. Etat de l'art du filtrage du spam](#unit1)  \n",
    "[II. Classification naive bayésienne](#unit2)  \n",
    "[III. Régression logistique](#unit3)  \n",
    "[III.1. Préparation des données](#unit3.1)  \n",
    "[III.2. Méthode MCMC - Metropolis Hastings](#unit3.2)  \n",
    "[III.3. Descente de gradient](#unit3.3)  \n",
    "[IV. Références](#unit4)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I. Etat de l'art du filtrage du spam <a id = \"unit1\" > </a >\n",
    "___\n",
    "\n",
    "Pendent les dernière années, les gens ont commencé à utiliser la méthode de descente de gradient pour éviter le problème des matrices non inversibles (overfitting) en utilisant la régression logistique. Le passage à la régression logistique avec la méthode de descente de gradient a amélioré les performances significativement tout en permettant de prendre en compte les corrélations entre les mots. Cependant la classification naive bayésienne reste très simple tout en donnant des résultats impressionants. Nous allons alors explorer la classification naive bayésienne et la régression logistique (avec l'algorithme de Metropolis-Hastings et la descente de gradient). **Ne paniquez pas si vous ne comprenez pas les termes utilisés, tout sera expliqué par la suite**.\n",
    "\n",
    "\n",
    "### II. Classification naive bayésienne <a id = \"unit2\" > </a >\n",
    "___\n",
    "\n",
    "La classification naive bayésienne$^{(3)}$ consiste à utiliser le théoreme de Bayes pour calculer la probabilité à posteriori en estimant les probabilités conditionnelles d'une façon naive à partir des données d'entrainement.\n",
    "\n",
    "Dans notre contexte, notons la variable d'intérêt $S$ qui est égale à $1$ si le mail est un Spam et $0$ sinon. Soit $W$ le vecteur des mots contenus dans un mail donné par $W = (w_1, ..., w_n)$. Alors, selon **le théorème de Bayes**, la probabilité que le mail soit un spam sachant son contenu (les mots qu'il contient) est:\n",
    "\n",
    "$$ P(S|W) = \\frac{P(W|S) P(S)}{P(W)}$$\n",
    "\n",
    "\n",
    "- La probabilité $P(S)$ représente notre probabilité a priori que le mail soit un spam. Dans ce TP, nous prenons **$P(S=1) = P(S=0) = \\frac{1}{2}$**. Cependant, nous pouvons aussi laisser cette probabilité comme un **hyperparametre** de notre algorithme que nous pouvons déterminer en utilisant **La validation croisée**. Dans ce cas, la probabilité qui sera retenue est celle qui minimise l'erreur de classification sur l'ensemble de validation.  \n",
    "\n",
    "\n",
    "- La probabilité $P(W) = P(W|S) P(S) + P(W|\\overline{S}) P(\\overline{S})$ n'est pas importante, car pour classifier un mail en tant que un spam ou non, nous allons comparer $P(S = 1|W) = \\frac{P(W|S = 1) P(S = 1)}{P(W)}$ et $P(S = 0|W) = \\frac{P(W|S = 0) P(S = 0)}{P(W)}$. Les 2 partagent le même dénominateur, donc il suffit de comparer les nominateurs. C'est ce qu'on va faire dans ce TD.\n",
    "\n",
    "- Il reste l'estimation de la probabilité $P(W|S) = P(w_1, ..., w_n|S)$. Il est très difficile d'estimer cette probabilité à partir de l'ensemble d'entrainement directement. Pour cela, on fait **l'hypothèse (importante) de l'indépendance** qui caractérise la classification naive bayésienne: les mots sont considérés conditionnellement indépendants, donc on peut écrire:\n",
    "\n",
    "$$P(W|S) = P(w_1, ..., w_n|S) = P(w_1|S) \\times .. \\times P(w_n|S) = \\prod_{i=1}^{n} P(w_i|S) $$\n",
    "\n",
    "Nous pouvons alors estimer directement les probabilités $P(w_i|S)$. Pour un mot donné $w_i$, la probabilité $P(w_i|S)$ représente la fréquence de son apparition dans l'ensemble d'entrainement (la présence d'un mot dans un mail n'est comptée qu'une seule fois).\n",
    "\n",
    "Passons maintenant à l'implémentation de cette approche. Nous allons devoir parser tous les mails de l'ensemble d'entrainement. Nous utilisons le dossier *enron1* en tant qu'ensemble d'entrainement.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "using DataFrames\n",
    "using Distributions\n",
    "using JLD\n",
    "using Plots\n",
    "using Query\n",
    "using Random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous créons la fonction *textProcessing* qui prend un texte et retourne l'ensemble de mots contenus dans le texte (de longueur supérieure strictement à 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "textProcessing (generic function with 1 method)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "textProcessing: transformer un texte en une liste de mots\n",
    "            qui figurent dans le texte, en enlevant toute substance\n",
    "            de longueur inférieure à 2 et en mettant toutes les\n",
    "            lettres en minuscule\n",
    "input : - text [str]: chaîne de caractères \n",
    "output: - words [Array]: tableau qui contient les mots (chaînes de caractères)\n",
    "            qui figurent dans le texte\n",
    "\"\"\"\n",
    "\n",
    "function textProcessing(text)\n",
    "    # définir les séparateurs dans un texte\n",
    "    separators = [' ','-', '.', '_', '!', '@', ':', ',', '/', ';']\n",
    "    # mettre toutes les lettres en minuscule\n",
    "    # utiliser les séparateurs pour séparer les mots\n",
    "    words = split(lowercase(text), separators)\n",
    "    # garder uniquement les substances à longueur supérieure strictement à 1\n",
    "    words = filter(x -> length(x) > 1, words)\n",
    "    # éviter la repétition des mots\n",
    "    words = unique(words)\n",
    "    \n",
    "    return words\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensuite, nous créons la fonction *parseEmail* qui prend en entrée le chemin vers un fichier-mail et effectue la lecture du mail ligne par ligne en utilisant la fonction définie précédemment *textProcessing* afin de récupérer les mots de chaque ligne."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "parseEmail"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "parseEmail: renvoyer une liste des mots qui figurent dans un mail\n",
    "input : - file_path [str]: chemin vers le fichier contenant le mail\n",
    "output: - list_words [Array] : tableau qui contient les mots (chaînes de caractères)\n",
    "            qui figurent dans le mail\n",
    "\"\"\"\n",
    "function parseEmail(file_path)\n",
    "    # initialiser la liste des mots\n",
    "    list_words = []\n",
    "    # lire le mail\n",
    "    f = open(file_path)\n",
    "    lines = readlines(f)\n",
    "    for line in lines\n",
    "        # récupérer les mots figurant dans cette ligne\n",
    "        words = textProcessing(line)\n",
    "        # concaténer tous les mots\n",
    "        list_words = vcat(list_words,words)\n",
    "    end\n",
    "    # éviter les répétition des mots\n",
    "    list_words = unique(list_words)\n",
    "    close(f)\n",
    "    return list_words   \n",
    "end\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La fonction *getTrainDF* permet de créer une dataframe qui contient tous les mots de l'ensemble d'entrainement et leurs probabilités (fréquences d'apparition). Notons bien que la lecture de certains fichiers retourne une erreur lorsqu'il contient des chaine de caractères avec un encodage non supporté par Julia pour les chaines de caractères. Pour cela, nous utilisons des outils de gestion d'exceptions (try/catch) et nous notons le nombre de fichiers non utilisés."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "getTrainDF (generic function with 1 method)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "getTrainDF: Créer un DFionnaire qui contient tous les mots\n",
    "            contenus dans les mails de l'ensemble d'entrainement.\n",
    "            Ces mots représentent les clés. Pour une clé donnée, la valeur\n",
    "            représente le nombre de mails dans lesqueles la clé (le mot)\n",
    "            est apparue. Cela donne la fréquence d'apparition de chaque\n",
    "            mot dans les mails.\n",
    "input : - dir_path [str]: chemin vers le dossier qui contient les mails\n",
    "output: - trainDF [Dataframe]: contient 2 colonnes: la première donne les mots contenus\n",
    "            dans les mails et le 2ème donne leurs fréquences d'apparition\n",
    "\"\"\"\n",
    "\n",
    "function getTrainDF(dir_path)\n",
    "    trainDF = DataFrame()\n",
    "    # initialiser l'ensemble des mots\n",
    "    all_words = []\n",
    "    # récupérer les fichier dans le dossier donné\n",
    "    train_dir = readdir(dir_path)\n",
    "    numFiles = length(train_dir)\n",
    "    # compteur du nombre des fichiers non utilisés\n",
    "    # à cause des problèmes d'encodage des chaines\n",
    "    # de caractères dans les mails\n",
    "    not_used_files = 0\n",
    "    i = 0\n",
    "    for path in train_dir\n",
    "        try\n",
    "            # récupérer le chemin vers le fichier\n",
    "            file_path = dir_path*\"/\"*path\n",
    "            # récupérer la liste des mots contenus dans ce mail\n",
    "            list_words = parseEmail(file_path)\n",
    "            # concaténer tous les mots\n",
    "            all_words = vcat(all_words, list_words)\n",
    "            # print(\"length of all words: \", length(all_words), \"\\n\")\n",
    "        catch\n",
    "            # incrémenter le nombre des fichiers non utilisés\n",
    "            not_used_files = not_used_files + 1\n",
    "        end\n",
    "        i = i +1\n",
    "    end\n",
    "    print(\"Le nombre de fichiers non utilisés est: \", not_used_files, \"\\n\")\n",
    "    \n",
    "    # nombre de fichiers utilisés réellement\n",
    "    numUsedFiles = i - not_used_files\n",
    "    \n",
    "    # créer un ensemble des mots uniques\n",
    "    unique_words = unique(all_words)\n",
    "        \n",
    "    # créer une dataframe avec tous les mots et leurs fréquences d'apparition\n",
    "    \n",
    "    # Ajouter la colonne des mots dans la dataframe\n",
    "    trainDF.words = unique_words\n",
    "    \n",
    "    # rajouter une colonne de probabilités , \"+1\" pour Laplace smoothing, section suivante\n",
    "    trainDF.proba = [(count(x->x==i,all_words) + 1)/numUsedFiles for i in unique_words]\n",
    "    \n",
    "    return trainDF, numUsedFiles\n",
    "    \n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Laplace Smoothing$^{(4)}$**\n",
    "\n",
    "En utilisant l'expression:\n",
    "\n",
    "$$P(W|S) = P(w_1, ..., w_n|S) = P(w_1|S) \\times .. \\times P(w_n|S) = \\prod_{i=1}^{n} P(w_i|S) $$\n",
    "\n",
    "on se rend compte rapidement que si un mot de l'ensemble test d'existe pas dans l'ensemble d'entrainement, sa probabilité estimée sera égale à zéro, et cela va faire que le produit soit nul et annuler l'effet de toutes les autres probabilités. C'est ce qu'on appelle le surapprentissage$^{(5)}$. De la même façon, nous souhaitons éviter les probabilités égales à 1. \n",
    "\n",
    "Pour cela, on va utiliser Laplace smoothing$^{(4)}$ qui consiste à changer la façon de laquelle on calcule l'estimation de la probabilité de la façon classique:\n",
    "\n",
    "$$ P(w_i|S) = \\frac{\\text{nombre de mail contenant } w_i}{\\text{nombre de mails de classe } S}$$\n",
    "\n",
    "à utiliser cette expression:\n",
    "\n",
    "$$ P(w_i|S) = \\frac{\\text{nombre de mail contenant} w_i + 1}{\\text{nombre de mails de classe } S + \\text{taille de l'ensemble d'entrainement}}$$\n",
    "\n",
    "Cela nous permet de résoudre le problème de surapprentissage.\n",
    "\n",
    "Nous allons également créer une colonne qui contient le logarithme des probabilités au lieu des probabilités elle-même. En effet, les probabilités sont très petites et leur produit pourrait nous causer des problèmes numériques. Donc nous préférons sommer les log des probabilités."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rescaleProba"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\"\"\"\n",
    "rescaleProba : mettre à jour le calcul de la probabilité\n",
    "            en utilisant Laplace smoothing. Créer une nouvelle colonne\n",
    "            qui représente le logarithme de la probabilité\n",
    "input : - trainDF [Dataframe]: contient les mots de l'ensemble d'entrainement\n",
    "            et leur probabilités d'apparition \n",
    "        - numUsedFiles [int]: Le nombre de fichier utilisés pour créer la Dataframe\n",
    "            pour cette classe (Spam ou Ham).\n",
    "        - totalNumFiles [int]: nombre total des fichiers utilisés pour l'entrainement\n",
    "output: - trainDF [Dataframe]: Dataframe mise à jour.\n",
    "\"\"\"\n",
    "function rescaleProba(trainDF, numUsedFiles, totalNumFiles)\n",
    "    # mettre à jour le calcul de la probabilité\n",
    "    trainDF.proba = map(x -> x * numUsedFiles /(numUsedFiles + totalNumFiles) , trainDF.proba)\n",
    "    # ajouter une nouvelle colonne pour le log de la probabilité\n",
    "    trainDF.logProba = map(x -> log(x) , trainDF.proba)\n",
    "    \n",
    "    return trainDF\n",
    "end\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous créons finalement la fonction *trainingProbabilities* qui connecte toutes les parties afin de retourner 2 dataframes pour les mails spam et non spams et le nombres de fichiers utilisés dans chaque cas. Puisque l'entrainement prend beaucoup de temps, nous avons sauvegardé ces résultats sous forme d'un fichier *trainData.jld*. Vous pouvez alors soit relancer l'entrainement (mode=\"train\") soit utiliser directement ce ficher (mode=\"upload\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "trainingProbabilities"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "trainingProbabilities : Récupérer les dataframes des probabilités pour les 2 classes\n",
    "            ham et spam, pour l'ensemble d'entrainement\n",
    "input : - path_train_spam [str]: chemin vers le dossier des mails spam pour\n",
    "            l'entrainement \n",
    "        - path_train_ham [str]: chemin vers le dossier des mails non spam pour\n",
    "            l'entrainement \n",
    "        - mode [binary - str : \"upload\"/train]: si mode est \"train\", nous récupérons les \n",
    "            fréquences des mots dans l'ensemble d'entrainement, nous les sauvegardons\n",
    "            dans le dossier \"data\" (pour utilisation ultérieure) et nous les retournons.\n",
    "            Si le mode est \"upload\", nous récupérons les données déjà sauvegardées, et nous\n",
    "            les retournons.\n",
    "output: - trainDFSpam [Dataframe]: contient les mots de l'ensemble d'entrainement\n",
    "            contenus dans des mails \"spam\" et leur probabilités d'apparition.\n",
    "        - trainDFHam [Dataframe]: contient les mots de l'ensemble d'entrainement\n",
    "            contenus dans des mails \"ham\" et leur probabilités d'apparition.\n",
    "\"\"\"\n",
    "function trainingProbabilities(path_train_spam, path_train_ham, mode=\"upload\")\n",
    "    \n",
    "    if mode == \"train\"\n",
    "        # créer des dataframes\n",
    "        processSpam = getTrainDF(path_train_spam)\n",
    "        processHam = getTrainDF(path_train_ham)\n",
    "        # récupérer les dataframes\n",
    "        trainDFSpam = processSpam[1]\n",
    "        trainDFHam = processHam[1]\n",
    "        # récupérer le nombre des fichier utilisés \n",
    "        numUsedFilesSpam = processSpam[2]\n",
    "        numUsedFilesHam = processHam[2]\n",
    "        # nombre total des fichiers utilisés pour l'entrainement\n",
    "        totalNumFiles = numUsedFilesSpam + numUsedFilesHam\n",
    "        # mettre à jour les probabilités\n",
    "        trainDFSpam = rescaleProba(trainDFSpam, numUsedFilesSpam, totalNumFiles)\n",
    "        trainDFHam = rescaleProba(trainDFHam, numUsedFilesHam, totalNumFiles)\n",
    "        # sauvegarder les données d'entrainement (fréquence de chaque mot)\n",
    "        save(\"data/trainData.jld\",\"trainDFSpam\",trainDFSpam,\n",
    "            \"trainDFHam\",trainDFHam, \"numUsedFilesSpam\", numUsedFilesSpam,\n",
    "            \"numUsedFilesHam\", numUsedFilesHam)\n",
    "    else\n",
    "        # mode: upload\n",
    "        # récupérer les données sauvegardées\n",
    "        trainDFSpam = load(\"data/trainData.jld\")[\"trainDFSpam\"]\n",
    "        trainDFHam = load(\"data/trainData.jld\")[\"trainDFHam\"]\n",
    "        numUsedFilesSpam = load(\"data/trainData.jld\")[\"numUsedFilesSpam\"]\n",
    "        numUsedFilesHam = load(\"data/trainData.jld\")[\"numUsedFilesHam\"]\n",
    "        \n",
    "    end\n",
    "    return trainDFSpam, trainDFHam, numUsedFilesSpam, numUsedFilesHam\n",
    "end\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous implémentons finalement la fonction *naiveBayes* qui prend une liste de mots et retourne *1* si le mail est classifié en tant que spam et 0 sinon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "naiveBayes (generic function with 2 methods)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\"\"\"\n",
    "naiveBayes : Implémentation simple de Naive Bayes avec Laplace smoothing.\n",
    "            Renvoie une variable binaire qui détermine si le mail est un spam ou non\n",
    "input : - testWords [Array]: liste des mots contenus dans le mail\n",
    "        - trainDFSpam [Dataframe]: contient les mots de l'ensemble d'entrainement\n",
    "            contenus dans des mails \"spam\" et leur probabilités d'apparition.\n",
    "        - trainDFHam [Dataframe]: contient les mots de l'ensemble d'entrainement\n",
    "            contenus dans des mails \"ham\" et leur probabilités d'apparition.\n",
    "        - spamProba [float]: probabilité qu'un mail soit un spam, en général (a priori)\n",
    "output: - isSpam [binary- 0,1] : égale à 1 si le mail est un spam, 0 sinon.\n",
    "\"\"\"\n",
    "\n",
    "function naiveBayes(testWords, trainDFSpam, trainDFHam, numUsedFilesSpam, numUsedFilesHam, spamProba = 0.5)\n",
    "    # la probabilité qu'un email ne soit pas spam, en général\n",
    "    hamProba = 1 - spamProba\n",
    "    \n",
    "    totalNumFiles = numUsedFilesHam + numUsedFilesSpam\n",
    "    # les probabilités pour qu'un nouveau mot soit un spam ou non\n",
    "    # sont données par Laplace smoothing\n",
    "    proba_new_word_spam = 1 / (numUsedFilesSpam + totalNumFiles)\n",
    "    proba_new_word_ham = 1 / (numUsedFilesHam + totalNumFiles)\n",
    "    # récupérer les mots contenus dans le mail qui figurent dans\n",
    "    # l'ensemble d'entrainement du spam\n",
    "    intersection_spam = intersect(testWords, trainDFSpam.words)\n",
    "    # nombre du nouveau mots par rapport à la Dataframe du spam\n",
    "    new_words_spam = length(testWords) - length(intersection_spam)\n",
    "    # récupérer les mots contenus dans le mail qui figurent dans\n",
    "    # l'ensemble d'entrainement du ham\n",
    "    intersection_ham = intersect(testWords, trainDFHam.words)\n",
    "    # nombre du nouveau mots par rapport à la Dataframe du non spam\n",
    "    new_words_ham = length(testWords) - length(intersection_ham)\n",
    "    # récupérer une sous dataframe avec les mots contenus dans le spam uniquement\n",
    "    spamDF = @from i in trainDFSpam begin\n",
    "        @where i.words in intersection_spam\n",
    "        @select {i.words, i.logProba, i.proba}\n",
    "        @collect DataFrame\n",
    "        end\n",
    "    # récupérer une sous dataframe avec les mots contenus dans le ham uniquement\n",
    "    hamDF = @from i in trainDFHam begin\n",
    "        @where i.words in intersection_ham\n",
    "        @select {i.words, i.logProba, i.proba}\n",
    "        @collect DataFrame\n",
    "        end\n",
    "    \n",
    "    # logarithme de la probabilité de que le mail soit un spam\n",
    "    logprobaspam = sum(spamDF.logProba) + new_words_spam * log(proba_new_word_spam) + log(spamProba)\n",
    "    # logarithme de la probabilité de que le mail ne soit pas un spam\n",
    "    logprobaham = sum(hamDF.logProba) + new_words_ham * log(proba_new_word_ham) + log(spamProba)\n",
    "    \n",
    "    # On définie une variable binaire égale à 1 si le mail est un spam\n",
    "    isSpam = 0\n",
    "    if logprobaspam > logprobaham\n",
    "        isSpam = 1\n",
    "    end\n",
    "    return isSpam\n",
    "    \n",
    "end\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous créons la fonction *emailIsSpam* qui elle prend le chemin vers un fichier test et appelle la fonction *naiveBayes* pour retourner si le mail est un spam ou non."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "emailIsSpam"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "emailIsSpam : renvoyer une variable binaire qui détermine si le mail est un spam ou non\n",
    "input : - file_path [str]: chemin vers le fichier contenant le mail test \n",
    "        - trainDFSpam [Dataframe]: contient les mots de l'ensemble d'entrainement\n",
    "            contenus dans des mails \"spam\" et leur probabilités d'apparition.\n",
    "        - trainDFHam [Dataframe]: contient les mots de l'ensemble d'entrainement\n",
    "            contenus dans des mails \"ham\" et leur probabilités d'apparition.\n",
    "        - spamProba [float]: probabilité qu'un mail soit un spam, en général (a priori)\n",
    "output: - isSpam [binary- 0,1] : égale à 1 si le mail est un spam, 0 sinon.\n",
    "\"\"\"\n",
    "function emailIsSpam(file_path, trainDFSpam, trainDFHam, numUsedFilesSpam, numUsedFilesHam, spamProba = 0.5)\n",
    "    # récupérer les mots dans le mail test\n",
    "    testWords = parseEmail(file_path)\n",
    "    # On récupère la variable binaire isSpam égale à 1 si le mail est un spam\n",
    "    isSpam = naiveBayes(testWords, trainDFSpam, trainDFHam, numUsedFilesSpam, numUsedFilesHam, spamProba)    \n",
    "    return isSpam\n",
    "    \n",
    "end  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entrainement :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"./data/enron1/ham\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_train_spam = \"./data/enron1/spam\"\n",
    "path_train_ham = \"./data/enron1/ham\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDFSpam, trainDFHam, numUsedFilesSpam, numUsedFilesHam = trainingProbabilities(path_train_spam,\n",
    "    path_train_ham);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test :\n",
    "- Pour un seul exemple:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tester la fonction emailIsSpam avec un email Spam\n",
    "test = \"./data/enron2/spam\"\n",
    "path = readdir(test)[5]\n",
    "file_path = test*\"/\"*path\n",
    "emailIsSpam(file_path, trainDFSpam, trainDFHam, numUsedFilesSpam, numUsedFilesHam)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La classification est correcte. La mail effectivement est un spam."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Pour tout l'ensemble d'entrainement:  \n",
    "Nous utilisons l'ensemble enron2 (en entier) pour tester la performance de notre classifieur. Vous avez le choix entre relancer l'évaluation (perform_test = true) ou faire un print de la précision trouvée lors d'un test précédent (perform_test = false)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy in the last test was: 0.9245810055865922"
     ]
    }
   ],
   "source": [
    "# tester la fonction emailIsSpam avec tout l'ensemble test spam et ham\n",
    "testSpam = \"./data/enron2/spam\"\n",
    "pathsSpam = readdir(testSpam)\n",
    "testHam = \"./data/enron2/ham\"\n",
    "pathsHam = readdir(testHam)\n",
    "\n",
    "perform_test = false\n",
    "\n",
    "if perform_test\n",
    "    i = 0\n",
    "    well_classified = 0\n",
    "    for path in pathsSpam\n",
    "        try\n",
    "            file_path = testSpam*\"/\"*path\n",
    "            well_classified = well_classified + emailIsSpam(file_path, trainDFSpam, trainDFHam, numUsedFilesSpam, numUsedFilesHam)\n",
    "\n",
    "        catch  \n",
    "            i = i + 1\n",
    "        end\n",
    "    end\n",
    "\n",
    "    for path in pathsHam\n",
    "        try\n",
    "            file_path = testHam*\"/\"*path\n",
    "            well_classified = well_classified + 1 - emailIsSpam(file_path, trainDFSpam, trainDFHam,\n",
    "                numUsedFilesSpam, numUsedFilesHam)\n",
    "        catch  \n",
    "            i = i + 1\n",
    "        end\n",
    "    end\n",
    "    accuracy = well_classified / (length(pathsSpam) + length(pathsHam) - i)\n",
    "    save(\"data/result.jld\",\"accuracy\",accuracy)\n",
    "    print(\"The accuracy is : \", accuracy)\n",
    "else\n",
    "    old_accuracy = load(\"data/result.jld\")[\"accuracy\"]\n",
    "    print(\"The accuracy in the last test was: \", old_accuracy)\n",
    "    \n",
    "end\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion: \n",
    "\n",
    "La précision de la classification naive bayésienne est **92%**. La classification naive bayésienne n'est pas assez naive finalement ! \n",
    "\n",
    "Nous pouvons améliorer les résultats en changeant notre probabilité a priori qu'un mail soit un spam (en utilisant notamment un ensemble de validation).  \n",
    "\n",
    "Nous passons maintenant à une 2ème méthode qui est la régression logistique."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### III. Régression logistique$^{(8)}$ <a id = \"unit3\" > </a >\n",
    "___\n",
    "La régression logistique est un algorithme de classification, utilisé lorsque la variable de réponse est catégorique. L'idée de la régression logistique est de trouver une **relation entre les caractéristiques (variables explicatives) et la probabilité d'un résultat particulier (classe particulière de la variable d'intérêt)**. Dans notre cas, la variable d'intérêt est une variable binaire codée en tant que 1 (le mail est un Spam) ou 0 (le mail n'est pas un Spam). En d’autres termes, le modèle de régression logistique prédit $P(Y = 1|X=x) = \\frac{e^{x\\beta}}{1 + e^{x\\beta}} $(appelée sigmoid) en déterminant le vecteur des coefficient $\\beta$.\n",
    "\n",
    "L'algorithme d'estimation de maximum de vraisemble (MLE) détermine les coefficients de régression du modèle qui prédit avec précision la probabilité de la variable d'intérêt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### III.1. Préparation des données: <a id = \"unit3.1\" > </a > "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Vectoriser chaque email:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vectoriser des emails consiste à:\n",
    "\n",
    "- Créer un vecteur pour chaque email, sa taille sera égale au nombre des mots générés à partir de l'ensemble d'entrainement.\n",
    "\n",
    "- Pour le mot n°i de l'ensemble d'entrainement, on vérifie s'il existe dans l'email qu'on veut vectoriser. Si c'est le cas, on donne la valeur 1 au $i^{eme}$ indice du vecteur, sinon 0.\n",
    "\n",
    "- On execute ce processus pour tous les emails."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le nombre de mot générés de la base d'entrainement excéde 50000, tandis que le nombre de fichiers dans l'entrainement/test est d'environs 5000. Cela pose problème pour la régression logistique, vu que le nombre d'example doit être toujours supérieur au nombre de paramètres (mots générés de l'ensemble d'entrainement) sinon on se retrouve dans le surapprentissage$^{(5)}$. Nous allons donc travailler avec les 1000 mots les plus présents dans le Spam et les 1000 mots les plus présents dans le Ham de l'ensemble d'entrainement (enron1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDFSpam = last(sort(trainDFSpam, :proba), 1000);\n",
    "trainDFHam = last(sort(trainDFHam, :proba), 1000);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "vectorize_parsed_mail"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "vectorize_parsed_mail: Transformer tous les email dans data_path\n",
    "                       à des vecteurs de même taille (0/1).\n",
    "Input: data_path: le chemin des données\n",
    "       train_words: Les mots générés de l'ensemble d'entrainement.\n",
    "Output: Matrice, chaque vecteur ligne représente un email.\n",
    "\"\"\"\n",
    "function vectorize_parsed_mail(data_path,train_words)\n",
    "    # Lecture des fichiers dans le data_path\n",
    "    data_dir = readdir(data_path)\n",
    "    # Créer une matrice vide où l'on mettra les vecteurs\n",
    "    vector = zeros(length(data_dir),length(train_words))\n",
    "    println(\"Nombre de fichier dans le dossier: \",length(data_dir))\n",
    "    # Problèmes avec caractères spéciaux -> sauter le fichier\n",
    "    files_used = []\n",
    "    files_not_used = []\n",
    "    for file in 1:length(data_dir)\n",
    "        if file%50 == 0\n",
    "            println(\"Nous sommes arrivés au fichier n° \",file)\n",
    "        end\n",
    "        try\n",
    "            # Extraire les mots du fichier\n",
    "            words = parseEmail(data_path*\"/\"*data_dir[file])\n",
    "            for i in 1:length(train_words)\n",
    "                # Vérification si le ieme mot dans train existe dans le fichier\n",
    "                if train_words[i] in words\n",
    "                    vector[file,i] = 1\n",
    "                end \n",
    "            end\n",
    "        catch\n",
    "            # Ajouter les fichiers non utilisés (problèmes de caractères spéciaux)\n",
    "            append!(files_not_used,file)\n",
    "        end\n",
    "    end\n",
    "    # Ne garder que les lignes des fichiers qui n'ont pas de caractères spéciaux\n",
    "    for i in 1:length(data_dir)\n",
    "        if !(i in files_not_used)\n",
    "           append!(files_used,i) \n",
    "        end\n",
    "    end\n",
    "    \n",
    "    vector = vector[files_used,:];\n",
    "    return vector\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Diviser les données en Train/Test:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On divise nos données pour avoir une base sur laquelle on entraîne notre modèle, puis une qui nous servira à généraliser la précision/erreur qu'on obtiendra avec des nouvelles données.\n",
    "\n",
    "Nous allons nous servir de la base **enron1** pour l'entrainement, puis la base **enron2** pour le test.\n",
    "\n",
    "Aussi, il ne faut surtout pas oublier d'ajouter le vecteur correspondant aux biais."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "prepare_data"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "prepare_data: Séparer les données en Train/Test sets\n",
    "input: Spam Train/Test & Ham Train/Test\n",
    "output: Input_train/test & Output_train/test\n",
    "\"\"\"\n",
    "function prepare_data(train_path_spam, train_path_ham, test_path_spam, test_path_ham, trainDFSpam, trainDFHam,mode=\"upload\")\n",
    "    # Extraction de tous les mots dans notre base d'entrainement\n",
    "    train_words = unique(vcat(trainDFHam[1], trainDFSpam[1]));\n",
    "    # Vectorisation + Ajout du biais + Création des outputs (Spam=1, Ham=0)\n",
    "    X_train_spam = vectorize_parsed_mail(train_path_spam, train_words);\n",
    "    println(\"taille de train_spam set: \",size(X_train_spam))\n",
    "    X_train_ham = vectorize_parsed_mail(train_path_ham, train_words);\n",
    "    println(\"taille de train_ham set: \",size(X_train_ham))\n",
    "    X_train = vcat(X_train_spam,X_train_ham);\n",
    "    X_train = hcat(ones(size(X_train)[1]), X_train);\n",
    "    println(\"taille de train set: \",size(X_train))\n",
    "    y_train = vcat(ones(size(X_train_spam)[1]), zeros(size(X_train_ham)[1]));\n",
    "\n",
    "    # Vectorisation + Ajout du biais + Création des outputs (Spam=1, Ham=0)\n",
    "    X_test_spam = vectorize_parsed_mail(test_path_spam, train_words);\n",
    "    println(\"taille de test_spam set: \",size(X_test_spam))\n",
    "    X_test_ham = vectorize_parsed_mail(test_path_ham, train_words);\n",
    "    println(\"taille de test_ham set: \",size(X_test_ham))\n",
    "    X_test = vcat(X_test_spam,X_test_ham);\n",
    "    X_test = hcat(ones(size(X_test)[1]), X_test);\n",
    "    println(\"taille de test set: \",size(X_test))\n",
    "    y_test = vcat(ones(size(X_test_spam)[1]), zeros(size(X_test_ham)[1]));\n",
    "        \n",
    "    return train_words, X_train, y_train, X_test, y_test\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de fichier dans le dossier: 1500\n",
      "Nous sommes arrivés au fichier n° 50\n",
      "Nous sommes arrivés au fichier n° 100\n",
      "Nous sommes arrivés au fichier n° 150\n",
      "Nous sommes arrivés au fichier n° 200\n",
      "Nous sommes arrivés au fichier n° 250\n",
      "Nous sommes arrivés au fichier n° 300\n",
      "Nous sommes arrivés au fichier n° 350\n",
      "Nous sommes arrivés au fichier n° 400\n",
      "Nous sommes arrivés au fichier n° 450\n",
      "Nous sommes arrivés au fichier n° 500\n",
      "Nous sommes arrivés au fichier n° 550\n",
      "Nous sommes arrivés au fichier n° 600\n",
      "Nous sommes arrivés au fichier n° 650\n",
      "Nous sommes arrivés au fichier n° 700\n",
      "Nous sommes arrivés au fichier n° 750\n",
      "Nous sommes arrivés au fichier n° 800\n",
      "Nous sommes arrivés au fichier n° 850\n",
      "Nous sommes arrivés au fichier n° 900\n",
      "Nous sommes arrivés au fichier n° 950\n",
      "Nous sommes arrivés au fichier n° 1000\n",
      "Nous sommes arrivés au fichier n° 1050\n",
      "Nous sommes arrivés au fichier n° 1100\n",
      "Nous sommes arrivés au fichier n° 1150\n",
      "Nous sommes arrivés au fichier n° 1200\n",
      "Nous sommes arrivés au fichier n° 1250\n",
      "Nous sommes arrivés au fichier n° 1300\n",
      "Nous sommes arrivés au fichier n° 1350\n",
      "Nous sommes arrivés au fichier n° 1400\n",
      "Nous sommes arrivés au fichier n° 1450\n",
      "Nous sommes arrivés au fichier n° 1500\n",
      "taille de train_spam set: (1485, 1520)\n",
      "Nombre de fichier dans le dossier: 3672\n",
      "Nous sommes arrivés au fichier n° 50\n",
      "Nous sommes arrivés au fichier n° 100\n",
      "Nous sommes arrivés au fichier n° 150\n",
      "Nous sommes arrivés au fichier n° 200\n",
      "Nous sommes arrivés au fichier n° 250\n",
      "Nous sommes arrivés au fichier n° 300\n",
      "Nous sommes arrivés au fichier n° 350\n",
      "Nous sommes arrivés au fichier n° 400\n",
      "Nous sommes arrivés au fichier n° 450\n",
      "Nous sommes arrivés au fichier n° 500\n",
      "Nous sommes arrivés au fichier n° 550\n",
      "Nous sommes arrivés au fichier n° 600\n",
      "Nous sommes arrivés au fichier n° 650\n",
      "Nous sommes arrivés au fichier n° 700\n",
      "Nous sommes arrivés au fichier n° 750\n",
      "Nous sommes arrivés au fichier n° 800\n",
      "Nous sommes arrivés au fichier n° 850\n",
      "Nous sommes arrivés au fichier n° 900\n",
      "Nous sommes arrivés au fichier n° 950\n",
      "Nous sommes arrivés au fichier n° 1000\n",
      "Nous sommes arrivés au fichier n° 1050\n",
      "Nous sommes arrivés au fichier n° 1100\n",
      "Nous sommes arrivés au fichier n° 1150\n",
      "Nous sommes arrivés au fichier n° 1200\n",
      "Nous sommes arrivés au fichier n° 1250\n",
      "Nous sommes arrivés au fichier n° 1300\n",
      "Nous sommes arrivés au fichier n° 1350\n",
      "Nous sommes arrivés au fichier n° 1400\n",
      "Nous sommes arrivés au fichier n° 1450\n",
      "Nous sommes arrivés au fichier n° 1500\n",
      "Nous sommes arrivés au fichier n° 1550\n",
      "Nous sommes arrivés au fichier n° 1600\n",
      "Nous sommes arrivés au fichier n° 1650\n",
      "Nous sommes arrivés au fichier n° 1700\n",
      "Nous sommes arrivés au fichier n° 1750\n",
      "Nous sommes arrivés au fichier n° 1800\n",
      "Nous sommes arrivés au fichier n° 1850\n",
      "Nous sommes arrivés au fichier n° 1900\n",
      "Nous sommes arrivés au fichier n° 1950\n",
      "Nous sommes arrivés au fichier n° 2000\n",
      "Nous sommes arrivés au fichier n° 2050\n",
      "Nous sommes arrivés au fichier n° 2100\n",
      "Nous sommes arrivés au fichier n° 2150\n",
      "Nous sommes arrivés au fichier n° 2200\n",
      "Nous sommes arrivés au fichier n° 2250\n",
      "Nous sommes arrivés au fichier n° 2300\n",
      "Nous sommes arrivés au fichier n° 2350\n",
      "Nous sommes arrivés au fichier n° 2400\n",
      "Nous sommes arrivés au fichier n° 2450\n",
      "Nous sommes arrivés au fichier n° 2500\n",
      "Nous sommes arrivés au fichier n° 2550\n",
      "Nous sommes arrivés au fichier n° 2600\n",
      "Nous sommes arrivés au fichier n° 2650\n",
      "Nous sommes arrivés au fichier n° 2700\n",
      "Nous sommes arrivés au fichier n° 2750\n",
      "Nous sommes arrivés au fichier n° 2800\n",
      "Nous sommes arrivés au fichier n° 2850\n",
      "Nous sommes arrivés au fichier n° 2900\n",
      "Nous sommes arrivés au fichier n° 2950\n",
      "Nous sommes arrivés au fichier n° 3000\n",
      "Nous sommes arrivés au fichier n° 3050\n",
      "Nous sommes arrivés au fichier n° 3100\n",
      "Nous sommes arrivés au fichier n° 3150\n",
      "Nous sommes arrivés au fichier n° 3200\n",
      "Nous sommes arrivés au fichier n° 3250\n",
      "Nous sommes arrivés au fichier n° 3300\n",
      "Nous sommes arrivés au fichier n° 3350\n",
      "Nous sommes arrivés au fichier n° 3400\n",
      "Nous sommes arrivés au fichier n° 3450\n",
      "Nous sommes arrivés au fichier n° 3500\n",
      "Nous sommes arrivés au fichier n° 3550\n",
      "Nous sommes arrivés au fichier n° 3600\n",
      "Nous sommes arrivés au fichier n° 3650\n",
      "taille de train_ham set: (3672, 1520)\n",
      "taille de train set: (5157, 1521)\n",
      "Nombre de fichier dans le dossier: 1496\n",
      "Nous sommes arrivés au fichier n° 50\n",
      "Nous sommes arrivés au fichier n° 100\n",
      "Nous sommes arrivés au fichier n° 150\n",
      "Nous sommes arrivés au fichier n° 200\n",
      "Nous sommes arrivés au fichier n° 250\n",
      "Nous sommes arrivés au fichier n° 300\n",
      "Nous sommes arrivés au fichier n° 350\n",
      "Nous sommes arrivés au fichier n° 400\n",
      "Nous sommes arrivés au fichier n° 450\n",
      "Nous sommes arrivés au fichier n° 500\n",
      "Nous sommes arrivés au fichier n° 550\n",
      "Nous sommes arrivés au fichier n° 600\n",
      "Nous sommes arrivés au fichier n° 650\n",
      "Nous sommes arrivés au fichier n° 700\n",
      "Nous sommes arrivés au fichier n° 750\n",
      "Nous sommes arrivés au fichier n° 800\n",
      "Nous sommes arrivés au fichier n° 850\n",
      "Nous sommes arrivés au fichier n° 900\n",
      "Nous sommes arrivés au fichier n° 950\n",
      "Nous sommes arrivés au fichier n° 1000\n",
      "Nous sommes arrivés au fichier n° 1050\n",
      "Nous sommes arrivés au fichier n° 1100\n",
      "Nous sommes arrivés au fichier n° 1150\n",
      "Nous sommes arrivés au fichier n° 1200\n",
      "Nous sommes arrivés au fichier n° 1250\n",
      "Nous sommes arrivés au fichier n° 1300\n",
      "Nous sommes arrivés au fichier n° 1350\n",
      "Nous sommes arrivés au fichier n° 1400\n",
      "Nous sommes arrivés au fichier n° 1450\n",
      "taille de test_spam set: (1368, 1520)\n",
      "Nombre de fichier dans le dossier: 4361\n",
      "Nous sommes arrivés au fichier n° 50\n",
      "Nous sommes arrivés au fichier n° 100\n",
      "Nous sommes arrivés au fichier n° 150\n",
      "Nous sommes arrivés au fichier n° 200\n",
      "Nous sommes arrivés au fichier n° 250\n",
      "Nous sommes arrivés au fichier n° 300\n",
      "Nous sommes arrivés au fichier n° 350\n",
      "Nous sommes arrivés au fichier n° 400\n",
      "Nous sommes arrivés au fichier n° 450\n",
      "Nous sommes arrivés au fichier n° 500\n",
      "Nous sommes arrivés au fichier n° 550\n",
      "Nous sommes arrivés au fichier n° 600\n",
      "Nous sommes arrivés au fichier n° 650\n",
      "Nous sommes arrivés au fichier n° 700\n",
      "Nous sommes arrivés au fichier n° 750\n",
      "Nous sommes arrivés au fichier n° 800\n",
      "Nous sommes arrivés au fichier n° 850\n",
      "Nous sommes arrivés au fichier n° 900\n",
      "Nous sommes arrivés au fichier n° 950\n",
      "Nous sommes arrivés au fichier n° 1000\n",
      "Nous sommes arrivés au fichier n° 1050\n",
      "Nous sommes arrivés au fichier n° 1100\n",
      "Nous sommes arrivés au fichier n° 1150\n",
      "Nous sommes arrivés au fichier n° 1200\n",
      "Nous sommes arrivés au fichier n° 1250\n",
      "Nous sommes arrivés au fichier n° 1300\n",
      "Nous sommes arrivés au fichier n° 1350\n",
      "Nous sommes arrivés au fichier n° 1400\n",
      "Nous sommes arrivés au fichier n° 1450\n",
      "Nous sommes arrivés au fichier n° 1500\n",
      "Nous sommes arrivés au fichier n° 1550\n",
      "Nous sommes arrivés au fichier n° 1600\n",
      "Nous sommes arrivés au fichier n° 1650\n",
      "Nous sommes arrivés au fichier n° 1700\n",
      "Nous sommes arrivés au fichier n° 1750\n",
      "Nous sommes arrivés au fichier n° 1800\n",
      "Nous sommes arrivés au fichier n° 1850\n",
      "Nous sommes arrivés au fichier n° 1900\n",
      "Nous sommes arrivés au fichier n° 1950\n",
      "Nous sommes arrivés au fichier n° 2000\n",
      "Nous sommes arrivés au fichier n° 2050\n",
      "Nous sommes arrivés au fichier n° 2100\n",
      "Nous sommes arrivés au fichier n° 2150\n",
      "Nous sommes arrivés au fichier n° 2200\n",
      "Nous sommes arrivés au fichier n° 2250\n",
      "Nous sommes arrivés au fichier n° 2300\n",
      "Nous sommes arrivés au fichier n° 2350\n",
      "Nous sommes arrivés au fichier n° 2400\n",
      "Nous sommes arrivés au fichier n° 2450\n",
      "Nous sommes arrivés au fichier n° 2500\n",
      "Nous sommes arrivés au fichier n° 2550\n",
      "Nous sommes arrivés au fichier n° 2600\n",
      "Nous sommes arrivés au fichier n° 2650\n",
      "Nous sommes arrivés au fichier n° 2700\n",
      "Nous sommes arrivés au fichier n° 2750\n",
      "Nous sommes arrivés au fichier n° 2800\n",
      "Nous sommes arrivés au fichier n° 2850\n",
      "Nous sommes arrivés au fichier n° 2900\n",
      "Nous sommes arrivés au fichier n° 2950\n",
      "Nous sommes arrivés au fichier n° 3000\n",
      "Nous sommes arrivés au fichier n° 3050\n",
      "Nous sommes arrivés au fichier n° 3100\n",
      "Nous sommes arrivés au fichier n° 3150\n",
      "Nous sommes arrivés au fichier n° 3200\n",
      "Nous sommes arrivés au fichier n° 3250\n",
      "Nous sommes arrivés au fichier n° 3300\n",
      "Nous sommes arrivés au fichier n° 3350\n",
      "Nous sommes arrivés au fichier n° 3400\n",
      "Nous sommes arrivés au fichier n° 3450\n",
      "Nous sommes arrivés au fichier n° 3500\n",
      "Nous sommes arrivés au fichier n° 3550\n",
      "Nous sommes arrivés au fichier n° 3600\n",
      "Nous sommes arrivés au fichier n° 3650\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nous sommes arrivés au fichier n° 3700\n",
      "Nous sommes arrivés au fichier n° 3750\n",
      "Nous sommes arrivés au fichier n° 3800\n",
      "Nous sommes arrivés au fichier n° 3850\n",
      "Nous sommes arrivés au fichier n° 3900\n",
      "Nous sommes arrivés au fichier n° 3950\n",
      "Nous sommes arrivés au fichier n° 4000\n",
      "Nous sommes arrivés au fichier n° 4050\n",
      "Nous sommes arrivés au fichier n° 4100\n",
      "Nous sommes arrivés au fichier n° 4150\n",
      "Nous sommes arrivés au fichier n° 4200\n",
      "Nous sommes arrivés au fichier n° 4250\n",
      "Nous sommes arrivés au fichier n° 4300\n",
      "Nous sommes arrivés au fichier n° 4350\n",
      "taille de test_ham set: (4360, 1520)\n",
      "taille de test set: (5728, 1521)\n"
     ]
    }
   ],
   "source": [
    "path_train_spam = \"./data/enron1/spam\"\n",
    "path_train_ham = \"./data/enron1/ham\"\n",
    "path_test_spam = \"./data/enron2/spam\"\n",
    "path_test_ham = \"./data/enron2/ham\"\n",
    "train_words, X_train, y_train, X_test, y_test = prepare_data(path_train_spam, path_train_ham,\n",
    "    path_test_spam, path_test_ham, trainDFSpam, trainDFHam);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### III.2. Méthode MCMC - Metropolis Hastings$^{(6)}$: <a id = \"unit3.2\" > </a > "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans cette implémentation, nous allons aborder la méthode Metropolis-Hastings.\n",
    "\n",
    "Pour l'implémenter, nous avons besoin de:\n",
    "\n",
    "- La loi à priori des poids $f_{\\beta}$\n",
    "\n",
    "- La vraisemblance $f_{Y|\\beta,X}$\n",
    "\n",
    "Cela nous permettra en premier lieu d'avoir la loi à posteriori des poids connaissant X et Y $$f_{\\beta|Y,X} = \\frac{f_{Y|\\beta,X}.f_{\\beta}}{f_Y}$$\n",
    "\n",
    "Ensuite, nous aurons besoins d'utiliser l'algorithme de Metropolis Hastings pour avoir de bonnes estimations des poids dont on dispose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sigmoid"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "sigmoid: caluler la sigmoid de la sortie pour modéliser la probabilité que Y=1.\n",
    "output: vecteur de la taille du nombre d'exemples. Chaque valeur sera entre 0 et 1.\n",
    "\"\"\"\n",
    "function sigmoid(output)\n",
    "    if typeof(output) == Float64\n",
    "        return exp(output)/(1+exp(output))\n",
    "    end\n",
    "    l = zeros(length(output))\n",
    "    for i in 1:size(output)[1]\n",
    "        l[i] = exp(output[i])/(1+exp(output[i]))\n",
    "    end\n",
    "    return l\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour des raisons de stabilité numérique, nous allons utiliser le $log(f_{\\beta|Y,X})$ à la place de $f_{\\beta|Y,X}$, la multiplication se transforme en somme alors.\n",
    "\n",
    "Concernant la loi à priori des poids, nous allons supposer que les poids suivent une loi à priori impropre.\n",
    "\n",
    "Nous pouvons tout de même supposer que chaque élément suit une loi normale de moyenne 0 et de variance 10, Il suffit de décommenter dans la prochaine cellule.$$f_{\\beta} = N(\\beta|0,10)$$\n",
    "\n",
    "On peut montrer que le $\\log$ de la vraisemblance devient comme suit:\n",
    "$$\n",
    "\\sum_{\\substack{1<i<m}} y^{(i)} log( \\hat{p}^{(i)} ) + (1-y^{(i)}) log( 1 - \\hat{p}^{(i)} )\n",
    "$$\n",
    "où $$\\hat{p}^{(i)} = P(y^{(i)}=1)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "log_posterior"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "log_posteriori: Retourne le logarithme de la loi à posteriori des poids\n",
    "\"\"\"\n",
    "function log_posterior(x, y, theta)\n",
    "    # P(y=1|X=x,Theta=theta)\n",
    "    proba = sigmoid(x*theta)\n",
    "    vraisemblance = 0\n",
    "    for i in 1:length(y)\n",
    "        vraisemblance = vraisemblance + y[i] * log(proba[i]) + (1-y[i]) * log(1-proba[i])\n",
    "    end\n",
    "    priori = 0\n",
    "    # Si on veut que la loi à priori des poids soit normale(0,10) décommentez ci-dessous\n",
    "    #for k in 1:length(theta)\n",
    "        #priori = priori + log(pdf(Normal(0,10),theta[k]))\n",
    "    #end\n",
    "    return vraisemblance + priori\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "logistic_bayes"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "logistique_bayes: Méthode itérative qui utiliser l'algorithme de Metropolis-Hastings\n",
    "                  pour trouver les meilleurs poids avec le jeu de données d'entrainement\n",
    "Inputs: x: Entrée\n",
    "        y: Sortie\n",
    "        n_iterations: pour M-H\n",
    "        sd: l'écart type de la loi normale de proposition des candidats\n",
    "Output: Les poids enregistrés pour chaque itération\n",
    "        Taux d'acceptation de chaque poid\n",
    "\"\"\"\n",
    "function logistic_bayes(x, y, n_iterations=10000,sd=0.1)\n",
    "    Random.seed!(1);\n",
    "    # Initialisation\n",
    "    acceptance = zeros(size(x)[2])\n",
    "    total = zeros(size(x)[2])\n",
    "    save = zeros(n_iterations,size(x)[2])\n",
    "    # Initialisation des poids par une lois normale\n",
    "    theta = randn(size(x)[2])\n",
    "    # Enregistrer les premiers poids\n",
    "    save[1,:] = theta\n",
    "    # Loi à posteriori\n",
    "    posterior = log_posterior(x,y,theta)\n",
    "    for i in 2:n_iterations\n",
    "        println(\"Itération n°\",i-1)\n",
    "        # Pour chaque poid\n",
    "        for j in 1:length(theta)\n",
    "            total[j] = total[j] + 1\n",
    "            theta_maybe = theta\n",
    "            # Générer un candidat pour le jeme poid\n",
    "            d = Normal(theta[j], sd)\n",
    "            theta_maybe[j] = rand(d,1)[1]\n",
    "            # Calcul de la nouvelle loi à posteriori\n",
    "            posterior_maybe = log_posterior(x,y,theta_maybe)\n",
    "            # Calcul du Ratio\n",
    "            R = min(exp(posterior_maybe - posterior),1)\n",
    "            # Générer un échantillon avec la loi uniforme\n",
    "            U = rand(1)[1]\n",
    "            # Si acceptation\n",
    "            if U < R\n",
    "                #println(i,\",\",j)\n",
    "                theta = theta_maybe\n",
    "                posterior = posterior_maybe\n",
    "                acceptance[j] = acceptance[j] + 1\n",
    "            end\n",
    "        end\n",
    "        save[i,:] = theta\n",
    "    end\n",
    "    return save, acceptance./total\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'algorithme de Metropolis Hastings prendra beaucoup de temps avec une grand nombre de poids. Nous appliquons l'algorithme avec 100 itérations seulement. Ça pourrait ne pas être suffisant, mais on verra ce que ça donne comme résultat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Itération n°1\n",
      "Itération n°2\n",
      "Itération n°3\n",
      "Itération n°4\n",
      "Itération n°5\n",
      "Itération n°6\n",
      "Itération n°7\n",
      "Itération n°8\n",
      "Itération n°9\n",
      "Itération n°10\n",
      "Itération n°11\n",
      "Itération n°12\n",
      "Itération n°13\n",
      "Itération n°14\n",
      "Itération n°15\n",
      "Itération n°16\n",
      "Itération n°17\n",
      "Itération n°18\n",
      "Itération n°19\n",
      "Itération n°20\n",
      "Itération n°21\n",
      "Itération n°22\n",
      "Itération n°23\n",
      "Itération n°24\n",
      "Itération n°25\n",
      "Itération n°26\n",
      "Itération n°27\n",
      "Itération n°28\n",
      "Itération n°29\n",
      "Itération n°30\n",
      "Itération n°31\n",
      "Itération n°32\n",
      "Itération n°33\n",
      "Itération n°34\n",
      "Itération n°35\n",
      "Itération n°36\n",
      "Itération n°37\n",
      "Itération n°38\n",
      "Itération n°39\n",
      "Itération n°40\n",
      "Itération n°41\n",
      "Itération n°42\n",
      "Itération n°43\n",
      "Itération n°44\n",
      "Itération n°45\n",
      "Itération n°46\n",
      "Itération n°47\n",
      "Itération n°48\n",
      "Itération n°49\n",
      "Itération n°50\n",
      "Itération n°51\n",
      "Itération n°52\n",
      "Itération n°53\n",
      "Itération n°54\n",
      "Itération n°55\n",
      "Itération n°56\n",
      "Itération n°57\n",
      "Itération n°58\n",
      "Itération n°59\n",
      "Itération n°60\n",
      "Itération n°61\n",
      "Itération n°62\n",
      "Itération n°63\n",
      "Itération n°64\n",
      "Itération n°65\n",
      "Itération n°66\n",
      "Itération n°67\n",
      "Itération n°68\n",
      "Itération n°69\n",
      "Itération n°70\n",
      "Itération n°71\n",
      "Itération n°72\n",
      "Itération n°73\n",
      "Itération n°74\n",
      "Itération n°75\n",
      "Itération n°76\n",
      "Itération n°77\n",
      "Itération n°78\n",
      "Itération n°79\n",
      "Itération n°80\n",
      "Itération n°81\n",
      "Itération n°82\n",
      "Itération n°83\n",
      "Itération n°84\n",
      "Itération n°85\n",
      "Itération n°86\n",
      "Itération n°87\n",
      "Itération n°88\n",
      "Itération n°89\n",
      "Itération n°90\n",
      "Itération n°91\n",
      "Itération n°92\n",
      "Itération n°93\n",
      "Itération n°94\n",
      "Itération n°95\n",
      "Itération n°96\n",
      "Itération n°97\n",
      "Itération n°98\n",
      "Itération n°99\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "593.3087520599365"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start = time()\n",
    "saved_weights, acceptance_rate = logistic_bayes(X_train,y_train,100);\n",
    "elapsed = time() - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "predict (generic function with 1 method)"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "predict: prédire si le mail (x) constitue un spam ou un ham\n",
    "params: x: constitue le mail\n",
    "        theta: le vecteur des poids\n",
    "\"\"\"\n",
    "function predict(x, theta)\n",
    "    # Si x est un vecteur (un seul email)\n",
    "    if typeof(x) == Array{Float64,1}\n",
    "        y = x'*theta\n",
    "        if sigmoid(y)>0.5\n",
    "            print(\"C'est un Spam !\")\n",
    "        else\n",
    "            print(\"C'est un Ham !\")\n",
    "        end\n",
    "        return\n",
    "    # Si x est constitué de plusieurs mails\n",
    "    else\n",
    "        y =  x * theta\n",
    "        l = []\n",
    "        for i in 1:length(y)\n",
    "            # Si proba>0.5 alors on décide que c'est un Spam\n",
    "            if (sigmoid(y).>0.5)[i] == true\n",
    "                append!(l, [\"Spam\"])\n",
    "            # Si proba<0.5 alors on décide que c'est un Spam\n",
    "            else\n",
    "                append!(l,[\"Ham\"])\n",
    "            end\n",
    "        end\n",
    "    end   \n",
    "    \n",
    "    return l\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Essayons maintenant de prédire avec un exemple si c'est un Spam ou un Ham !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C'est un Spam !"
     ]
    }
   ],
   "source": [
    "predict(X_train[1,:], weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On a donc une bonne prédiction !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour faciliter la tâche, et être plus transparent, nous créons la fonction suivante, qui prédit à partir du chemin du fichier, si c'est un spam ou ham."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "predict_from_file (generic function with 1 method)"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "predict_from_file: prédire si le mail (x) constitue un spam ou un ham\n",
    "params: file_path: le chemin du fichier\n",
    "        theta: le vecteur des poids\n",
    "\"\"\"\n",
    "function predict_from_file(file_path,theta)\n",
    "    \n",
    "    x =zeros(length(train_words))\n",
    "    try\n",
    "        global words = parseEmail(file_path)\n",
    "    catch\n",
    "        println(file_path,\" contient des caractères non traitées !\")\n",
    "        return\n",
    "    end\n",
    "    for i in 1:length(train_words)\n",
    "        if train_words[i] in words\n",
    "            x[i] = 1\n",
    "        end \n",
    "    end\n",
    "    print(file_path,\" --> \")\n",
    "    predict(vcat(1,x),theta)\n",
    "    print(\"\\n\")\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data/enron2/ham/0003.1999-12-10.kaminski.ham.txt --> C'est un Ham !\n"
     ]
    }
   ],
   "source": [
    "predict_from_file(\"./data/enron2/ham/0003.1999-12-10.kaminski.ham.txt\", theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "accuracy"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "accuracy: Proportion de bonnes réponses\n",
    "\"\"\"\n",
    "function accuracy(x, y, theta)\n",
    "    prediction = sigmoid(x*theta) .>0.5\n",
    "    return sum(prediction .== y)/size(x)[1]\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7449371508379888"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(X_test,y_test, saved_weights[end,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Résultats\n",
    "\n",
    "Avec 100 itérations uniquement de l'algorithme Metropolis-Hastings et 1500 mots ( de l'ensemble d'entrainement, nous avons pu atteindre une précision de **75%** sur l'ensemble de test. Nous recommandons d'effectuer plus d'itérations (environs 10000) pour avoir un meilleur résultat plus stable, même si cela prendra beaucoup de temps (environs **16h**)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### III.3. Descente de gradient$^{(7)}$: <a id = \"unit3.3\" > </a > "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans cette partie, nous allons voir la méthode de descente de gradient. \n",
    "\n",
    "La descente de gradient est un algorithme qui permet trouver la solution optimale d'un certains nombre de problèmes. Le principe est le suivant: on définit une **fonction de coût J**  qui caractérise le problème.\n",
    "Cette fonction dépend d'un ensemble de **paramètres $\\theta$ **. La descente de gradient cherche à **minimiser** la fonction de coût en **modifiant itérativement** les paramètres.\n",
    "\n",
    "#### Gradient\n",
    "\n",
    "Le gradient de la fonction de coûts pour un $\\theta$ donné, correspond à la direction dans laquelle il faut modifier $\\theta$ pour réduire la valeur de la fonction de coût. \n",
    "\n",
    "La fonction de coût est minimale quand le gradient est nul.\n",
    "\n",
    "Concrètement, on initialize $\\theta$ aléatoirement, et on effectue à chaque itération un pas pour réduire la fonction de coût jusqu'à convergence de l'algorithme à un minimum.\n",
    "\n",
    "#### Learning rate\n",
    "\n",
    "Le taux d'apprentissage correspond à la taille du pas que l'on va effectuer dans la direction du gradient.\n",
    "Plus il est grand, plus la convergence est rapide mais il y a un risque que l'algorithme diverge.\n",
    "\n",
    "Plus il est petit, plus la convergence est lente.\n",
    "\n",
    "#### Epoch\n",
    "\n",
    "Il s'agit de combien on a besoin d'exemple du train set avant de faire la descente de gradient, soit une unique mise à jour de gradient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### La prédiction doit être entre 0 et 1.\n",
    "\n",
    "Pour cela, nous faisons introduire la fonction Sigmoid de la forme suivante:\n",
    "\n",
    "$$ P(Y^{(k)}=1| X^{(k)}) = \\hat{p}^{(k)} = sigmoid(output_k) = \\frac{exp(output_k)}{1 + exp(output_k)} $$\n",
    "\n",
    "où n est le nombre de catégorie qu'on a, n=2 dans notre cas car nous avons les deux catégories Spam/Ham."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sigmoid"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "sigmoid: caluler la sigmoid de la sortie pour avoir une nouvelle sortie entre 0 et 1\n",
    "params: x: considéré dans notre cas comme la sortie\n",
    "\"\"\"\n",
    "function sigmoid(output)\n",
    "    if typeof(output) == Float64\n",
    "        return exp(output)/(1+exp(output))\n",
    "    end\n",
    "    l = zeros(length(output))\n",
    "    for i in 1:size(output)[1]\n",
    "        l[i] = exp(output[i])/(1+exp(output[i]))\n",
    "    end\n",
    "    return l\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour appliquer l'algorithme de descente de gradient, nous avons besoin d'une fontion de coût que nous allons essayer de minimiser. Cette fonction de coût est très importante, elle est induite du logarithme du maximum de vraisemblance.\n",
    "\n",
    "$$ J( \\Theta) = -\\frac{1}{m}\\sum_{\\substack{1<i<m}} y^{(i)} log( \\hat{p}^{(i)} ) + (1-y^{(i)}) log(1 - \\hat{p}^{(i)}) $$\n",
    "\n",
    "Où $m$ est le nombre d'exemples dans l'ensemble de train.\n",
    "\n",
    "Un bon exercice serait de la démontrer étape par étape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "loss"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "loss: Calculer le coût sur l'ensemble donné\n",
    "params: X: La matrice d'entrée\n",
    "        y: Le vecteur sortie\n",
    "        theta: le vecteur des poids\n",
    "\"\"\"\n",
    "function loss(x, y, theta)\n",
    "    err = 0\n",
    "    output = sigmoid(x*theta)\n",
    "    for i in 1:size(y)[1]\n",
    "        err += y[i] * log(output[i]) + (1-y[i]) * log(1-output[i])\n",
    "    end\n",
    "    return -err/size(y)[1]\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensuite, nous souhaitons minimiser cette fonction de coût par rapport au vecteur des poids.\n",
    "\n",
    "Pour cela nous devons, pour chaque poids, aller contre la direction de la descente. \n",
    "\n",
    "La direction de la descente est le vecteur des dérivées de la fonction de coût par rapport à chaque poids:\n",
    "\n",
    "$$ \\Delta_{\\theta}J( \\Theta) = \\frac{1}{m} \\sum_{\\substack{1<i<m}}( \\hat{p}^{(i)} - y^{(i)})x^{(i)}  $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "gradient"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "gradient: calcule le gradient de la fonction de coût par rapport à theta\n",
    "\"\"\"\n",
    "function gradient(x, y, theta)\n",
    "    grad = zeros(size(theta))\n",
    "    output = sigmoid(x*theta)\n",
    "    for i in 1:size(y)[1]\n",
    "        grad = grad + (output[i] - y[i]) * x[i,:]\n",
    "    end\n",
    "    return grad/size(x)[1]\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maintenant que le gradient est calculé, nous devons commencer la partie d'entrainement qui consiste à appliquer la descente de gradient à chaque fois après avoir parcouru tout l'ensemble d'entrainement. Nous allons arrêter l'algorithme si l'un des deux cas est vérifié:\n",
    "\n",
    "- La fonction de coût est plus petite qu'un seuil qu'on choisit.\n",
    "\n",
    "- On atteint le nombre maximale d'itérations  qu'on choisit sur l'ensemble d'entrainement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "train"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "train: Phase d'entrainement\n",
    "params: x_train/test: input\n",
    "        y_train/test: output\n",
    "        theta: vecteur de poids\n",
    "        max_iter: Le nombre maximale d'itérations sur la base d'entrainement qu'on souhaite atteindre\n",
    "        learning_rate: taux d'apprentissage correspond à la taille du pas que l'on prendre\n",
    "                       effectuer dans la direction du gradient\n",
    "\"\"\"\n",
    "function train(x_train, y_train, x_test, y_test, max_iter = 1000, learning_rate=0.05)\n",
    "    theta = rand(1:10,size(x_test)[2])/100;\n",
    "    err_train = 10000\n",
    "    train_loss_evolution = []\n",
    "    test_loss_evolution = []\n",
    "    train_accuracy_evolution = []\n",
    "    test_accuracy_evolution = []\n",
    "    i = 0\n",
    "    while err_train > 0.005 && i < max_iter\n",
    "        # Mise à jour des poids\n",
    "        theta = theta - learning_rate * gradient(x_train, y_train, theta)\n",
    "        # Calcul de la fonction de coût sur l'ensemble d'entrainement\n",
    "        err_train = loss(x_train, y_train, theta)\n",
    "        # Calcul de la fonction de coût sur l'ensemble de test\n",
    "        err_test = loss(x_test, y_test, theta)\n",
    "        append!(train_loss_evolution, err_train)\n",
    "        append!(test_loss_evolution, err_test)\n",
    "        # La précision sur l'ensemble d'entrainement\n",
    "        append!(train_accuracy_evolution, accuracy(x_train, y_train, theta))\n",
    "        # La précision sur l'ensemble de test\n",
    "        append!(test_accuracy_evolution, accuracy(x_test, y_test, theta))\n",
    "        if (i-1)%10 ==0\n",
    "            println(\"Episode $i\")\n",
    "            println(\"cost: $err_train\")\n",
    "        end\n",
    "        i += 1\n",
    "    end\n",
    "    return theta, train_loss_evolution, test_loss_evolution, train_accuracy_evolution, test_accuracy_evolution\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous effectuons l'entrainement avec 500 itérations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1\n",
      "cost: 1.8977410322382209\n",
      "Episode 11\n",
      "cost: 0.6244071082647707\n",
      "Episode 21\n",
      "cost: 0.46837476621508217\n",
      "Episode 31\n",
      "cost: 0.42900239654198136\n",
      "Episode 41\n",
      "cost: 0.40363286513389984\n",
      "Episode 51\n",
      "cost: 0.3830286233005719\n",
      "Episode 61\n",
      "cost: 0.3654656553773008\n",
      "Episode 71\n",
      "cost: 0.35023683019343577\n",
      "Episode 81\n",
      "cost: 0.3368829317523144\n",
      "Episode 91\n",
      "cost: 0.32506170421833835\n",
      "Episode 101\n",
      "cost: 0.31450866515775516\n",
      "Episode 111\n",
      "cost: 0.3050163440466543\n",
      "Episode 121\n",
      "cost: 0.2964201868308755\n",
      "Episode 131\n",
      "cost: 0.2885883432577878\n",
      "Episode 141\n",
      "cost: 0.281414121806792\n",
      "Episode 151\n",
      "cost: 0.2748103538567921\n",
      "Episode 161\n",
      "cost: 0.26870514726465416\n",
      "Episode 171\n",
      "cost: 0.2630386637375379\n",
      "Episode 181\n",
      "cost: 0.2577606578267748\n",
      "Episode 191\n",
      "cost: 0.2528285861891832\n",
      "Episode 201\n",
      "cost: 0.24820614565388996\n",
      "Episode 211\n",
      "cost: 0.24386213477392726\n",
      "Episode 221\n",
      "cost: 0.2397695602291072\n",
      "Episode 231\n",
      "cost: 0.2359049293421706\n",
      "Episode 241\n",
      "cost: 0.23224768483653693\n",
      "Episode 251\n",
      "cost: 0.22877974905179227\n",
      "Episode 261\n",
      "cost: 0.22548515307155148\n",
      "Episode 271\n",
      "cost: 0.22234973231734417\n",
      "Episode 281\n",
      "cost: 0.21936087466582052\n",
      "Episode 291\n",
      "cost: 0.21650731047123215\n",
      "Episode 301\n",
      "cost: 0.21377893633458864\n",
      "Episode 311\n",
      "cost: 0.21116666628920763\n",
      "Episode 321\n",
      "cost: 0.20866230544103165\n",
      "Episode 331\n",
      "cost: 0.20625844213604363\n",
      "Episode 341\n",
      "cost: 0.20394835551593038\n",
      "Episode 351\n",
      "cost: 0.20172593593165902\n",
      "Episode 361\n",
      "cost: 0.19958561615879314\n",
      "Episode 371\n",
      "cost: 0.19752231173175988\n",
      "Episode 381\n",
      "cost: 0.19553136901110982\n",
      "Episode 391\n",
      "cost: 0.19360851983557578\n",
      "Episode 401\n",
      "cost: 0.1917498418029048\n",
      "Episode 411\n",
      "cost: 0.18995172337964403\n",
      "Episode 421\n",
      "cost: 0.1882108331677924\n",
      "Episode 431\n",
      "cost: 0.18652409276136006\n",
      "Episode 441\n",
      "cost: 0.18488865271272054\n",
      "Episode 451\n",
      "cost: 0.1833018712007295\n",
      "Episode 461\n",
      "cost: 0.18176129505270786\n",
      "Episode 471\n",
      "cost: 0.18026464282266502\n",
      "Episode 481\n",
      "cost: 0.17880978967042033\n",
      "Episode 491\n",
      "cost: 0.1773947538218417\n"
     ]
    }
   ],
   "source": [
    "theta, tr_loss_evol, te_loss_evol, tr_accuracy_evol, te_accuracy_evol = train(X_train, y_train, X_test, y_test, 500);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous visualisons l'évolution de la fonction de coût sur l'ensemble d'entrainement et de test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"utf-8\"?>\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"600\" height=\"400\" viewBox=\"0 0 2400 1600\">\n",
       "<defs>\n",
       "  <clipPath id=\"clip7600\">\n",
       "    <rect x=\"0\" y=\"0\" width=\"2000\" height=\"2000\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<defs>\n",
       "  <clipPath id=\"clip7601\">\n",
       "    <rect x=\"0\" y=\"0\" width=\"2400\" height=\"1600\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<polygon clip-path=\"url(#clip7601)\" points=\"\n",
       "0,1600 2400,1600 2400,0 0,0 \n",
       "  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<defs>\n",
       "  <clipPath id=\"clip7602\">\n",
       "    <rect x=\"480\" y=\"0\" width=\"1681\" height=\"1600\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<polygon clip-path=\"url(#clip7601)\" points=\"\n",
       "224.386,1440.48 2321.26,1440.48 2321.26,125.984 224.386,125.984 \n",
       "  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<defs>\n",
       "  <clipPath id=\"clip7603\">\n",
       "    <rect x=\"224\" y=\"125\" width=\"2098\" height=\"1315\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<polyline clip-path=\"url(#clip7603)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  279.767,1440.48 279.767,125.984 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip7603)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  676.197,1440.48 676.197,125.984 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip7603)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  1072.63,1440.48 1072.63,125.984 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip7603)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  1469.06,1440.48 1469.06,125.984 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip7603)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  1865.48,1440.48 1865.48,125.984 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip7603)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  2261.91,1440.48 2261.91,125.984 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip7603)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  224.386,1260.5 2321.26,1260.5 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip7603)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  224.386,1039.98 2321.26,1039.98 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip7603)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  224.386,819.446 2321.26,819.446 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip7603)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  224.386,598.917 2321.26,598.917 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip7603)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  224.386,378.387 2321.26,378.387 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip7603)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  224.386,157.858 2321.26,157.858 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip7601)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  224.386,1440.48 2321.26,1440.48 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip7601)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  224.386,1440.48 224.386,125.984 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip7601)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  279.767,1440.48 279.767,1420.77 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip7601)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  676.197,1440.48 676.197,1420.77 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip7601)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1072.63,1440.48 1072.63,1420.77 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip7601)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1469.06,1440.48 1469.06,1420.77 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip7601)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1865.48,1440.48 1865.48,1420.77 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip7601)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  2261.91,1440.48 2261.91,1420.77 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip7601)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  224.386,1260.5 255.839,1260.5 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip7601)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  224.386,1039.98 255.839,1039.98 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip7601)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  224.386,819.446 255.839,819.446 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip7601)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  224.386,598.917 255.839,598.917 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip7601)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  224.386,378.387 255.839,378.387 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip7601)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  224.386,157.858 255.839,157.858 \n",
       "  \"/>\n",
       "<g clip-path=\"url(#clip7601)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:middle;\" transform=\"rotate(0, 279.767, 1494.48)\" x=\"279.767\" y=\"1494.48\">0</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip7601)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:middle;\" transform=\"rotate(0, 676.197, 1494.48)\" x=\"676.197\" y=\"1494.48\">100</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip7601)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:middle;\" transform=\"rotate(0, 1072.63, 1494.48)\" x=\"1072.63\" y=\"1494.48\">200</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip7601)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:middle;\" transform=\"rotate(0, 1469.06, 1494.48)\" x=\"1469.06\" y=\"1494.48\">300</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip7601)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:middle;\" transform=\"rotate(0, 1865.48, 1494.48)\" x=\"1865.48\" y=\"1494.48\">400</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip7601)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:middle;\" transform=\"rotate(0, 2261.91, 1494.48)\" x=\"2261.91\" y=\"1494.48\">500</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip7601)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:end;\" transform=\"rotate(0, 200.386, 1278)\" x=\"200.386\" y=\"1278\">0.5</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip7601)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:end;\" transform=\"rotate(0, 200.386, 1057.48)\" x=\"200.386\" y=\"1057.48\">1.0</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip7601)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:end;\" transform=\"rotate(0, 200.386, 836.946)\" x=\"200.386\" y=\"836.946\">1.5</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip7601)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:end;\" transform=\"rotate(0, 200.386, 616.417)\" x=\"200.386\" y=\"616.417\">2.0</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip7601)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:end;\" transform=\"rotate(0, 200.386, 395.887)\" x=\"200.386\" y=\"395.887\">2.5</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip7601)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:end;\" transform=\"rotate(0, 200.386, 175.358)\" x=\"200.386\" y=\"175.358\">3.0</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip7601)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:84px; text-anchor:middle;\" transform=\"rotate(0, 1272.82, 73.2)\" x=\"1272.82\" y=\"73.2\">Loss evolution</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip7601)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:66px; text-anchor:middle;\" transform=\"rotate(0, 1272.82, 1590.4)\" x=\"1272.82\" y=\"1590.4\">Number of epochs</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip7601)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:66px; text-anchor:middle;\" transform=\"rotate(-90, 57.6, 783.233)\" x=\"57.6\" y=\"783.233\">Loss</text>\n",
       "</g>\n",
       "<polyline clip-path=\"url(#clip7603)\" style=\"stroke:#009af9; stroke-width:12; stroke-opacity:1; fill:none\" points=\"\n",
       "  283.732,540.839 287.696,644.019 291.66,740.7 295.625,829.648 299.589,909.673 303.553,979.833 307.517,1039.65 311.482,1089.25 315.446,1129.39 319.41,1161.24 \n",
       "  323.375,1186.19 327.339,1205.63 331.303,1220.79 335.267,1232.67 339.232,1242.08 343.196,1249.61 347.16,1255.73 351.125,1260.78 355.089,1265.02 359.053,1268.61 \n",
       "  363.018,1271.73 366.982,1274.45 370.946,1276.88 374.91,1279.06 378.875,1281.05 382.839,1282.89 386.803,1284.59 390.768,1286.19 394.732,1287.7 398.696,1289.13 \n",
       "  402.66,1290.5 406.625,1291.82 410.589,1293.09 414.553,1294.31 418.518,1295.5 422.482,1296.65 426.446,1297.77 430.411,1298.87 434.375,1299.94 438.339,1300.98 \n",
       "  442.303,1302.01 446.268,1303.01 450.232,1303.99 454.196,1304.96 458.161,1305.9 462.125,1306.83 466.089,1307.75 470.053,1308.65 474.018,1309.53 477.982,1310.4 \n",
       "  481.946,1311.25 485.911,1312.1 489.875,1312.92 493.839,1313.74 497.804,1314.54 501.768,1315.33 505.732,1316.11 509.696,1316.88 513.661,1317.64 517.625,1318.38 \n",
       "  521.589,1319.12 525.554,1319.84 529.518,1320.56 533.482,1321.26 537.446,1321.95 541.411,1322.64 545.375,1323.32 549.339,1323.98 553.304,1324.64 557.268,1325.29 \n",
       "  561.232,1325.93 565.197,1326.56 569.161,1327.18 573.125,1327.8 577.089,1328.41 581.054,1329 585.018,1329.6 588.982,1330.18 592.947,1330.76 596.911,1331.33 \n",
       "  600.875,1331.89 604.839,1332.45 608.804,1333 612.768,1333.54 616.732,1334.08 620.697,1334.61 624.661,1335.13 628.625,1335.65 632.59,1336.16 636.554,1336.67 \n",
       "  640.518,1337.17 644.482,1337.66 648.447,1338.15 652.411,1338.64 656.375,1339.11 660.34,1339.59 664.304,1340.05 668.268,1340.52 672.232,1340.97 676.197,1341.43 \n",
       "  680.161,1341.87 684.125,1342.32 688.09,1342.76 692.054,1343.19 696.018,1343.62 699.983,1344.04 703.947,1344.46 707.911,1344.88 711.875,1345.29 715.84,1345.7 \n",
       "  719.804,1346.1 723.768,1346.5 727.733,1346.9 731.697,1347.29 735.661,1347.68 739.625,1348.06 743.59,1348.45 747.554,1348.82 751.518,1349.2 755.483,1349.57 \n",
       "  759.447,1349.93 763.411,1350.3 767.376,1350.66 771.34,1351.01 775.304,1351.36 779.268,1351.71 783.233,1352.06 787.197,1352.41 791.161,1352.75 795.126,1353.08 \n",
       "  799.09,1353.42 803.054,1353.75 807.018,1354.08 810.983,1354.4 814.947,1354.73 818.911,1355.05 822.876,1355.37 826.84,1355.68 830.804,1355.99 834.769,1356.3 \n",
       "  838.733,1356.61 842.697,1356.91 846.661,1357.22 850.626,1357.52 854.59,1357.81 858.554,1358.11 862.519,1358.4 866.483,1358.69 870.447,1358.98 874.411,1359.26 \n",
       "  878.376,1359.55 882.34,1359.83 886.304,1360.11 890.269,1360.38 894.233,1360.66 898.197,1360.93 902.162,1361.2 906.126,1361.47 910.09,1361.73 914.054,1362 \n",
       "  918.019,1362.26 921.983,1362.52 925.947,1362.78 929.912,1363.03 933.876,1363.29 937.84,1363.54 941.804,1363.79 945.769,1364.04 949.733,1364.29 953.697,1364.53 \n",
       "  957.662,1364.78 961.626,1365.02 965.59,1365.26 969.555,1365.5 973.519,1365.73 977.483,1365.97 981.447,1366.2 985.412,1366.43 989.376,1366.66 993.34,1366.89 \n",
       "  997.305,1367.12 1001.27,1367.35 1005.23,1367.57 1009.2,1367.79 1013.16,1368.01 1017.13,1368.23 1021.09,1368.45 1025.05,1368.67 1029.02,1368.88 1032.98,1369.1 \n",
       "  1036.95,1369.31 1040.91,1369.52 1044.88,1369.73 1048.84,1369.94 1052.8,1370.15 1056.77,1370.35 1060.73,1370.56 1064.7,1370.76 1068.66,1370.96 1072.63,1371.16 \n",
       "  1076.59,1371.36 1080.55,1371.56 1084.52,1371.76 1088.48,1371.95 1092.45,1372.15 1096.41,1372.34 1100.38,1372.53 1104.34,1372.72 1108.3,1372.91 1112.27,1373.1 \n",
       "  1116.23,1373.29 1120.2,1373.48 1124.16,1373.66 1128.13,1373.85 1132.09,1374.03 1136.05,1374.21 1140.02,1374.39 1143.98,1374.57 1147.95,1374.75 1151.91,1374.93 \n",
       "  1155.88,1375.11 1159.84,1375.28 1163.8,1375.46 1167.77,1375.63 1171.73,1375.8 1175.7,1375.98 1179.66,1376.15 1183.63,1376.32 1187.59,1376.48 1191.55,1376.65 \n",
       "  1195.52,1376.82 1199.48,1376.99 1203.45,1377.15 1207.41,1377.32 1211.38,1377.48 1215.34,1377.64 1219.31,1377.8 1223.27,1377.96 1227.23,1378.12 1231.2,1378.28 \n",
       "  1235.16,1378.44 1239.13,1378.6 1243.09,1378.76 1247.06,1378.91 1251.02,1379.07 1254.98,1379.22 1258.95,1379.37 1262.91,1379.53 1266.88,1379.68 1270.84,1379.83 \n",
       "  1274.81,1379.98 1278.77,1380.13 1282.73,1380.28 1286.7,1380.43 1290.66,1380.57 1294.63,1380.72 1298.59,1380.86 1302.56,1381.01 1306.52,1381.15 1310.48,1381.3 \n",
       "  1314.45,1381.44 1318.41,1381.58 1322.38,1381.72 1326.34,1381.86 1330.31,1382 1334.27,1382.14 1338.23,1382.28 1342.2,1382.42 1346.16,1382.56 1350.13,1382.69 \n",
       "  1354.09,1382.83 1358.06,1382.96 1362.02,1383.1 1365.98,1383.23 1369.95,1383.37 1373.91,1383.5 1377.88,1383.63 1381.84,1383.76 1385.81,1383.89 1389.77,1384.02 \n",
       "  1393.73,1384.15 1397.7,1384.28 1401.66,1384.41 1405.63,1384.54 1409.59,1384.67 1413.56,1384.79 1417.52,1384.92 1421.48,1385.05 1425.45,1385.17 1429.41,1385.29 \n",
       "  1433.38,1385.42 1437.34,1385.54 1441.31,1385.66 1445.27,1385.79 1449.23,1385.91 1453.2,1386.03 1457.16,1386.15 1461.13,1386.27 1465.09,1386.39 1469.06,1386.51 \n",
       "  1473.02,1386.63 1476.98,1386.75 1480.95,1386.86 1484.91,1386.98 1488.88,1387.1 1492.84,1387.21 1496.81,1387.33 1500.77,1387.44 1504.73,1387.56 1508.7,1387.67 \n",
       "  1512.66,1387.78 1516.63,1387.9 1520.59,1388.01 1524.56,1388.12 1528.52,1388.23 1532.48,1388.34 1536.45,1388.46 1540.41,1388.57 1544.38,1388.68 1548.34,1388.78 \n",
       "  1552.31,1388.89 1556.27,1389 1560.23,1389.11 1564.2,1389.22 1568.16,1389.32 1572.13,1389.43 1576.09,1389.54 1580.06,1389.64 1584.02,1389.75 1587.98,1389.85 \n",
       "  1591.95,1389.96 1595.91,1390.06 1599.88,1390.17 1603.84,1390.27 1607.81,1390.37 1611.77,1390.47 1615.73,1390.58 1619.7,1390.68 1623.66,1390.78 1627.63,1390.88 \n",
       "  1631.59,1390.98 1635.56,1391.08 1639.52,1391.18 1643.48,1391.28 1647.45,1391.38 1651.41,1391.48 1655.38,1391.58 1659.34,1391.67 1663.31,1391.77 1667.27,1391.87 \n",
       "  1671.23,1391.96 1675.2,1392.06 1679.16,1392.16 1683.13,1392.25 1687.09,1392.35 1691.06,1392.44 1695.02,1392.54 1698.98,1392.63 1702.95,1392.73 1706.91,1392.82 \n",
       "  1710.88,1392.91 1714.84,1393.01 1718.81,1393.1 1722.77,1393.19 1726.73,1393.28 1730.7,1393.37 1734.66,1393.46 1738.63,1393.56 1742.59,1393.65 1746.56,1393.74 \n",
       "  1750.52,1393.83 1754.48,1393.92 1758.45,1394 1762.41,1394.09 1766.38,1394.18 1770.34,1394.27 1774.31,1394.36 1778.27,1394.45 1782.23,1394.53 1786.2,1394.62 \n",
       "  1790.16,1394.71 1794.13,1394.79 1798.09,1394.88 1802.06,1394.97 1806.02,1395.05 1809.98,1395.14 1813.95,1395.22 1817.91,1395.31 1821.88,1395.39 1825.84,1395.47 \n",
       "  1829.81,1395.56 1833.77,1395.64 1837.73,1395.72 1841.7,1395.81 1845.66,1395.89 1849.63,1395.97 1853.59,1396.05 1857.56,1396.14 1861.52,1396.22 1865.48,1396.3 \n",
       "  1869.45,1396.38 1873.41,1396.46 1877.38,1396.54 1881.34,1396.62 1885.31,1396.7 1889.27,1396.78 1893.24,1396.86 1897.2,1396.94 1901.16,1397.02 1905.13,1397.1 \n",
       "  1909.09,1397.18 1913.06,1397.25 1917.02,1397.33 1920.99,1397.41 1924.95,1397.49 1928.91,1397.56 1932.88,1397.64 1936.84,1397.72 1940.81,1397.79 1944.77,1397.87 \n",
       "  1948.74,1397.95 1952.7,1398.02 1956.66,1398.1 1960.63,1398.17 1964.59,1398.25 1968.56,1398.32 1972.52,1398.4 1976.49,1398.47 1980.45,1398.55 1984.41,1398.62 \n",
       "  1988.38,1398.69 1992.34,1398.77 1996.31,1398.84 2000.27,1398.91 2004.24,1398.98 2008.2,1399.06 2012.16,1399.13 2016.13,1399.2 2020.09,1399.27 2024.06,1399.34 \n",
       "  2028.02,1399.42 2031.99,1399.49 2035.95,1399.56 2039.91,1399.63 2043.88,1399.7 2047.84,1399.77 2051.81,1399.84 2055.77,1399.91 2059.74,1399.98 2063.7,1400.05 \n",
       "  2067.66,1400.12 2071.63,1400.19 2075.59,1400.26 2079.56,1400.32 2083.52,1400.39 2087.49,1400.46 2091.45,1400.53 2095.41,1400.6 2099.38,1400.66 2103.34,1400.73 \n",
       "  2107.31,1400.8 2111.27,1400.87 2115.24,1400.93 2119.2,1401 2123.16,1401.07 2127.13,1401.13 2131.09,1401.2 2135.06,1401.27 2139.02,1401.33 2142.99,1401.4 \n",
       "  2146.95,1401.46 2150.91,1401.53 2154.88,1401.59 2158.84,1401.66 2162.81,1401.72 2166.77,1401.79 2170.74,1401.85 2174.7,1401.91 2178.66,1401.98 2182.63,1402.04 \n",
       "  2186.59,1402.11 2190.56,1402.17 2194.52,1402.23 2198.49,1402.29 2202.45,1402.36 2206.41,1402.42 2210.38,1402.48 2214.34,1402.55 2218.31,1402.61 2222.27,1402.67 \n",
       "  2226.24,1402.73 2230.2,1402.79 2234.16,1402.85 2238.13,1402.92 2242.09,1402.98 2246.06,1403.04 2250.02,1403.1 2253.99,1403.16 2257.95,1403.22 2261.91,1403.28 \n",
       "  \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip7603)\" style=\"stroke:#e26f46; stroke-width:12; stroke-opacity:1; fill:none\" points=\"\n",
       "  283.732,163.187 287.696,310.284 291.66,449.237 295.625,578.206 299.589,695.331 303.553,799.025 307.517,888.312 311.482,963.089 315.446,1024.17 319.41,1073.08 \n",
       "  323.375,1111.73 327.339,1142.06 331.303,1165.84 335.267,1184.56 339.232,1199.4 343.196,1211.28 347.16,1220.9 351.125,1228.77 355.089,1235.29 359.053,1240.77 \n",
       "  363.018,1245.41 366.982,1249.4 370.946,1252.87 374.91,1255.92 378.875,1258.62 382.839,1261.05 386.803,1263.23 390.768,1265.23 394.732,1267.06 398.696,1268.76 \n",
       "  402.66,1270.34 406.625,1271.82 410.589,1273.21 414.553,1274.52 418.518,1275.77 422.482,1276.96 426.446,1278.1 430.411,1279.2 434.375,1280.25 438.339,1281.27 \n",
       "  442.303,1282.25 446.268,1283.2 450.232,1284.13 454.196,1285.02 458.161,1285.9 462.125,1286.75 466.089,1287.58 470.053,1288.4 474.018,1289.19 477.982,1289.97 \n",
       "  481.946,1290.74 485.911,1291.49 489.875,1292.22 493.839,1292.94 497.804,1293.65 501.768,1294.35 505.732,1295.04 509.696,1295.71 513.661,1296.38 517.625,1297.03 \n",
       "  521.589,1297.68 525.554,1298.31 529.518,1298.94 533.482,1299.56 537.446,1300.17 541.411,1300.78 545.375,1301.37 549.339,1301.96 553.304,1302.54 557.268,1303.11 \n",
       "  561.232,1303.68 565.197,1304.24 569.161,1304.8 573.125,1305.34 577.089,1305.89 581.054,1306.42 585.018,1306.95 588.982,1307.48 592.947,1308 596.911,1308.51 \n",
       "  600.875,1309.02 604.839,1309.52 608.804,1310.02 612.768,1310.51 616.732,1311 620.697,1311.49 624.661,1311.97 628.625,1312.44 632.59,1312.91 636.554,1313.38 \n",
       "  640.518,1313.84 644.482,1314.29 648.447,1314.75 652.411,1315.2 656.375,1315.64 660.34,1316.08 664.304,1316.52 668.268,1316.95 672.232,1317.38 676.197,1317.8 \n",
       "  680.161,1318.23 684.125,1318.64 688.09,1319.06 692.054,1319.47 696.018,1319.88 699.983,1320.28 703.947,1320.68 707.911,1321.08 711.875,1321.47 715.84,1321.86 \n",
       "  719.804,1322.25 723.768,1322.63 727.733,1323.01 731.697,1323.39 735.661,1323.76 739.625,1324.14 743.59,1324.5 747.554,1324.87 751.518,1325.23 755.483,1325.59 \n",
       "  759.447,1325.95 763.411,1326.3 767.376,1326.66 771.34,1327 775.304,1327.35 779.268,1327.69 783.233,1328.03 787.197,1328.37 791.161,1328.71 795.126,1329.04 \n",
       "  799.09,1329.37 803.054,1329.7 807.018,1330.02 810.983,1330.35 814.947,1330.67 818.911,1330.99 822.876,1331.3 826.84,1331.61 830.804,1331.93 834.769,1332.23 \n",
       "  838.733,1332.54 842.697,1332.84 846.661,1333.15 850.626,1333.45 854.59,1333.74 858.554,1334.04 862.519,1334.33 866.483,1334.62 870.447,1334.91 874.411,1335.2 \n",
       "  878.376,1335.48 882.34,1335.77 886.304,1336.05 890.269,1336.33 894.233,1336.6 898.197,1336.88 902.162,1337.15 906.126,1337.42 910.09,1337.69 914.054,1337.96 \n",
       "  918.019,1338.23 921.983,1338.49 925.947,1338.75 929.912,1339.01 933.876,1339.27 937.84,1339.53 941.804,1339.78 945.769,1340.03 949.733,1340.29 953.697,1340.54 \n",
       "  957.662,1340.78 961.626,1341.03 965.59,1341.27 969.555,1341.52 973.519,1341.76 977.483,1342 981.447,1342.24 985.412,1342.47 989.376,1342.71 993.34,1342.94 \n",
       "  997.305,1343.17 1001.27,1343.41 1005.23,1343.63 1009.2,1343.86 1013.16,1344.09 1017.13,1344.31 1021.09,1344.54 1025.05,1344.76 1029.02,1344.98 1032.98,1345.2 \n",
       "  1036.95,1345.41 1040.91,1345.63 1044.88,1345.84 1048.84,1346.06 1052.8,1346.27 1056.77,1346.48 1060.73,1346.69 1064.7,1346.9 1068.66,1347.1 1072.63,1347.31 \n",
       "  1076.59,1347.51 1080.55,1347.72 1084.52,1347.92 1088.48,1348.12 1092.45,1348.32 1096.41,1348.51 1100.38,1348.71 1104.34,1348.91 1108.3,1349.1 1112.27,1349.29 \n",
       "  1116.23,1349.49 1120.2,1349.68 1124.16,1349.87 1128.13,1350.05 1132.09,1350.24 1136.05,1350.43 1140.02,1350.61 1143.98,1350.8 1147.95,1350.98 1151.91,1351.16 \n",
       "  1155.88,1351.34 1159.84,1351.52 1163.8,1351.7 1167.77,1351.88 1171.73,1352.05 1175.7,1352.23 1179.66,1352.4 1183.63,1352.58 1187.59,1352.75 1191.55,1352.92 \n",
       "  1195.52,1353.09 1199.48,1353.26 1203.45,1353.43 1207.41,1353.6 1211.38,1353.76 1215.34,1353.93 1219.31,1354.09 1223.27,1354.26 1227.23,1354.42 1231.2,1354.58 \n",
       "  1235.16,1354.74 1239.13,1354.9 1243.09,1355.06 1247.06,1355.22 1251.02,1355.38 1254.98,1355.53 1258.95,1355.69 1262.91,1355.84 1266.88,1356 1270.84,1356.15 \n",
       "  1274.81,1356.3 1278.77,1356.45 1282.73,1356.6 1286.7,1356.75 1290.66,1356.9 1294.63,1357.05 1298.59,1357.2 1302.56,1357.35 1306.52,1357.49 1310.48,1357.64 \n",
       "  1314.45,1357.78 1318.41,1357.92 1322.38,1358.07 1326.34,1358.21 1330.31,1358.35 1334.27,1358.49 1338.23,1358.63 1342.2,1358.77 1346.16,1358.91 1350.13,1359.04 \n",
       "  1354.09,1359.18 1358.06,1359.32 1362.02,1359.45 1365.98,1359.59 1369.95,1359.72 1373.91,1359.85 1377.88,1359.99 1381.84,1360.12 1385.81,1360.25 1389.77,1360.38 \n",
       "  1393.73,1360.51 1397.7,1360.64 1401.66,1360.77 1405.63,1360.89 1409.59,1361.02 1413.56,1361.15 1417.52,1361.27 1421.48,1361.4 1425.45,1361.52 1429.41,1361.65 \n",
       "  1433.38,1361.77 1437.34,1361.89 1441.31,1362.02 1445.27,1362.14 1449.23,1362.26 1453.2,1362.38 1457.16,1362.5 1461.13,1362.62 1465.09,1362.74 1469.06,1362.85 \n",
       "  1473.02,1362.97 1476.98,1363.09 1480.95,1363.21 1484.91,1363.32 1488.88,1363.44 1492.84,1363.55 1496.81,1363.67 1500.77,1363.78 1504.73,1363.89 1508.7,1364 \n",
       "  1512.66,1364.12 1516.63,1364.23 1520.59,1364.34 1524.56,1364.45 1528.52,1364.56 1532.48,1364.67 1536.45,1364.78 1540.41,1364.89 1544.38,1364.99 1548.34,1365.1 \n",
       "  1552.31,1365.21 1556.27,1365.31 1560.23,1365.42 1564.2,1365.53 1568.16,1365.63 1572.13,1365.73 1576.09,1365.84 1580.06,1365.94 1584.02,1366.04 1587.98,1366.15 \n",
       "  1591.95,1366.25 1595.91,1366.35 1599.88,1366.45 1603.84,1366.55 1607.81,1366.65 1611.77,1366.75 1615.73,1366.85 1619.7,1366.95 1623.66,1367.05 1627.63,1367.15 \n",
       "  1631.59,1367.24 1635.56,1367.34 1639.52,1367.44 1643.48,1367.53 1647.45,1367.63 1651.41,1367.73 1655.38,1367.82 1659.34,1367.92 1663.31,1368.01 1667.27,1368.1 \n",
       "  1671.23,1368.2 1675.2,1368.29 1679.16,1368.38 1683.13,1368.47 1687.09,1368.57 1691.06,1368.66 1695.02,1368.75 1698.98,1368.84 1702.95,1368.93 1706.91,1369.02 \n",
       "  1710.88,1369.11 1714.84,1369.2 1718.81,1369.29 1722.77,1369.37 1726.73,1369.46 1730.7,1369.55 1734.66,1369.64 1738.63,1369.72 1742.59,1369.81 1746.56,1369.9 \n",
       "  1750.52,1369.98 1754.48,1370.07 1758.45,1370.15 1762.41,1370.24 1766.38,1370.32 1770.34,1370.41 1774.31,1370.49 1778.27,1370.57 1782.23,1370.66 1786.2,1370.74 \n",
       "  1790.16,1370.82 1794.13,1370.9 1798.09,1370.98 1802.06,1371.07 1806.02,1371.15 1809.98,1371.23 1813.95,1371.31 1817.91,1371.39 1821.88,1371.47 1825.84,1371.55 \n",
       "  1829.81,1371.63 1833.77,1371.7 1837.73,1371.78 1841.7,1371.86 1845.66,1371.94 1849.63,1372.02 1853.59,1372.09 1857.56,1372.17 1861.52,1372.25 1865.48,1372.32 \n",
       "  1869.45,1372.4 1873.41,1372.47 1877.38,1372.55 1881.34,1372.63 1885.31,1372.7 1889.27,1372.77 1893.24,1372.85 1897.2,1372.92 1901.16,1373 1905.13,1373.07 \n",
       "  1909.09,1373.14 1913.06,1373.22 1917.02,1373.29 1920.99,1373.36 1924.95,1373.43 1928.91,1373.5 1932.88,1373.58 1936.84,1373.65 1940.81,1373.72 1944.77,1373.79 \n",
       "  1948.74,1373.86 1952.7,1373.93 1956.66,1374 1960.63,1374.07 1964.59,1374.14 1968.56,1374.21 1972.52,1374.28 1976.49,1374.34 1980.45,1374.41 1984.41,1374.48 \n",
       "  1988.38,1374.55 1992.34,1374.62 1996.31,1374.68 2000.27,1374.75 2004.24,1374.82 2008.2,1374.88 2012.16,1374.95 2016.13,1375.02 2020.09,1375.08 2024.06,1375.15 \n",
       "  2028.02,1375.21 2031.99,1375.28 2035.95,1375.34 2039.91,1375.41 2043.88,1375.47 2047.84,1375.54 2051.81,1375.6 2055.77,1375.67 2059.74,1375.73 2063.7,1375.79 \n",
       "  2067.66,1375.86 2071.63,1375.92 2075.59,1375.98 2079.56,1376.04 2083.52,1376.11 2087.49,1376.17 2091.45,1376.23 2095.41,1376.29 2099.38,1376.35 2103.34,1376.41 \n",
       "  2107.31,1376.48 2111.27,1376.54 2115.24,1376.6 2119.2,1376.66 2123.16,1376.72 2127.13,1376.78 2131.09,1376.84 2135.06,1376.9 2139.02,1376.96 2142.99,1377.02 \n",
       "  2146.95,1377.07 2150.91,1377.13 2154.88,1377.19 2158.84,1377.25 2162.81,1377.31 2166.77,1377.37 2170.74,1377.42 2174.7,1377.48 2178.66,1377.54 2182.63,1377.6 \n",
       "  2186.59,1377.65 2190.56,1377.71 2194.52,1377.77 2198.49,1377.82 2202.45,1377.88 2206.41,1377.94 2210.38,1377.99 2214.34,1378.05 2218.31,1378.1 2222.27,1378.16 \n",
       "  2226.24,1378.21 2230.2,1378.27 2234.16,1378.32 2238.13,1378.38 2242.09,1378.43 2246.06,1378.49 2250.02,1378.54 2253.99,1378.59 2257.95,1378.65 2261.91,1378.7 \n",
       "  \n",
       "  \"/>\n",
       "<polygon clip-path=\"url(#clip7601)\" points=\"\n",
       "1800.65,390.944 2249.26,390.944 2249.26,209.504 1800.65,209.504 \n",
       "  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip7601)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1800.65,390.944 2249.26,390.944 2249.26,209.504 1800.65,209.504 1800.65,390.944 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip7601)\" style=\"stroke:#009af9; stroke-width:12; stroke-opacity:1; fill:none\" points=\"\n",
       "  1824.65,269.984 1968.65,269.984 \n",
       "  \"/>\n",
       "<g clip-path=\"url(#clip7601)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:start;\" transform=\"rotate(0, 1992.65, 287.484)\" x=\"1992.65\" y=\"287.484\">Train loss</text>\n",
       "</g>\n",
       "<polyline clip-path=\"url(#clip7601)\" style=\"stroke:#e26f46; stroke-width:12; stroke-opacity:1; fill:none\" points=\"\n",
       "  1824.65,330.464 1968.65,330.464 \n",
       "  \"/>\n",
       "<g clip-path=\"url(#clip7601)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:start;\" transform=\"rotate(0, 1992.65, 347.964)\" x=\"1992.65\" y=\"347.964\">Test loss</text>\n",
       "</g>\n",
       "</svg>\n"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Plots.plot(hcat(tr_loss_evol,te_loss_evol), title=\"Loss evolution\",label=[\"Train loss\" \"Test loss\"],lw=3)\n",
    "xlabel!(\"Number of epochs\")\n",
    "ylabel!(\"Loss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous remarquons que les deux fonctions de coût décroient rapidement dans les premières itérations, puis commence à se stabiliser en augmentant le nombre d'itérations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regardons maintenant l'evolution de la précision (pourcentage de bonne réponses) sur les deux ensembles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"utf-8\"?>\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"600\" height=\"400\" viewBox=\"0 0 2400 1600\">\n",
       "<defs>\n",
       "  <clipPath id=\"clip7800\">\n",
       "    <rect x=\"0\" y=\"0\" width=\"2000\" height=\"2000\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<defs>\n",
       "  <clipPath id=\"clip7801\">\n",
       "    <rect x=\"0\" y=\"0\" width=\"2400\" height=\"1600\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<polygon clip-path=\"url(#clip7801)\" points=\"\n",
       "0,1600 2400,1600 2400,0 0,0 \n",
       "  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<defs>\n",
       "  <clipPath id=\"clip7802\">\n",
       "    <rect x=\"480\" y=\"0\" width=\"1681\" height=\"1600\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<polygon clip-path=\"url(#clip7801)\" points=\"\n",
       "224.386,1440.48 2321.26,1440.48 2321.26,125.984 224.386,125.984 \n",
       "  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<defs>\n",
       "  <clipPath id=\"clip7803\">\n",
       "    <rect x=\"224\" y=\"125\" width=\"2098\" height=\"1315\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<polyline clip-path=\"url(#clip7803)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  279.767,1440.48 279.767,125.984 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip7803)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  676.197,1440.48 676.197,125.984 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip7803)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  1072.63,1440.48 1072.63,125.984 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip7803)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  1469.06,1440.48 1469.06,125.984 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip7803)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  1865.48,1440.48 1865.48,125.984 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip7803)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  2261.91,1440.48 2261.91,125.984 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip7803)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  224.386,1297.61 2321.26,1297.61 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip7803)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  224.386,1124.88 2321.26,1124.88 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip7803)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  224.386,952.152 2321.26,952.152 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip7803)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  224.386,779.42 2321.26,779.42 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip7803)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  224.386,606.689 2321.26,606.689 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip7803)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  224.386,433.957 2321.26,433.957 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip7803)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  224.386,261.226 2321.26,261.226 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip7801)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  224.386,1440.48 2321.26,1440.48 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip7801)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  224.386,1440.48 224.386,125.984 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip7801)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  279.767,1440.48 279.767,1420.77 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip7801)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  676.197,1440.48 676.197,1420.77 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip7801)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1072.63,1440.48 1072.63,1420.77 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip7801)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1469.06,1440.48 1469.06,1420.77 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip7801)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1865.48,1440.48 1865.48,1420.77 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip7801)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  2261.91,1440.48 2261.91,1420.77 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip7801)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  224.386,1297.61 255.839,1297.61 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip7801)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  224.386,1124.88 255.839,1124.88 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip7801)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  224.386,952.152 255.839,952.152 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip7801)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  224.386,779.42 255.839,779.42 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip7801)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  224.386,606.689 255.839,606.689 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip7801)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  224.386,433.957 255.839,433.957 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip7801)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  224.386,261.226 255.839,261.226 \n",
       "  \"/>\n",
       "<g clip-path=\"url(#clip7801)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:middle;\" transform=\"rotate(0, 279.767, 1494.48)\" x=\"279.767\" y=\"1494.48\">0</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip7801)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:middle;\" transform=\"rotate(0, 676.197, 1494.48)\" x=\"676.197\" y=\"1494.48\">100</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip7801)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:middle;\" transform=\"rotate(0, 1072.63, 1494.48)\" x=\"1072.63\" y=\"1494.48\">200</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip7801)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:middle;\" transform=\"rotate(0, 1469.06, 1494.48)\" x=\"1469.06\" y=\"1494.48\">300</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip7801)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:middle;\" transform=\"rotate(0, 1865.48, 1494.48)\" x=\"1865.48\" y=\"1494.48\">400</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip7801)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:middle;\" transform=\"rotate(0, 2261.91, 1494.48)\" x=\"2261.91\" y=\"1494.48\">500</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip7801)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:end;\" transform=\"rotate(0, 200.386, 1315.11)\" x=\"200.386\" y=\"1315.11\">0.3</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip7801)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:end;\" transform=\"rotate(0, 200.386, 1142.38)\" x=\"200.386\" y=\"1142.38\">0.4</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip7801)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:end;\" transform=\"rotate(0, 200.386, 969.652)\" x=\"200.386\" y=\"969.652\">0.5</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip7801)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:end;\" transform=\"rotate(0, 200.386, 796.92)\" x=\"200.386\" y=\"796.92\">0.6</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip7801)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:end;\" transform=\"rotate(0, 200.386, 624.189)\" x=\"200.386\" y=\"624.189\">0.7</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip7801)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:end;\" transform=\"rotate(0, 200.386, 451.457)\" x=\"200.386\" y=\"451.457\">0.8</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip7801)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:end;\" transform=\"rotate(0, 200.386, 278.726)\" x=\"200.386\" y=\"278.726\">0.9</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip7801)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:84px; text-anchor:middle;\" transform=\"rotate(0, 1272.82, 73.2)\" x=\"1272.82\" y=\"73.2\">Accuracy evolution</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip7801)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:66px; text-anchor:middle;\" transform=\"rotate(0, 1272.82, 1590.4)\" x=\"1272.82\" y=\"1590.4\">Number of epochs</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip7801)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:66px; text-anchor:middle;\" transform=\"rotate(-90, 57.6, 783.233)\" x=\"57.6\" y=\"783.233\">Accuracy</text>\n",
       "</g>\n",
       "<polyline clip-path=\"url(#clip7803)\" style=\"stroke:#009af9; stroke-width:12; stroke-opacity:1; fill:none\" points=\"\n",
       "  283.732,1318.41 287.696,1318.41 291.66,1323.77 295.625,1324.78 299.589,1292.29 303.553,1182.76 307.517,1003.9 311.482,853.51 315.446,735.944 319.41,666.61 \n",
       "  323.375,628.092 327.339,588.903 331.303,565.122 335.267,544.355 339.232,535.982 343.196,530.957 347.16,525.933 351.125,522.919 355.089,519.569 359.053,515.55 \n",
       "  363.018,511.865 366.982,510.526 370.946,506.841 374.91,503.492 378.875,502.152 382.839,500.812 386.803,490.764 390.768,488.419 394.732,487.414 398.696,487.414 \n",
       "  402.66,485.405 406.625,483.06 410.589,483.06 414.553,481.05 418.518,479.041 422.482,478.036 426.446,475.021 430.411,473.682 434.375,472.342 438.339,469.997 \n",
       "  442.303,469.662 446.268,468.323 450.232,465.308 454.196,464.638 458.161,462.628 462.125,459.949 466.089,456.934 470.053,452.915 474.018,449.566 477.982,447.891 \n",
       "  481.946,446.886 485.911,443.202 489.875,441.527 493.839,440.187 497.804,437.173 501.768,434.828 505.732,432.148 509.696,430.809 513.661,429.469 517.625,427.794 \n",
       "  521.589,425.784 525.554,424.11 529.518,422.1 533.482,417.746 537.446,415.401 541.411,412.387 545.375,410.712 549.339,409.037 553.304,406.358 557.268,404.013 \n",
       "  561.232,404.013 565.197,403.343 569.161,400.998 573.125,397.649 577.089,395.639 581.054,393.965 585.018,391.285 588.982,390.28 592.947,387.936 596.911,385.926 \n",
       "  600.875,384.251 604.839,382.241 608.804,378.222 612.768,377.887 616.732,376.212 620.697,374.873 624.661,372.863 628.625,371.523 632.59,370.183 636.554,368.844 \n",
       "  640.518,365.829 644.482,363.819 648.447,362.145 652.411,361.14 656.375,359.8 660.34,358.125 664.304,356.786 668.268,355.446 672.232,354.441 676.197,353.436 \n",
       "  680.161,352.096 684.125,351.092 688.09,349.752 692.054,347.072 696.018,344.728 699.983,344.058 703.947,343.053 707.911,341.378 711.875,338.699 715.84,337.024 \n",
       "  719.804,335.684 723.768,333.004 727.733,331.33 731.697,329.99 735.661,327.98 739.625,326.306 743.59,323.961 747.554,322.621 751.518,320.946 755.483,317.932 \n",
       "  759.447,316.257 763.411,314.917 767.376,312.573 771.34,309.893 775.304,307.549 779.268,305.539 783.233,302.524 787.197,301.185 791.161,300.515 795.126,298.505 \n",
       "  799.09,297.835 803.054,296.495 807.018,295.156 810.983,294.151 814.947,293.816 818.911,293.146 822.876,291.471 826.84,289.796 830.804,289.462 834.769,288.792 \n",
       "  838.733,286.782 842.697,284.102 846.661,283.767 850.626,283.098 854.59,280.418 858.554,279.413 862.519,278.743 866.483,278.073 870.447,277.738 874.411,276.734 \n",
       "  878.376,274.724 882.34,274.389 886.304,273.384 890.269,271.04 894.233,269.7 898.197,270.035 902.162,269.365 906.126,267.02 910.09,265.68 914.054,264.006 \n",
       "  918.019,263.336 921.983,263.001 925.947,261.996 929.912,262.331 933.876,260.656 937.84,259.651 941.804,258.981 945.769,257.307 949.733,256.302 953.697,254.627 \n",
       "  957.662,253.287 961.626,252.952 965.59,252.283 969.555,251.613 973.519,248.933 977.483,247.593 981.447,247.258 985.412,246.589 989.376,246.589 993.34,246.923 \n",
       "  997.305,246.923 1001.27,246.254 1005.23,245.584 1009.2,245.249 1013.16,244.244 1017.13,244.579 1021.09,243.909 1025.05,243.239 1029.02,242.904 1032.98,242.234 \n",
       "  1036.95,241.229 1040.91,240.559 1044.88,239.89 1048.84,239.89 1052.8,239.22 1056.77,238.215 1060.73,238.55 1064.7,238.55 1068.66,237.545 1072.63,236.54 \n",
       "  1076.59,236.54 1080.55,236.54 1084.52,236.54 1088.48,235.87 1092.45,235.2 1096.41,234.53 1100.38,234.196 1104.34,232.856 1108.3,232.521 1112.27,231.851 \n",
       "  1116.23,231.851 1120.2,231.516 1124.16,231.516 1128.13,231.181 1132.09,230.176 1136.05,230.176 1140.02,229.841 1143.98,229.841 1147.95,228.836 1151.91,228.166 \n",
       "  1155.88,228.166 1159.84,226.492 1163.8,226.157 1167.77,225.487 1171.73,225.487 1175.7,224.817 1179.66,224.817 1183.63,224.482 1187.59,224.147 1191.55,223.812 \n",
       "  1195.52,223.477 1199.48,222.472 1203.45,222.137 1207.41,221.468 1211.38,221.468 1215.34,220.463 1219.31,219.793 1223.27,218.788 1227.23,218.788 1231.2,218.453 \n",
       "  1235.16,217.448 1239.13,217.113 1243.09,216.443 1247.06,216.443 1251.02,215.774 1254.98,215.104 1258.95,215.104 1262.91,214.769 1266.88,214.434 1270.84,213.429 \n",
       "  1274.81,213.429 1278.77,213.094 1282.73,213.094 1286.7,212.759 1290.66,211.419 1294.63,211.084 1298.59,211.084 1302.56,210.414 1306.52,210.414 1310.48,210.414 \n",
       "  1314.45,209.41 1318.41,209.075 1322.38,208.74 1326.34,208.74 1330.31,208.07 1334.27,207.4 1338.23,206.395 1342.2,205.725 1346.16,205.725 1350.13,205.055 \n",
       "  1354.09,204.72 1358.06,204.72 1362.02,204.385 1365.98,204.05 1369.95,204.05 1373.91,203.715 1377.88,203.046 1381.84,202.711 1385.81,202.376 1389.77,201.371 \n",
       "  1393.73,201.036 1397.7,200.701 1401.66,200.701 1405.63,200.366 1409.59,200.031 1413.56,199.696 1417.52,199.696 1421.48,199.696 1425.45,199.696 1429.41,199.361 \n",
       "  1433.38,199.696 1437.34,199.696 1441.31,199.696 1445.27,199.361 1449.23,199.361 1453.2,199.361 1457.16,198.691 1461.13,198.691 1465.09,198.356 1469.06,198.356 \n",
       "  1473.02,198.356 1476.98,198.356 1480.95,198.356 1484.91,198.356 1488.88,197.686 1492.84,198.021 1496.81,198.021 1500.77,197.686 1504.73,197.686 1508.7,197.686 \n",
       "  1512.66,198.021 1516.63,197.686 1520.59,197.351 1524.56,197.017 1528.52,196.682 1532.48,196.682 1536.45,196.347 1540.41,196.012 1544.38,195.677 1548.34,195.342 \n",
       "  1552.31,195.342 1556.27,195.342 1560.23,195.007 1564.2,194.672 1568.16,194.337 1572.13,194.337 1576.09,194.337 1580.06,194.337 1584.02,194.002 1587.98,194.002 \n",
       "  1591.95,194.002 1595.91,194.337 1599.88,194.002 1603.84,193.667 1607.81,194.002 1611.77,194.002 1615.73,194.002 1619.7,193.332 1623.66,192.997 1627.63,192.997 \n",
       "  1631.59,192.327 1635.56,192.327 1639.52,183.619 1643.48,183.284 1647.45,182.949 1651.41,182.949 1655.38,183.284 1659.34,183.284 1663.31,182.949 1667.27,182.614 \n",
       "  1671.23,182.279 1675.2,181.944 1679.16,180.604 1683.13,180.604 1687.09,178.929 1691.06,178.929 1695.02,178.595 1698.98,178.26 1702.95,178.26 1706.91,177.925 \n",
       "  1710.88,177.925 1714.84,177.925 1718.81,177.925 1722.77,178.26 1726.73,178.595 1730.7,178.595 1734.66,178.595 1738.63,177.925 1742.59,177.925 1746.56,177.925 \n",
       "  1750.52,177.255 1754.48,176.92 1758.45,176.585 1762.41,176.25 1766.38,175.915 1770.34,175.58 1774.31,175.58 1778.27,175.58 1782.23,175.58 1786.2,175.245 \n",
       "  1790.16,174.91 1794.13,174.91 1798.09,174.91 1802.06,174.91 1806.02,174.91 1809.98,174.575 1813.95,174.575 1817.91,174.575 1821.88,173.905 1825.84,174.24 \n",
       "  1829.81,173.905 1833.77,173.905 1837.73,173.57 1841.7,173.905 1845.66,173.905 1849.63,173.905 1853.59,173.905 1857.56,173.905 1861.52,173.57 1865.48,173.57 \n",
       "  1869.45,173.57 1873.41,173.57 1877.38,173.905 1881.34,173.235 1885.31,173.235 1889.27,173.235 1893.24,173.235 1897.2,173.235 1901.16,172.9 1905.13,172.9 \n",
       "  1909.09,172.566 1913.06,172.231 1917.02,172.231 1920.99,171.896 1924.95,171.226 1928.91,171.226 1932.88,171.226 1936.84,171.226 1940.81,171.226 1944.77,171.226 \n",
       "  1948.74,170.891 1952.7,170.891 1956.66,170.891 1960.63,170.556 1964.59,170.556 1968.56,169.886 1972.52,169.886 1976.49,169.886 1980.45,169.886 1984.41,169.886 \n",
       "  1988.38,169.551 1992.34,169.216 1996.31,169.216 2000.27,168.881 2004.24,168.881 2008.2,168.211 2012.16,168.211 2016.13,168.211 2020.09,167.876 2024.06,167.876 \n",
       "  2028.02,167.876 2031.99,167.541 2035.95,167.206 2039.91,167.206 2043.88,167.206 2047.84,167.206 2051.81,167.206 2055.77,167.206 2059.74,166.536 2063.7,166.536 \n",
       "  2067.66,166.202 2071.63,165.867 2075.59,165.867 2079.56,165.867 2083.52,165.532 2087.49,165.532 2091.45,165.532 2095.41,165.197 2099.38,165.197 2103.34,165.197 \n",
       "  2107.31,165.197 2111.27,165.197 2115.24,165.197 2119.2,165.197 2123.16,165.532 2127.13,165.532 2131.09,165.867 2135.06,165.867 2139.02,165.867 2142.99,165.867 \n",
       "  2146.95,165.867 2150.91,165.867 2154.88,165.197 2158.84,164.862 2162.81,164.862 2166.77,164.862 2170.74,164.862 2174.7,164.527 2178.66,164.527 2182.63,164.192 \n",
       "  2186.59,164.192 2190.56,164.192 2194.52,164.527 2198.49,164.527 2202.45,164.527 2206.41,164.192 2210.38,164.527 2214.34,164.527 2218.31,164.527 2222.27,164.527 \n",
       "  2226.24,164.527 2230.2,164.527 2234.16,164.192 2238.13,163.857 2242.09,163.857 2246.06,163.522 2250.02,163.522 2253.99,163.522 2257.95,163.522 2261.91,163.187 \n",
       "  \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip7803)\" style=\"stroke:#e26f46; stroke-width:12; stroke-opacity:1; fill:none\" points=\"\n",
       "  283.732,1403.28 287.696,1402.98 291.66,1397.25 295.625,1390.61 299.589,1359.25 303.553,1303.46 307.517,1181.64 311.482,1074.88 315.446,950.342 319.41,837.862 \n",
       "  323.375,765.488 327.339,696.733 331.303,643.056 335.267,619.535 339.232,589.681 343.196,564.652 347.16,548.066 351.125,538.416 355.089,526.052 359.053,518.514 \n",
       "  363.018,514.292 366.982,513.387 370.946,507.959 374.91,506.451 378.875,505.245 382.839,502.833 386.803,502.229 390.768,499.817 394.732,495.294 398.696,493.484 \n",
       "  402.66,488.961 406.625,488.659 410.589,485.945 414.553,484.438 418.518,482.025 422.482,480.517 426.446,477.803 430.411,476.597 434.375,472.978 438.339,470.868 \n",
       "  442.303,468.455 446.268,466.646 450.232,465.44 454.196,463.63 458.161,462.424 462.125,461.218 466.089,458.202 470.053,456.694 474.018,453.076 477.982,452.473 \n",
       "  481.946,450.663 485.911,447.045 489.875,445.838 493.839,445.235 497.804,443.728 501.768,442.823 505.732,441.918 509.696,441.014 513.661,439.807 517.625,438.3 \n",
       "  521.589,436.792 525.554,436.49 529.518,437.093 533.482,435.887 537.446,434.078 541.411,433.475 545.375,431.062 549.339,430.459 553.304,428.047 557.268,426.84 \n",
       "  561.232,426.539 565.197,424.729 569.161,422.015 573.125,419.905 577.089,418.698 581.054,415.984 585.018,415.683 588.982,414.778 592.947,414.175 596.911,412.366 \n",
       "  600.875,409.953 604.839,408.144 608.804,407.239 612.768,406.033 616.732,405.128 620.697,403.922 624.661,402.414 628.625,402.414 632.59,401.208 636.554,398.796 \n",
       "  640.518,396.685 644.482,393.368 648.447,392.765 652.411,391.86 656.375,390.05 660.34,388.543 664.304,388.241 668.268,387.94 672.232,387.336 676.197,384.622 \n",
       "  680.161,382.21 684.125,381.305 688.09,379.798 692.054,378.893 696.018,377.988 699.983,377.084 703.947,377.084 707.911,375.274 711.875,372.862 715.84,372.259 \n",
       "  719.804,369.846 723.768,367.735 727.733,366.529 731.697,364.72 735.661,362.609 739.625,360.498 743.59,356.879 747.554,354.165 751.518,354.165 755.483,352.356 \n",
       "  759.447,352.054 763.411,350.547 767.376,347.833 771.34,346.626 775.304,344.515 779.268,343.309 783.233,341.5 787.197,336.072 791.161,333.961 795.126,329.739 \n",
       "  799.09,327.327 803.054,325.517 807.018,323.708 810.983,321.296 814.947,320.391 818.911,319.185 822.876,317.979 826.84,318.582 830.804,318.28 834.769,315.265 \n",
       "  838.733,314.963 842.697,313.757 846.661,312.551 850.626,310.741 854.59,308.932 858.554,308.027 862.519,308.027 866.483,306.821 870.447,305.615 874.411,305.313 \n",
       "  878.376,305.615 882.34,305.313 886.304,305.615 890.269,304.408 894.233,304.408 898.197,302.599 902.162,302.599 906.126,301.694 910.09,301.393 914.054,301.393 \n",
       "  918.019,300.79 921.983,299.584 925.947,298.076 929.912,296.266 933.876,296.266 937.84,297.171 941.804,296.266 945.769,295.06 949.733,294.156 953.697,293.854 \n",
       "  957.662,291.743 961.626,289.934 965.59,290.235 969.555,289.331 973.519,289.331 977.483,288.426 981.447,288.426 985.412,287.823 989.376,287.22 993.34,287.22 \n",
       "  997.305,286.617 1001.27,286.315 1005.23,286.014 1009.2,286.014 1013.16,285.712 1017.13,284.807 1021.09,283.3 1025.05,283.601 1029.02,283.601 1032.98,282.696 \n",
       "  1036.95,281.792 1040.91,279.982 1044.88,279.078 1048.84,278.776 1052.8,278.173 1056.77,276.967 1060.73,276.665 1064.7,276.062 1068.66,275.459 1072.63,275.158 \n",
       "  1076.59,274.856 1080.55,273.951 1084.52,273.047 1088.48,272.444 1092.45,272.444 1096.41,271.539 1100.38,270.936 1104.34,270.031 1108.3,268.523 1112.27,267.92 \n",
       "  1116.23,267.92 1120.2,267.619 1124.16,267.317 1128.13,267.016 1132.09,266.412 1136.05,266.412 1140.02,265.508 1143.98,264.302 1147.95,264 1151.91,263.698 \n",
       "  1155.88,262.492 1159.84,262.492 1163.8,262.492 1167.77,262.191 1171.73,261.587 1175.7,260.381 1179.66,259.778 1183.63,259.175 1187.59,258.572 1191.55,257.969 \n",
       "  1195.52,257.969 1199.48,257.366 1203.45,257.667 1207.41,257.969 1211.38,257.366 1215.34,257.366 1219.31,256.461 1223.27,256.461 1227.23,256.159 1231.2,255.858 \n",
       "  1235.16,255.556 1239.13,255.255 1243.09,255.858 1247.06,254.953 1251.02,254.049 1254.98,253.747 1258.95,252.842 1262.91,252.239 1266.88,251.636 1270.84,251.636 \n",
       "  1274.81,251.033 1278.77,250.43 1282.73,251.335 1286.7,251.033 1290.66,250.43 1294.63,250.128 1298.59,248.922 1302.56,248.922 1306.52,248.621 1310.48,248.922 \n",
       "  1314.45,248.621 1318.41,247.414 1322.38,246.208 1326.34,245.605 1330.31,245.605 1334.27,245.605 1338.23,245.303 1342.2,245.303 1346.16,244.7 1350.13,244.399 \n",
       "  1354.09,244.399 1358.06,244.097 1362.02,243.494 1365.98,243.494 1369.95,243.494 1373.91,243.494 1377.88,243.494 1381.84,243.796 1385.81,243.494 1389.77,243.193 \n",
       "  1393.73,243.193 1397.7,242.891 1401.66,242.589 1405.63,241.986 1409.59,241.685 1413.56,241.685 1417.52,241.383 1421.48,241.082 1425.45,241.383 1429.41,241.685 \n",
       "  1433.38,241.986 1437.34,241.685 1441.31,240.78 1445.27,240.479 1449.23,240.177 1453.2,239.875 1457.16,239.875 1461.13,239.875 1465.09,238.066 1469.06,237.161 \n",
       "  1473.02,236.558 1476.98,235.654 1480.95,235.352 1484.91,236.257 1488.88,236.257 1492.84,235.955 1496.81,235.955 1500.77,235.955 1504.73,235.051 1508.7,234.447 \n",
       "  1512.66,234.447 1516.63,234.146 1520.59,234.146 1524.56,233.844 1528.52,233.543 1532.48,233.241 1536.45,233.844 1540.41,234.749 1544.38,235.051 1548.34,235.051 \n",
       "  1552.31,235.051 1556.27,234.749 1560.23,234.749 1564.2,234.749 1568.16,234.146 1572.13,234.146 1576.09,234.146 1580.06,233.543 1584.02,233.241 1587.98,233.543 \n",
       "  1591.95,233.844 1595.91,233.543 1599.88,233.241 1603.84,232.638 1607.81,232.337 1611.77,232.337 1615.73,232.337 1619.7,231.13 1623.66,231.432 1627.63,231.432 \n",
       "  1631.59,231.432 1635.56,231.432 1639.52,231.733 1643.48,230.527 1647.45,230.226 1651.41,230.829 1655.38,231.13 1659.34,231.13 1663.31,231.13 1667.27,230.829 \n",
       "  1671.23,230.829 1675.2,230.226 1679.16,230.829 1683.13,230.527 1687.09,230.527 1691.06,230.226 1695.02,229.924 1698.98,229.924 1702.95,229.924 1706.91,230.226 \n",
       "  1710.88,229.924 1714.84,229.924 1718.81,230.226 1722.77,230.226 1726.73,230.226 1730.7,230.226 1734.66,230.829 1738.63,230.829 1742.59,230.829 1746.56,230.527 \n",
       "  1750.52,230.527 1754.48,230.226 1758.45,229.924 1762.41,229.623 1766.38,229.623 1770.34,229.321 1774.31,228.416 1778.27,228.416 1782.23,228.416 1786.2,228.416 \n",
       "  1790.16,228.115 1794.13,227.813 1798.09,227.512 1802.06,227.512 1806.02,227.512 1809.98,227.813 1813.95,227.813 1817.91,227.813 1821.88,227.813 1825.84,227.21 \n",
       "  1829.81,227.21 1833.77,227.21 1837.73,227.21 1841.7,227.512 1845.66,227.512 1849.63,226.305 1853.59,226.305 1857.56,226.607 1861.52,226.607 1865.48,226.305 \n",
       "  1869.45,226.305 1873.41,226.305 1877.38,225.099 1881.34,225.099 1885.31,225.099 1889.27,225.099 1893.24,224.798 1897.2,224.798 1901.16,224.195 1905.13,224.496 \n",
       "  1909.09,224.195 1913.06,223.893 1917.02,223.893 1920.99,223.893 1924.95,223.893 1928.91,224.195 1932.88,224.496 1936.84,224.496 1940.81,224.496 1944.77,224.496 \n",
       "  1948.74,224.496 1952.7,224.195 1956.66,223.591 1960.63,223.591 1964.59,223.29 1968.56,222.988 1972.52,222.988 1976.49,222.988 1980.45,222.988 1984.41,222.687 \n",
       "  1988.38,222.687 1992.34,222.385 1996.31,221.782 2000.27,222.084 2004.24,222.084 2008.2,222.084 2012.16,221.782 2016.13,221.782 2020.09,221.179 2024.06,221.179 \n",
       "  2028.02,221.179 2031.99,221.481 2035.95,221.481 2039.91,221.481 2043.88,221.481 2047.84,221.481 2051.81,221.481 2055.77,221.481 2059.74,222.084 2063.7,221.782 \n",
       "  2067.66,221.782 2071.63,221.481 2075.59,221.481 2079.56,221.179 2083.52,221.179 2087.49,221.179 2091.45,220.877 2095.41,220.877 2099.38,220.877 2103.34,220.576 \n",
       "  2107.31,220.576 2111.27,220.576 2115.24,220.576 2119.2,220.877 2123.16,221.179 2127.13,221.179 2131.09,220.576 2135.06,220.576 2139.02,220.274 2142.99,220.274 \n",
       "  2146.95,220.576 2150.91,219.973 2154.88,219.973 2158.84,219.973 2162.81,220.274 2166.77,219.671 2170.74,219.671 2174.7,219.671 2178.66,219.37 2182.63,219.37 \n",
       "  2186.59,219.068 2190.56,219.068 2194.52,219.068 2198.49,219.068 2202.45,218.766 2206.41,218.766 2210.38,218.465 2214.34,218.465 2218.31,218.465 2222.27,218.465 \n",
       "  2226.24,218.766 2230.2,218.465 2234.16,218.163 2238.13,217.862 2242.09,217.862 2246.06,217.862 2250.02,217.56 2253.99,217.56 2257.95,217.259 2261.91,216.957 \n",
       "  \n",
       "  \"/>\n",
       "<polygon clip-path=\"url(#clip7801)\" points=\"\n",
       "1693.65,390.944 2249.26,390.944 2249.26,209.504 1693.65,209.504 \n",
       "  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip7801)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1693.65,390.944 2249.26,390.944 2249.26,209.504 1693.65,209.504 1693.65,390.944 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip7801)\" style=\"stroke:#009af9; stroke-width:12; stroke-opacity:1; fill:none\" points=\"\n",
       "  1717.65,269.984 1861.65,269.984 \n",
       "  \"/>\n",
       "<g clip-path=\"url(#clip7801)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:start;\" transform=\"rotate(0, 1885.65, 287.484)\" x=\"1885.65\" y=\"287.484\">Train accuracy</text>\n",
       "</g>\n",
       "<polyline clip-path=\"url(#clip7801)\" style=\"stroke:#e26f46; stroke-width:12; stroke-opacity:1; fill:none\" points=\"\n",
       "  1717.65,330.464 1861.65,330.464 \n",
       "  \"/>\n",
       "<g clip-path=\"url(#clip7801)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:start;\" transform=\"rotate(0, 1885.65, 347.964)\" x=\"1885.65\" y=\"347.964\">Test accuracy</text>\n",
       "</g>\n",
       "</svg>\n"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Plots.plot(hcat(tr_accuracy_evol,te_accuracy_evol), title=\"Accuracy evolution\",label=[\"Train accuracy\" \"Test accuracy\"],lw=3)\n",
    "xlabel!(\"Number of epochs\")\n",
    "ylabel!(\"Accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9256284916201117"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(X_test,y_test,theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Résultats:\n",
    "\n",
    "Nous remarquons qu'on atteint une précision de **92%**.\n",
    "\n",
    "Cela est largement meilleure que la régression logistique en utilisant l'algorithme de Metropolis-Hastings (100 itérations). Cependant, on est proche de la performance de la classification Naïve Bayésienne. Ces deux performances ne sont pas très comparables puisque nous n'avons pas utilisé le même nombre de mots dans chacune des deux méthodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C'est un Ham !"
     ]
    }
   ],
   "source": [
    "predict(X_train[1496,:], theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[1496]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous avons donc une bonne prédiction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Essayons maintenant de prédire à partir du chemin d'un fichier, si ce fichier est un spam ou un ham"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data/enron2/ham/0041.1999-12-17.kaminski.ham.txt --> C'est un Ham !\n"
     ]
    }
   ],
   "source": [
    "predict_from_file(\"./data/enron2/ham/0041.1999-12-17.kaminski.ham.txt\", theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion de la régression logistique:\n",
    "\n",
    "Pour estimer les poids, nous avons eu recours à deux algorithmes: \n",
    "\n",
    "- Metropolis Hastings: Cet algorithme permet de converger quelque soit l'initialisation de nos paramètres, d'où son utilisation. Le problème est qu'il est moins utilisé quand le nombre de paramètres devient grand; On utiise l'algorithme de Gibbs dans ce cas. Mais en n'ayant pas la loi de la loi conditionnelle complète des paramètres, nous ne pouvons qu'utiliser Metropolis Hasting avec un nombre de paramètres égal à 907. Nous remarquons aussi qu'avec 100 itérations **(très peu)**, nous avons attendu environs 5 minutes, ce qui limite la recherche de meilleurs paramètres.\n",
    "\n",
    "- Descente de gradient: Cet algorithme est un algorithme de classification très robuste, qui malgré les performances des nouveaux algorithmes tels que les réseaux de neurones, est toujours utilisés dans beaucoup de domaines. Nous avons aussi pu voir la rapidité de l'entrainement (500 itérations) qui nous a permis d'arriver à 0.87% en précision sur l'ensemble de Test."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IV. <a id = \"unit4\" > </a >  Références \n",
    "____\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "(1) Base de données utilisée \n",
    ": https://labs-repos.iit.demokritos.gr/skel/i-config/downloads/enron-spam/preprocessed/ ; \n",
    "\n",
    "(2) Sur la base de données d'enron: https://en.wikipedia.org/wiki/Enron_Corpus  \n",
    "\n",
    "(3) Classification naïve bayésienne:  https://fr.wikipedia.org/wiki/Classification_na%C3%AFve_bay%C3%A9sienne\n",
    "\n",
    "(4) Laplace smoothing: \n",
    "-  https://en.wikipedia.org/wiki/Additive_smoothing  \n",
    "- https://stats.stackexchange.com/questions/108797/in-naive-bayes-why-bother-with-laplace-smoothing-when-we-have-unknown-words-in    \n",
    "\n",
    "(5) Surapprentissage: https://fr.wikipedia.org/wiki/Surapprentissage  \n",
    "\n",
    "(6) Metropolis-Hastings : http://www.mit.edu/~ilkery/papers/MetropolisHastingsSampling.pdf  \n",
    "\n",
    "(7) Descente de gradient: https://eric.univ-lyon2.fr/~ricco/cours/slides/gradient_descent.pdf\n",
    "\n",
    "(8) Régression logistique: https://data.princeton.edu/wws509/notes/c3.pdf"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.1.0",
   "language": "julia",
   "name": "julia-1.1"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
